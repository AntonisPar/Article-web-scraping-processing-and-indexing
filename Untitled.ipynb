{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89777a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/antonis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/antonis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/antonis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "100020\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import dicttoxml\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import os\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import random\n",
    "from nltk import FreqDist\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import json\n",
    "\n",
    "stopword = stopwords.words('english')\n",
    "print(stopword)\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "ps = PorterStemmer()\n",
    "articles_processed = []\n",
    "vocabulary = []\n",
    "article_ids = []\n",
    "\n",
    "closed_categories = ['CC', 'CD', 'DT', 'EX', 'IN', 'LS', 'MD', 'PDT', 'POS', 'PRP', 'PRP$', 'RP', 'TO', 'UH', 'WDT',\n",
    "                     'WP', 'WP$', 'WRB']\n",
    "\n",
    "\n",
    "def gather():\n",
    "    articles_processed = {}\n",
    "    vocabulary = []\n",
    "    article_ids = {}\n",
    "    id = 0\n",
    "    iid = 0\n",
    "    for file in os.listdir('news'):\n",
    "\n",
    "        if file.endswith('.json'):\n",
    "            path = os.path.join('news', file)\n",
    "            f = open(path)\n",
    "            newsite = json.load(f)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "        articles = []\n",
    "        for i in newsite:\n",
    "            id = id + 1\n",
    "            article_ids[id] = i\n",
    "            newsite[i] = newsite[i].lower()\n",
    "            articles.append(nltk.word_tokenize(newsite[i]))\n",
    "            # [res.append(x) for x in test_list if x not in res]\n",
    "        for article in articles:\n",
    "            iid = iid + 1\n",
    "            article = [word for word in article if word not in stopword]\n",
    "            article = [word for word in article if word.isalnum()]\n",
    "            tagged_article = nltk.pos_tag(article)\n",
    "            article = [custom_lemmatizer(word[0],word[1]) for word in tagged_article]\n",
    "            articles_processed[iid] = article\n",
    "\n",
    "    return articles_processed, article_ids\n",
    "\n",
    "\n",
    "def custom_lemmatizer(word, pos_tag):\n",
    "\n",
    "    flag = 0\n",
    "    for tag in closed_categories:\n",
    "        if pos_tag == tag:\n",
    "            flag = 1\n",
    "    if flag == 0:\n",
    "        if pos_tag.startswith(\"N\"):\n",
    "            word = wordnet_lemmatizer.lemmatize(word, wordnet.NOUN)\n",
    "        elif pos_tag.startswith('V'):\n",
    "            word = wordnet_lemmatizer.lemmatize(word, wordnet.VERB)\n",
    "        elif pos_tag.startswith('J'):\n",
    "            word = wordnet_lemmatizer.lemmatize(word, wordnet.ADJ)\n",
    "        elif pos_tag.startswith('R'):\n",
    "            word = wordnet_lemmatizer.lemmatize(word, wordnet.ADV)\n",
    "    vocabulary.append(word)\n",
    "\n",
    "    return(word)\n",
    "\n",
    "def create(vocabulary, articles_processed):\n",
    "    # Creating an index for each word in our vocab.\n",
    "    index_dict = {}  # Dictionary to store index for each word\n",
    "    i = 0\n",
    "    for word in vocabulary:\n",
    "        index_dict[word] = i\n",
    "        i += 1\n",
    "\n",
    "\n",
    "# keep the count of the documents containing a word\n",
    "def count_dict(articles_processed):\n",
    "    word_count = {}\n",
    "    for word in vocabulary:\n",
    "        word_count[word] = 0\n",
    "        for article in articles_processed:\n",
    "            if word in articles_processed[article]:\n",
    "                word_count[word] += 1\n",
    "    return word_count\n",
    "\n",
    "\n",
    "def termfreq(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance / N\n",
    "\n",
    "\n",
    "def inverse_doc_freq(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(len(articles_processed) / word_occurance)\n",
    "\n",
    "\n",
    "def create_index(articles_processed):\n",
    "    inverted_index = {}\n",
    "    for word in vocabulary:\n",
    "        wordd = {}\n",
    "        for i in articles_processed:\n",
    "            if word in articles_processed[i]:\n",
    "                tf = termfreq(articles_processed[i], word)\n",
    "                idf = inverse_doc_freq(word)\n",
    "                value = tf * idf\n",
    "                wordd[i] = value\n",
    "        inverted_index[word] = wordd\n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "dictionary = {}\n",
    "articles_processed, article_ids = gather()\n",
    "word_count = count_dict(articles_processed)\n",
    "# dictionary = create(vocabulary,articles_processed)\n",
    "inverted_index = create_index(articles_processed)\n",
    "\n",
    "print(len(vocabulary))\n",
    "print(len(articles_processed))\n",
    "\n",
    "\n",
    "\n",
    "def save(file, name):\n",
    "    a_file = open(name + \".json\", \"w\")\n",
    "    json.dump(file, a_file)\n",
    "    a_file.close()\n",
    "\n",
    "#\n",
    "# save(inverted_index, \"tf_idf\")\n",
    "save(article_ids, \"article_map\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b773dbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'writexml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dz/wvvxlg856kb9w1q9w9_p5mjh0000gn/T/ipykernel_1685/3128446292.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfile_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filename.xml\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritexml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'writexml'"
     ]
    }
   ],
   "source": [
    "import dict2xml\n",
    "tfidf = dict2xml.dict2xml(inverted_index)\n",
    "print(type(tfidf))\n",
    "file_handle = open(\"filename.xml\",\"wb\")\n",
    "tfidf.writexml(file_handle)\n",
    "file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cf2d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

 1/1: print("hee")
 4/1: print("hello world")
 6/1:
import random
num_samples = 100000000

def inside(p):     
  x, y = random.random(), random.random()
  return x*x + y*y < 1

count = sc.parallelize(range(0, num_samples)).filter(inside).count()

pi = 4 * count / num_samples
print(pi)
 6/2:
import random
num_samples = 100000000

def inside(p):     
  x, y = random.random(), random.random()
  return x*x + y*y < 1

count = sc.parallelize(range(0, num_samples)).filter(inside).count()

pi = 4 * count / num_samples
print(pi)
 6/3:
import random
num_samples = 100000000

def inside(p):     
  x, y = random.random(), random.random()
  return x*x + y*y < 1

count = sc.parallelize(range(0, num_samples)).filter(inside).count()

pi = 4 * count / num_samples
print(pi)
sc.stop()
 9/1:
import random
num_samples = 100000000

def inside(p):     
  x, y = random.random(), random.random()
  return x*x + y*y < 1

count = sc.parallelize(range(0, num_samples)).filter(inside).count()

pi = 4 * count / num_samples
print(pi)
 9/2:
import random
num_samples = 100000000

def inside(p):     
  x, y = random.random(), random.random()
  return x*x + y*y < 1

count = sc.parallelize(range(0, num_samples)).filter(inside).count()

pi = 4 * count / num_samples
print(pi)
10/1:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
10/2:
#create a new Spark Session
spark = SparkSession \
    .builder \
    .appName('Album Data Example') \
    .getOrCreate()
10/3:
#show the spark session object properties - link to the Spark UI
spark
10/4:
#load a csv file as a Spark DataFrame
df = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("album_ratings.csv") #filename to read from
     )
10/5:
#check if the schema inference worked
df.printSchema()
10/6:
#show the first 3 rows of the dataframe
df.show(3)
10/7:
#find the artist with the most albums
df2 = df.select(df["Artist"]).groupby(df["Artist"]).count()
df2.orderBy(df2["count"].desc()).limit(1).show()
10/8:
#find the top 10 albums of 2001
df.filter(df["Release Year"]==2001).orderBy(df["Metacritic User Score"].desc()).\
    select(df["Artist"], df["Title"], df["Metacritic User Score"]).limit(10).show()
10/9:
#find all albums belonging to the top 3 genres (genres with most albums)
#first, find the top 3 genres, and ignore rows where genre is null
genres = df.select(df["genre"]).filter(df["genre"].isNotNull()).groupby(df["genre"]).count()
top3g = genres.orderBy(genres["count"].desc()).limit(3)
top3g.show()
#now join with the main dataframe
result = df.join(top3g, df["Genre"] == top3g["Genre"], "inner")
result.show(3)
10/10:
#find the top album of each year in the list of albums from the top 3 genres
#one problem is that a year may have 1 or more albums with the same score (top score)
#let's devise a metric to fix this - new metric will be score*reviews
#first, remove rows where score or reviews is null
result = result.filter(result["Metacritic User Reviews"].isNotNull() & result["Metacritic User Score"].isNotNull())
result= result.withColumn("megascore", result["Metacritic User Score"]*result["Metacritic User Reviews"])
#get only the data we want
result2 = result.select(result["Artist"],result["Title"], result["Release Year"], result["megascore"])
result2.show(3)

#so as not to clash with python inbuilt max function
from pyspark.sql.functions import max as sparkMax 

#use the .agg function to specify aggregation on just megascore (otherwise it does it on all numeric cols)
maxscores = result2.groupBy("Release Year")\
    .agg(sparkMax(result2["megascore"]).alias("maxscore"))\
    .orderBy("Release Year")
maxscores.show(3)

#now join
final = result2.join(maxscores, (maxscores["Release Year"] == result2["Release Year"]) &\
                     (maxscores["maxscore"]==result2["megascore"]), "inner").orderBy(result2["Release Year"])
final.show(3)
10/11:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import pandas 
import random
from pyspark.sql.functions import broadcast
10/12:
#create a new Spark Session
spark = SparkSession \
    .builder \
    .appName('Album Data Example') \
    .getOrCreate()
10/13:
#show the spark session object properties - link to the Spark UI
spark
10/14:
#load a csv file as a Spark DataFrame
df = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("album_ratings.csv") #filename to read from
     )
10/15:
#check if the schema inference worked
df.printSchema()
10/16:
#show the first 3 rows of the dataframe
df.show(3)
11/1: import pandas as pd
12/1: import pandas as pd
12/2:
import pandas as pd
import numpy as np
12/3:
import pandas as pd
import numpy as np


df = pd.read_csv('movies.csv')
print(df)
14/1:
import pandas as pd
import numpy as np


df = pd.read_csv('movies.csv')
print(df)
14/2:
import pandas as pd
import numpy as np


df = pd.read_csv('movie.csv')
print(df)
15/1:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
15/2:
#create a new Spark Session
spark = SparkSession \
    .builder \
    .appName('Album Data Example') \
    .getOrCreate()
15/3:
#show the spark session object properties - link to the Spark UI
spark
15/4:
#load a csv file as a Spark DataFrame
df = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("album_ratings.csv") #filename to read from
     )
15/5:
#check if the schema inference worked
df.printSchema()
15/6:
#show the first 3 rows of the dataframe
df.show(3)
15/7:
#find the artist with the most albums
df2 = df.select(df["Artist"]).groupby(df["Artist"]).count()
df2.orderBy(df2["count"].desc()).limit(1).show()
15/8:
#find the top 10 albums of 2001
df.filter(df["Release Year"]==2001).orderBy(df["Metacritic User Score"].desc()).\
    select(df["Artist"], df["Title"], df["Metacritic User Score"]).limit(10).show()
15/9:
#find all albums belonging to the top 3 genres (genres with most albums)
#first, find the top 3 genres, and ignore rows where genre is null
genres = df.select(df["genre"]).filter(df["genre"].isNotNull()).groupby(df["genre"]).count()
top3g = genres.orderBy(genres["count"].desc()).limit(3)
top3g.show()
#now join with the main dataframe
result = df.join(top3g, df["Genre"] == top3g["Genre"], "inner")
result.show(3)
15/10:
#find the top album of each year in the list of albums from the top 3 genres
#one problem is that a year may have 1 or more albums with the same score (top score)
#let's devise a metric to fix this - new metric will be score*reviews
#first, remove rows where score or reviews is null
result = result.filter(result["Metacritic User Reviews"].isNotNull() & result["Metacritic User Score"].isNotNull())
result= result.withColumn("megascore", result["Metacritic User Score"]*result["Metacritic User Reviews"])
#get only the data we want
result2 = result.select(result["Artist"],result["Title"], result["Release Year"], result["megascore"])
result2.show(3)

#so as not to clash with python inbuilt max function
from pyspark.sql.functions import max as sparkMax 

#use the .agg function to specify aggregation on just megascore (otherwise it does it on all numeric cols)
maxscores = result2.groupBy("Release Year")\
    .agg(sparkMax(result2["megascore"]).alias("maxscore"))\
    .orderBy("Release Year")
maxscores.show(3)

#now join
final = result2.join(maxscores, (maxscores["Release Year"] == result2["Release Year"]) &\
                     (maxscores["maxscore"]==result2["megascore"]), "inner").orderBy(result2["Release Year"])
final.show(3)
16/1:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
16/2:
spark = SparkSession \
    .builder \
    .appName('Project Vaseis') \
    .getOrCreate()
16/3: spark
16/4:
df = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )
16/5: df.printSchema()
16/6: df.show(3)
16/7:
movies = spark.read.option("header", "true").csv("movies.csv")
movies.show(5)
16/8:
movies = spark.read.option("header", "true").csv("movie.csv")
movies.show(5)
16/9:
ratings = spark.read.option("header", "true").csv("ml-20m/ratings.csv")
ratings.show(5)
16/10:
ratings = spark.read.option("header", "true").csv("rating.csv")
ratings.show(5)
16/11:
from pyspark.sql.functions import *

most_popular = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc("num_ratings"))
16/12: most_popular.show(10)
16/13:
most_popular_movies = most_popular.join(movies, most_popular.movieId == movies.movieId)
most_popular_movies.show(20, truncate=False)
`
16/14:
most_popular_movies = most_popular.join(movies, most_popular.movieId == movies.movieId)
most_popular_movies.show(20, truncate=False)
17/1:
seen_jumanji = ratings\
.agg(count("userId"))
17/2:
import pyspark.sql.functions as F

seen_jumanji = ratings\
.agg(count("userId"))
17/3:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
17/4:
spark = SparkSession \
    .builder \
    .appName('Project Vaseis') \
    .getOrCreate()
17/5: spark
17/6:
df = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )
17/7: df.printSchema()
17/8: df.show(3)
17/9:
movies = spark.read.option("header", "true").csv("movie.csv")
movies.show(5)
17/10:
ratings = spark.read.option("header", "true").csv("rating.csv")
ratings.show(5)
17/11:
from pyspark.sql.functions import *

most_popular = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc("num_ratings"))
17/12: most_popular.show(10)
17/13:
most_popular_movies = most_popular.join(movies, most_popular.movieId == movies.movieId)
most_popular_movies.show(20, truncate=False)
17/14:
import pyspark.sql.functions as F

seen_jumanji = ratings\
.agg(count("userId"))
17/15:
import pyspark.sql.functions as F

seen_jumanji = ratings\
.agg(count("userId"))
17/16: print(seen_jumanji)
17/17: seen_jumanji.show()
17/18:
import pyspark.sql.functions as F

seen_jumanji = ratings\
.agg(count("userId" where movieId = 2)
17/19:
import pyspark.sql.functions as F

seen_jumanji = ratings\
.filter(F.col("movieId") == 2).count()
17/20: seen_jumanji.show()
17/21: print(seen_jumanji)
17/22:
import pyspark.sql.functions as F

merged = movies.merge(ratings, on="movieId")

seen_jumanji = ratings\
.filter(F.col("movieId") == 2).count()
17/23:
import pyspark.sql.functions as F
import pandas as pd

merged = movies.merge(ratings, on="movieId")

seen_jumanji = ratings\
.filter(F.col("movieId") == 2).count()
17/24:
import pyspark.sql.functions as F
import pandas as pd

merged = pd.merge(movies,ratings,on"movieId")

seen_jumanji = ratings\
.filter(F.col("movieId") == 2).count()
17/25:
import pyspark.sql.functions as F
import pandas as pd

merged = pd.merge(movies,ratings,on="movieId")

seen_jumanji = ratings\
.filter(F.col("movieId") == 2).count()
17/26:
import pyspark.sql.functions as F
import pandas as pd

merged = movies.union(ratings)

seen_jumanji = ratings\
.filter(F.col("movieId") == 2).count()
17/27:
top=movie.join(Rating,movie("movieId")===Rating("MovieId"),"inner").sort(desc("Rating"))
top.select("title","Rating").show(20)
17/28:
top=movie.join(Rating,movie("movieId")==Rating("MovieId"),"inner").sort(desc("Rating"))
top.select("title","Rating").show(20)
17/29:
top=movies.join(Rating,movie("movieId")==Rating("MovieId"),"inner").sort(desc("Rating"))
top.select("title","Rating").show(20)
17/30:
top=movies.join(Rating,movie("movieId")==ratings("MovieId"),"inner").sort(desc("Rating"))
top.select("title","Rating").show(20)
17/31:
top=movies.join(ratings,movies("movieId")==ratings("MovieId"),"inner").sort(desc("Rating"))
top.select("title","Rating").show(20)
18/1:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import pandas 
import random
from pyspark.sql.functions import broadcast
18/2:
#create a new Spark Session
spark = SparkSession \
    .builder \
    .appName('Album Data Example') \
    .getOrCreate()
18/3:
#show the spark session object properties - link to the Spark UI
spark
18/4:
#load a csv file as a Spark DataFrame
df = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("album_ratings.csv") #filename to read from
     )
18/5:
#show the first 3 rows of the dataframe
df.show(3)
18/6:
#find the artist with the most albums
df2 = df.select(df["Artist"]).groupby(df["Artist"]).count()
df2.orderBy(df2["count"].desc()).limit(1).show()
18/7:
#find the top 10 albums of 2001
df.filter(df["Release Year"]==2001).orderBy(df["Metacritic User Score"].desc()).\
    select(df["Artist"], df["Title"], df["Metacritic User Score"]).limit(10).show()
18/8:
#find all albums belonging to the top 3 genres (genres with most albums)
#first, find the top 3 genres, and ignore rows where genre is null
genres = df.select(df["genre"]).filter(df["genre"].isNotNull()).groupby(df["genre"]).count()
top3g = genres.orderBy(genres["count"].desc()).limit(3)
top3g.show()
#now join with the main dataframe
result = df.join(top3g, df["Genre"] == top3g["Genre"], "inner")
result.show(3)
18/9:
#find the top album of each year in the list of albums from the top 3 genres
#one problem is that a year may have 1 or more albums with the same score (top score)
#let's devise a metric to fix this - new metric will be score*reviews
#first, remove rows where score or reviews is null
result = result.filter(result["Metacritic User Reviews"].isNotNull() & result["Metacritic User Score"].isNotNull())
result= result.withColumn("megascore", result["Metacritic User Score"]*result["Metacritic User Reviews"])
#get only the data we want
result2 = result.select(result["Artist"],result["Title"], result["Release Year"], result["megascore"])
result2.show(3)

#so as not to clash with python inbuilt max function
from pyspark.sql.functions import max as sparkMax 

#use the .agg function to specify aggregation on just megascore (otherwise it does it on all numeric cols)
maxscores = result2.groupBy("Release Year")\
    .agg(sparkMax(result2["megascore"]).alias("maxscore"))\
    .orderBy("Release Year")
maxscores.show(3)

#now join
final = result2.join(maxscores, (maxscores["Release Year"] == result2["Release Year"]) &\
                     (maxscores["maxscore"]==result2["megascore"]), "inner").orderBy(result2["Release Year"])
final.show(3)
17/32:
import pyspark.sql.functions as F
import pandas as pd

seen_jumanji = ratings\
.filter(F.col("movieId") == 2).count()
17/33: right_join = movies.join(ratings, movies.movieId == ratings.movieId,how='right')
17/34:
right_join = movies.join(ratings, movies.movieId == ratings.movieId,how='right') 
right_join.show()
17/35: new_df = movies.join(ratings, on=['movieId'], how='left_outer')
17/36:
new_df = movies.join(ratings, on=['movieId'], how='left_outer')
new_df.show()
17/37:
import pyspark.sql.functions as F
import pandas as pd

seen_jumanji = ratings\
.filter(F.col("movieId") == "Jumanji (1995)").count()
17/38: print(seen_jumanji)
17/39:
import pyspark.sql.functions as F
import pandas as pd

seen_jumanji = ratings\
.filter(F.col("title") == "Jumanji (1995)").count()
17/40:
merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = ratings\
.filter(F.col("title") == "Jumanji (1995)").count()
17/41:
merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()
17/42: seen_jumanji.show()
17/43: print(seen_jumanji)
17/44:
merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)
17/45:
merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)
17/46: genome_tags = spark.read.option("header", "true").csv("genome_tags.csv")
17/47:
genome_tags = spark.read.option("header", "true").csv("genome_tags.csv")
tag = spark.read.option("header", "true").csv("tag.csv")
genome_scores = spark.read.option("header", "true").csv("genome_scores.csv")
link = spark.read.option("header", "true").csv("link.csv")
17/48:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
merge_tags.show()
17/49:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')
df.show()

boring = merge_tags\
.groupBy
17/50:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = merge_tags\
.groupBy('movieId')\
.filter(F.col("tag") == "boring")
17/51:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = merge_tags\
.filter(F.col("tag") == "boring")
17/52:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = merge_tags\
.filter(F.col("tag") == "boring")

print(boring)
17/53:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = merge_tags\
.filter(F.col("tag") == "boring")

boring.show()
17/54:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.groupby("movieId")\
.agg(F.col("tag") == "boring")
17/55:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.groupby("movieId")\
.agg(F.filter("tag") == "boring")
17/56:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.groupby("movieId")\
.agg(filter.(F.col("tag") == "boring")
17/57:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.groupby("movieId")\
.agg(.filter.(F.col("tag") == "boring")
17/58:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.groupby("movieId")\
.filter.(F.col("tag") == "boring")
17/59:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.filter(F.col("tag") == "boring")
17/60:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.filter(df.tag == "boring").collect()
17/61:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.filter(df.tag == "boring").collect()

boring.show()
17/62:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.filter(df.tag == "boring").collect()

print(boring)
17/63:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.filter(df.tag == "boring").collect()

print(boring.movieId)
17/64:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('movieId')\
.where('tag'=='boring')
17/65:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('movieId')\
.where('tag' == "boring")
17/66:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('movieId')\
.where(col('tag') == "boring")
17/67:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('movieId')\
.where(col('tag') == "boring")

boring.show()
17/68:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('movieId')\
.where(col('tag') == "boring")

df.show()
17/69:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('movieId')\
.where(col('tag') == "boring")

print(boring)
17/70:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('title')\
.where(col('tag') == "boring")

boring.show()
17/71:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('title')\
.where(col('tag') == "sdasd")

boring.show()
17/72:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('title')\
.where(col('tag') == "boring")

boring.show()
17/73:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('title')\
.where(col('tag') == "boring")

boring.show(100)
17/74:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('title')\
.where(col('tag') == "boring")

boring.show(1000)
17/75:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('title')\
.where(col('tag') == "awesome")

boring.show(1000)
17/76:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('title')\
.where(col('tag') == "what")

boring.show(1000)
17/77:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('title')\
.where(col('tag').like("boring"))

boring.show(1000)
17/78:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = df\
.select('title')\
.where(col('tag').like("bor"))

boring.show(1000)
17/79:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')


kappa = df\
.filter(F.col("tag") == "boring").count()
print(kappa)

boring = df\
.select('title')\
.where(col('tag').like("bo"))

boring.show(1000)
17/80:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')


kappa = merge_tags\
.filter(F.col("tag") == "boring").count()
print(kappa)

boring = df\
.select('title')\
.where(col('tag').like("bo"))

boring.show(1000)
17/81:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')


kappa = genome_scores\
.filter(F.col("tag") == "boring").count()
print(kappa)

boring = df\
.select('title')\
.where(col('tag').like("bo"))

boring.show(1000)
17/82:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')


kappa = genome_tags\
.filter(F.col("tag") == "boring").count()
print(kappa)

boring = df\
.select('title')\
.where(col('tag').like("bo"))

boring.show(1000)
17/83:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')


kappa = genome_tags\
.filter(F.col("tag").like("boring")).count()
print(kappa)

boring = df\
.select('title')\
.where(col('tag').like("bo"))

boring.show(1000)
17/84:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')


kappa = genome_tags\

boring = df\
.select('title')\
.where(F.col('tag').like("boring"))

boring.show(1000)
17/85:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')


kappa = genome_tags\

boring = df\
.select('title')\
.where(F.col('tag').like("boring"))

boring.show(1100000)
17/86:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')


kappa = genome_tags\

boring = df\
.select('title')\
.where(F.col('tag').contains("boring"))
17/87:
merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
df = movies.join(merge_tags, on=['movieId'], how='left_outer')


kappa = genome_tags\
.filter(F.col("tag").contains("boring")).count()
print(kappa)

boring = df\
.select('title')\
.where(F.col('tag').like("bo"))
17/88: tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')
17/89:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))
17/90: tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')
17/91:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')

tags_movies_and_ratings.show()
17/92:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

tags_and_movies.show()
17/93:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("Bollywood") & F.col('rating') > 3)
17/94:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("Bollywood"))
17/95:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("Bollywood"))

bollywood.show()
17/96:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("bollywood"))
17/97:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("bollywood"))

bollywood.show()
17/98:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("birds"))

bollywood.show(1000)
17/99:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("complex"))

bollywood.show(1000)
17/100:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='left_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("complex"))

bollywood.show(1000)
17/101:
tags_movies_and_ratings = tags_and_movies.join(rating,on=['movieId'],how='left_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("complex"))

bollywood.show(1000)
17/102:
tags_movies_and_ratings = tags_and_movies.join(ratings,on=['movieId'],how='right_outer')

bollywood = tags_movies_and_ratings\
.select('title')\
.where(F.col('tag').contains("complex"))

bollywood.show(1000)
17/103:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

tags_and_movies.show()

spark.sql("SELECT title from tags_and_movies where tag contains "boring"")
17/104:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

tags_and_movies.show()

spark.sql("SELECT title from tags_and_movies where tag contains 'boring'")
17/105:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

tags_and_movies.show()

spark.sql("SELECT title from tags_and_movies where tag == boring)
17/106:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

tags_and_movies.show()

spark.sql("SELECT title from tags_and_movies where tag == boring")
17/107:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

tags_and_movies.show()

tags_and_movies.createOrReplaceTempView("ta_and_mo")

spark.sql("SELECT title from ta_and_mo where tag == boring")
17/108:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

tags_and_movies.show()

tags_and_movies.createOrReplaceTempView("ta_and_mo")

spark.sql("SELECT title from ta_and_mo where tag == 'boring'")
17/109:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

tags_and_movies.show()

tags_and_movies.createOrReplaceTempView("ta_and_mo")

spark.sql("SELECT title from ta_and_mo where tag == 'boring'").show()
17/110:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

tags_and_movies.show()

tags_and_movies.createOrReplaceTempView("ta_and_mo")

spark.sql("SELECT title from ta_and_mo where tag == 'boring'").show(1000)
17/111: tags_and_movies.show()
17/112: ratings.show()
17/113: tags_movies_ratings = tags_and_movies.join(ratings,on=['movieid'],how='right_join')
17/114: tags_movies_ratings = tags_and_movies.join(ratings,on=['movieid'],how='inner')
17/115:
tags_movies_ratings = tags_and_movies.join(ratings,on=['movieid'],how='inner')

tags_movies_ratings.show()
17/116:
tags_movies_ratings = tags_and_movies.join(ratings,on=['movieid'],how='inner')

tags_movies_ratings.show(10000)
17/117:
tags_movies_ratings = tags_and_movies.join(ratings,on=['movieid'],how='inner')

tags_movies_ratings.show(10000)

tags_movies_ratings.createOrReplaceTempView("ta_mo_ra")

spark.sql("SELECT title from ta_mo_ra where tag == 'boring'").show(1000)
17/118:
tags_movies_ratings = tags_and_movies.join(ratings,on=['movieid'],how='full_outer')

tags_movies_ratings.show(10000)

tags_movies_ratings.createOrReplaceTempView("ta_mo_ra")

spark.sql("SELECT title from ta_mo_ra where tag == 'boring'").show(1000)
17/119: tags_and_ratings = ratings.join(merge_tags,on['movieId'], how='left_outer')
17/120: tags_and_ratings = ratings.join(merge_tags,on=['movieId'], how='left_outer')
17/121:
tags_and_ratings = ratings.join(merge_tags,on=['movieId'], how='left_outer')

tags_and_ratings.show()
17/122:
tags_and_ratings = ratings.join(merge_tags,on=['movieId'], how='left_outer')

tags_and_ratings.show(10000)
17/123:
tags_and_ratings = ratings.join(merge_tags,on=['movieId'], how='left_outer')

tags_and_ratings.show(10000)

bollywood= tags_and_ratings\
.select('movieId')\
.where(F.col('tag').contains("bollywood"))
17/124:
tags_and_ratings = ratings.join(merge_tags,on=['movieId'], how='left_outer')


bollywood= tags_and_ratings\
.select('movieId')\
.where(F.col('tag').contains("bollywood"))

bollywood.show(1000)
17/125:
tags_and_ratings = ratings.join(merge_tags,on=['movieId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood"))

bollywood.show(1000)
17/126:
tags_and_ratings = ratings.join(merge_tags,on=['movieId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood"))

bollywood.show(100000)
17/127:
#QUERY 2

merge_tags = genome_scores.join(genome_tags, on=['tagId'], how='left_outer')
tags_and_movies= movies.join(merge_tags, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

#SAME
tags_and_movies.createOrReplaceTempView("ta_and_mo")

spark.sql("SELECT title from ta_and_mo where tag == 'boring'").show(1000)
17/128:
#QUERY 2

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

#SAME
tags_and_movies.createOrReplaceTempView("ta_and_mo")

spark.sql("SELECT title from ta_and_mo where tag == 'boring'").show(1000)
17/129:
tags_and_ratings = ratings.join(tag,on=['movieId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood"))

bollywood.show(100000)
17/130:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood"))

bollywood.show(100000)
17/131:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood.show(100000)
17/132:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood") & F.col('rating') > 3).distinct()

bollywood.show(100000)
17/133:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood") && F.col('rating') > 3).distinct()

bollywood.show(100000)
17/134:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.where(F.when ((F.col('tag').contains("bollywood") & (F.col('rating') > 3 ))) .distinct()


bollywood.show(100000)
17/135:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.where(F.when ((F.col('tag').contains("bollywood") & (F.col('rating') > 3 ))).distinct()
17/136:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.where(F.when ((F.col('tag').contains("bollywood") & (F.col('rating') > 3 )).distinct()
17/137:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.where(F.when ((F.col('tag').contains("bollywood") & (F.col('rating') > 3 ))))).distinct()
17/138:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.where(F.when ((F.col('tag').contains("bollywood") & (F.col('rating') > 3 )))).distinct()
17/139:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.when((F.col('tag').contains("bollywood") & (F.col('rating') > 3 ))).distinct()
17/140:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.F.when((F.col('tag').contains("bollywood") & (F.col('rating') > 3 )))).distinct()
17/141:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.F.when((F.col('tag').contains("bollywood") & (F.col('rating') > 3 ))).distinct()
17/142:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.where(F.when ((F.col('tag').contains("bollywood") & (F.col('rating') > 3 ))).distinct()
17/143:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.where((F.when((F.col('tag').contains("bollywood") & (F.col('rating') > 3 ))).distinct()
17/144:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()

bollywood= tags_and_ratings\
.select('userId')\
.where((F.when((F.col('tag').contains("bollywood") & (F.col('rating') > 3 )))).distinct()
17/145:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


bollywood= tags_and_ratings\
.select('userId')\
.where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))) .distinct()
17/146:
tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))) .distinct()
          
bollywood.show()
17/147:
titles = tags_and_ratings['title']

print(titles)
17/148:
titles = tags_and_movies['title']

print(titles)
17/149:
titles[] = tags_and_movies['title']

print(titles)
17/150:
titles = tags_and_movies['title'].tolist()

print(titles)
17/151:
titles = tags_and_movies['title'].collect()

print(titles)
17/152: titles = tags_and_movies['title'].collect()
17/153: titles = tags_and_movies.select('title').collect()
17/154:
titles = tags_and_movies.select('title').collect()

print(titles)
17/155:
titles = tags_and_movies.select('title').collect()

print(titles[2])
17/156:
titles = tags_and_movies.select('title').collect()

print(titles[1])
17/157:
titles = tags_and_movies.select('title').collect()

print(titles[132234])
17/158:
titles = tags_and_movies.select('title').collect()

print(titles[13223422])
17/159:
titles = tags_and_movies.select('title').collect()

print(titles[132356])
17/160:
titles = tags_and_movies.select('title').collect()

for x in titles:
    print(x)
17/161:
titles = movies.select('title').collect()

print(movies)
17/162:
titles = movies.select('title').collect()

print(titles)
17/163:
titles = movies.select('title').collect()

for x in titles:
    int s
    for s in x.split() if s.isdigit()
17/164:
titles = movies.select('title').collect()

for x in titles:
    int(s) for s in x.split() if s.isdigit()
17/165:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
17/166:
titles = movies.select('title').collect()

for x in titles:
    re.findall(r'\d+',x)
17/167:
titles = movies.select('title').collect()

for x in titles:
    print(x)
17/168:
titles = movies.select('title').collect()

for x in titles:
    val movie = row.getString(x)
17/169:
titles = movies.select('title').collect()

for x in titles:
    movie = row.getString(x)
21/1:
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
21/2:
spark = SparkSession \
    .builder \
    .appName('Project Vaseis') \
    .getOrCreate()
21/3: spark
21/4:
df = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )
21/5: df.printSchema()
21/6: df.show(3)
21/7:
movies = spark.read.option("header", "true").csv("movie.csv")
movies.show(5)
21/8:
ratings = spark.read.option("header", "true").csv("rating.csv")
ratings.show(5)
21/9:
from pyspark.sql.functions import *

most_popular = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc("num_ratings"))
21/10: most_popular.show(10)
21/11:
most_popular_movies = most_popular.join(movies, most_popular.movieId == movies.movieId)
most_popular_movies.show(20, truncate=False)
21/12:
import pyspark.sql.functions as F
import pandas as pd

seen_jumanji = ratings\
.filter(F.col("title") == "Jumanji (1995)").count()
21/13:
titles = movies.select('title').collect()

for x in titles:
#     movie = row.getString(x)
    print(x)
21/14:
# titles = movies.select('title').collect()

titles = movies.select('title',substring('title'))


for x in titles:
#     movie = row.getString(x)
    print(x)
21/15:
# titles = movies.select('title').collect()

titles = movies.select('title',substring('title',1,10))


for x in titles:
#     movie = row.getString(x)
    print(x)
21/16:
# titles = movies.select('title').collect()

titles = movies.select('title',substring('title',1,10))
titles.show()

for x in titles:
#     movie = row.getString(x)
    print(x)
21/17:
# titles = movies.select('title').collect()

titles = movies.select('title',substring('title',1,10))

stringtitles = []

for x in titles:
    stringtitles = x.toString()
21/18:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('title',1,4))



for x in titles:
    stringtitles = x.toString()
21/19:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('title',1,4))
21/20:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('title',1,4))
21/21:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('timestamp',1,4))
21/22:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('timestamp',1,4))


years.show()
21/23:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('timestamp',1,4).distinct())


years.show()
21/24:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('timestamp',1,4)).distinct()


years.show()
21/25:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('timestamp',1,4)).distinct()


years.show()
21/26:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('timestamp',1,4).collect())


years.show()
21/27:
# titles = movies.select('title').collect()

years = ratings.select('timestamp',substring('timestamp',1,4)).collect()


years.show()
21/28:
ratings_per_movie = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc("num_ratings"))
21/29:
ratings_per_movie = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc("num_ratings"))

print(ratings_per_movie)
21/30:
ratings_per_movie = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc("num_ratings"))

ratings_per_movie.show()
21/31:
ratings_per_movie = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc("movieId"))

ratings_per_movie.show()
21/32:
ratings_per_movie = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(asc("movieId"))

ratings_per_movie.show()
21/33:
ratings_per_movie = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(asc("movieId"))

ratings_per_movie.show(22000)
21/34:
ratings_per_movie = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")

ratings_per_movie.show(22000)
21/35:
ratings_per_movie = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")

ratings_per_movie.show(220000)
21/36:
#QUERY 6

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")


ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)
21/37:
top_rated = ratings\
.groupBy("movieId")\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))
21/38:
top_rated = ratings\
.groupBy("movieId")\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated.show()
21/39:
top_rated = ratings\
.groupBy("movieId","timestamp")\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated.show()
21/40:
top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated.show()
21/41:
top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)

top_rated.show()
21/42:
top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(max(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)

top_rated.show()
21/43:
top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(F.max(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)

top_rated.show()
21/44:
top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(F.max(col("rating")))\
.withColumnRenamed("max(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)

top_rated.show()
21/45:
top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)
21/46:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy(ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)
21/47:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy(ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))


top_rated.show()
21/48:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)
21/49:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()
21/50:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()

final = top_rated\
.groupBy("movieId","substring(timestamp, 1, 4)")\
.agg(max(col("avg_rating")))
21/51:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)



final = top_rated\
.groupBy("movieId","substring(timestamp, 1, 4)")\
.agg(max(col("avg_rating")))

final.show()
21/52:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)



final = top_rated\
.groupBy("substring(timestamp, 1, 4)")\
.agg(max(col("avg_rating")))

final.show()
21/53:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)



final = top_rated\
.groupBy("substring(timestamp, 1, 4)")\
.agg(max(col("avg_rating")))

final.show(50)
21/54:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)



final = top_rated\
.groupBy("substring(timestamp, 1, 4)")\
.agg(max(col("avg_rating")))
.sort(desc("substring(timestamp, 1, 4)"))

final.show(50)
21/55:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)



final = top_rated\
.groupBy("substring(timestamp, 1, 4)")\
.agg(max(col("avg_rating")))\
.sort(desc("substring(timestamp, 1, 4)"))

final.show(50)
21/56:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)




# final = top_rated\
# .groupBy("substring(timestamp, 1, 4)")\
# .agg(max(col("avg_rating")))\
# .sort(desc("substring(timestamp, 1, 4)"))

# final.show(50)
21/57:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()


# final = top_rated\
# .groupBy("substring(timestamp, 1, 4)")\
# .agg(max(col("avg_rating")))\
# .sort(desc("substring(timestamp, 1, 4)"))

# final.show(50)
21/58:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating","substring(timestamp, 1, 4)","year")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()


# final = top_rated\
# .groupBy("substring(timestamp, 1, 4)")\
# .agg(max(col("avg_rating")))\
# .sort(desc("substring(timestamp, 1, 4)"))

# final.show(50)
21/59:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("kapp"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()


# final = top_rated\
# .groupBy("substring(timestamp, 1, 4)")\
# .agg(max(col("avg_rating")))\
# .sort(desc("substring(timestamp, 1, 4)"))

# final.show(50)
21/60:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()


# final = top_rated\
# .groupBy("substring(timestamp, 1, 4)")\
# .agg(max(col("avg_rating")))\
# .sort(desc("substring(timestamp, 1, 4)"))

# final.show(50)
21/61:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()


final = top_rated\
.groupBy("Year")\
.sort(desc("Year")).limit(10)

final.show(50)
21/62:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated.count().filter("`count` >= 10").orderBy('count', ascending=False)
21/63:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated.count().filter("`count` >= 10").orderBy('count', ascending=False)

top_rated.show()
21/64:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated.orderBy(top_rated.Year.desc())
21/65:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated.orderBy(top_rated.Year.desc())

top_rated.show()
21/66:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


final = top_rated\
.groupBy("Year")\

final.show(50)
21/67:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


final = top_rated\
.groupBy("Year")\

final.show(50)
21/68:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating")).limit(10)

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)
21/69:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating")).limit(10)

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()
21/70:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()
21/71:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)



final = top_rated\
.groupBy("Year")\
.agg(max(col("rating")))\
.sort(desc("avg_rating"))

final.show()
21/72:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)



final = top_rated\
.groupBy("Year")\
.agg(max(col("avg_rating")))\
.sort(desc("avg_rating"))

final.show()
21/73:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()

final = top_rated\
.groupBy("Year")\
.agg(max(col("avg_rating")))\
.sort(desc("avg_rating"))

final.show()
21/74:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

top_rated.show()

final = top_rated\
.groupBy("Year")\
.agg(F.max(col("avg_rating")))\
.sort(desc("avg_rating"))

final.show()
21/75:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)



final = top_rated\
.groupBy("Year")\
.agg(F.max(col("avg_rating")))\
.sort(desc("avg_rating"))

final.show()
21/76:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)


window = WindowpartitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())
21/77:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
21/78:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)


window = WindowpartitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())
21/79:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
21/80:
spark = SparkSession \
    .builder \
    .appName('Project Vaseis') \
    .getOrCreate()
21/81: spark
21/82:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
21/83:
spark = SparkSession \
    .builder \
    .appName('Project Vaseis') \
    .getOrCreate()
21/84: spark
21/85:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)


window = WindowpartitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())
21/86:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)


window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())
21/87:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
top_rated_movies.show(10)


window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select('*',rank().over(window).alias(rank)).filter(col(rank) <= 2).show()
21/88:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select('*',rank().over(window).alias(rank)).filter(col(rank) <= 2).show()
21/89:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select('*', rank().over(window).alias('rank')) 
  .filter(col('rank') <= 2) 
  .show()
21/90:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select('*', rank().over(window).alias('rank'))\
.filter(col('rank') <= 2)\
.show()
21/91:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select('*', rank().over(window).alias('rank'))\
.filter(col('rank') <= 2)\
.show(1500)
21/92:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select('*', rank().over(window).alias('rank'))\
.filter(col('rank') = 10)\
.show(1500)
21/93:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select('*', rank().over(window).alias('rank'))\
.filter(col('rank') == 10)\
.show(1500)
21/94:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

window.show()

# top_rated.select('*', rank().over(window).alias('rank'))\
# .filter(col('rank') == 10)\
# .show(1500)
21/95:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

print(window)

# top_rated.select('*', rank().over(window).alias('rank'))\
# .filter(col('rank') == 10)\
# .show(1500)
21/96:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select(col('*'), row_number().over(window)\
.where(col('row_number') <= 10)




# top_rated.select('*', rank().over(window).alias('rank'))\
# .filter(col('rank') == 10)\
# .show(1500)
21/97:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select(col('*'), row_number().over(window)\
.where(col('row_number') <= 10))




# top_rated.select('*', rank().over(window).alias('rank'))\
# .filter(col('rank') == 10)\
# .show(1500)
21/98:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select(col('*'), row_number().over(window)\
.where(col('row_number') <= 10) \
.limit(20) \
.toPandas()




# top_rated.select('*', rank().over(window).alias('rank'))\
# .filter(col('rank') == 10)\
# .show(1500)
21/99:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

window = Window.partitionBy(top_rated["Year"]).orderBy(top_rated['avg_rating'].desc())

top_rated.select(col('*'), row_number().over(window)\
.where(col('row_number') <= 10) \
.limit(20) \
.toPandas())




# top_rated.select('*', rank().over(window).alias('rank'))\
# .filter(col('rank') == 10)\
# .show(1500)
21/100:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=1):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, [group_by_columns],[order_by_columns], 1
21/101:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=1):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, [group_by_columns],[order_by_columns], 1)
21/102:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=1):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, ["Year"],["avg_rating"], 1)
21/103:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=1):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 1)
21/104:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=1):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 1)

top_n_df.show()
21/105:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=1):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 1)

top_n_df.show(100)
21/106:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 1)

top_n_df.show(100)
21/107:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

top_n_df.show(100)
21/108:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

top_n_df.show(1000)
21/109:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated_movies, top_rated_movies["Year"],top_rated_movies["avg_rating"], 10)

top_n_df.show(1000)
21/110:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

top_n_df.show(1000)
21/111:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

top_n_df.show(1000)
21/112:
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)

from pyspark.sql import Window
import pyspark.sql.functions as f

def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

top_n_df.show(1000)
21/113:
tags = tag\
.select("movieId","tag")\
.where(F.col(ratings['timestamp'].substr(1,4).alias("Year")) == '2015')
21/114:
#READING

genome_tags = spark.read.option("header", "true").csv("genome_tags.csv")
tag = spark.read.option("header", "true").csv("tag.csv")
genome_scores = spark.read.option("header", "true").csv("genome_scores.csv")
link = spark.read.option("header", "true").csv("link.csv")
21/115:
tags = tag\
.select("movieId","tag")\
.where(F.col(ratings['timestamp'].substr(1,4).alias("Year")) == '2015')
21/116:
tags = tag\
.select("movieId","tag")\
.where(ratings['timestamp'].substr(1,4).alias("Year") == '2015')
21/117:
tags = tag\
.select("movieId","tag")\
.where(col(ratings['timestamp'].substr(1,4).alias("Year") == '2015'))
21/118:
seen_jumanji = merged\
.filter(F.col(ratings['timestamp'].substr(1,4).alias("Year") == '2015'))
21/119:
kappa = tag\
.filter(F.col(ratings['timestamp'].substr(1,4).alias("Year") == '2015'))
21/120:
kappa = tag\
.filter(F.col(ratings['timestamp'].substr(1,4) == '2015'))
21/121:
kappa = tag\
.filter(F.col(tag['timestamp'].substr(1,4) == '2015'))
21/122:
kappa = tag\
.filter(F.col['timestamp'].substr(1,4) == '2015'))
21/123:
kappa = tag\
.filter(F.col['timestamp'].substr(1,4) == '2015')
21/124:
kappa = tag\
.filter(F.col('timestamp'].substr(1,4)) == '2015')
21/125:
kappa = tag\
.filter(F.col('timestamp').substr(1,4) == '2015')
21/126:
kappa = tag\
.filter(F.col('timestamp').substr(1,4) == '2015')

kappa.show()
21/127:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015'))
21/128:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')
21/129:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags.show()
21/130:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')\
.grouby("movieId")

tags.show()
21/131:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')\


tags.show()
21/132:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')\

finalg = tags\
.groupby("movieId")

finalg.show()
21/133:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')\

finalg = tags\
.orderby("movieId")

finalg.show()
21/134:
tags = tag\
.groupby("movieId")
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')\


finalg.show()
21/135:
tags = tag\
.groupby("movieId")\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')\


finalg.show()
21/136:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')\


finalg.show()
21/137:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')
21/138:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags.show()
21/139:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, tags.movieId == movies.movieId)

tags.show()
21/140:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, tags.movieId == movies.movieId)

tags_and_movies.show()
21/141:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on['movieId']
tags_and_movies.show()
21/142:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId']
tags_and_movies.show()
21/143:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])
tags_and_movies.show()
21/144:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

dataCollect() = tags.collect()

for row in dataCollect:
    print(row['tag'])
21/145:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    print(row['tag'])
21/146:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    id = row['movieId']
21/147:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    id = row['movieId']
21/148:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags.show()

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    id = row['movieId']
21/149:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

kappa =[][]

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    id = row['movieId']
21/150:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
Matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    id = row['movieId']
21/151:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    id = row['movieId']
21/152:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    id = row['movieId']
    matrix[id][tag]
21/153:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    id = row['movieId']
    matrix[id][row] = row['tag']
21/154:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    id = row['movieId']
    matrix[row][row] = row['tag']
21/155:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    print(row)
    id = row['movieId']
21/156:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
for row in dataCollect:
    x=1
    id = row['movieId']
    matrix[x][x] = row['tag']
21/157:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
x=1
for row in dataCollect:
    id = row['movieId']
    matrix[x][x] = row['tag']
21/158:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
x=1

id = row['movieId']
matrix[x][x] = row['tag']
for row+1 in dataCollect:
21/159:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

w, h = 8, 5;
matrix = [[0 for x in range(w)] for y in range(h)] 

dataCollect=tags.rdd.toLocalIterator()
x=1

id = row['movieId']
matrix[x][x] = row['tag']
for row+1 in dataCollect:
    print(kappa)
21/160:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()



# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/161:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

finalg = tags_and_movies\
.groupby('movieId')

# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/162:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

finalg = tags_and_movies\
.groupby('movieId')

finalg.show(10)
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/163:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

finalg = tags_and_movies\
.groupby('movieId')\
.count()

finalg.show(10)
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/164:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

finalg = tags_and_movies\
.orderby('movieId')\


finalg.show(10)
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/165:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()

finalg = tags_and_movies\
.groupby('movieId')\


finalg.show(10)
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/166:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()


tags_and_movies = tags_and_movies.sort(col("movieId").desc())

finalg.show(10)
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/167:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()


tags_and_movies = tags_and_movies.sort(col("movieId").desc())

tags_and_movies.show()
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/168:
tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()


tags_and_movies = tags_and_movies.sort(col("movieId").asc())

tags_and_movies.show()
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/169:
most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))
21/170:
most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("counts", "counts")\
.sort(desc("counts"))
21/171:
most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))
21/172:
most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))

most_ratings.show()
21/173:
most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)
21/174:
most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

top_n_df = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


top_n_df.show(1000)
21/175:
#QUERY 7

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


final_most_ratings(1000)
21/176:
#QUERY 7

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


final_most_ratings(1000)
21/177:
#QUERY 4

from pyspark.sql import Window
import pyspark.sql.functions as f


# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show(1000)
21/178:
#QUERY 7

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


final_most_ratings.show(1000)
21/179: genres = movies.withColumn("genres",explode(split("genres","[|]")))
21/180:
genres = movies.withColumn("genres",explode(split("genres","[|]")))

genres.show()
21/181:
genres = movies.withColumn("genres",explode(split("genres","[|]")))

genres.show(200000)
21/182:
genres = movies.withColumn("genres",explode(split("genres","[|]")))

genres.show(400000)
21/183:
genres = movies.withColumn("genres",explode(split("genres","[|]")))

genres_and_ratings = genres.join(ratings,on=['movieId'], how='left_outer')
21/184:
genres = movies.withColumn("genres",explode(split("genres","[|]")))

genres_and_ratings = genres.join(ratings,on=['movieId'], how='left_outer')

genres_and_ratings.show(20000)
21/185:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


genres.show(20000)
21/186:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


genres.show(20000)
21/187:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))
21/188:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))

FinalGenres.show(50000)
21/189:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

FinalGenres.show(50000)
21/190:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

FinalGenres.show(50000)
21/191:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

FinalGenres.show(50000)
21/192:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

FinalGenres.show(50000)
21/193:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

TheFinalGenres = FinalGenres\
.groupby("genres")\
.agg(max(col("rating")))

FinalGenres.show(50000)
21/194:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

TheFinalGenres = FinalGenres\
.groupby("genres")\
.agg(max(col("num_ratings")))

FinalGenres.show(50000)
21/195:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

TheFinalGenres = FinalGenres\
.groupby("genres")\
.agg(max(col("num_ratings")))

TheFinalGenres.show()
21/196:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))

TheFinalGenres.show()
21/197:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))

TheFinalGenres.show(2000
21/198:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))

TheFinalGenres.show(2000)
21/199:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","max_num_rating")\
.sort(desc("max_num_rating")


TheFinalGenres.show(2000)
21/200:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","max_num_rating")\
.sort(desc("max_num_rating")


TheFinalGenres.show(2000)
21/201:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","max_num_rating")\
.sort(desc("max_num_rating"))
      


TheFinalGenres.show(2000)
21/202:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","max_num_rating")\
.sort(desc("max_num_rating"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["max_num_rating"], 2)



TheFinalGenres.show(2000)
21/203:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","max_num_rating")\
.sort(desc("max_num_rating"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["max_num_rating"], 2)


topN_df.show(200)
21/204:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","max_num_rating")\
.sort(desc("max_num_rating"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["max_num_rating"], 2)


final_most_ratings_genres.show(200)
21/205:


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show(200)
21/206: tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')
21/207:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

tags_and_ratings.show()
21/208:
tags_and_ratings = ratings.join(tag,on=["userId"], how='left_outer')

tags_and_ratings.show()
21/209:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

tags_and_ratings.show()
21/210:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

tags_and_ratings.show(20000)
21/211:
bollywood= tags_and_ratings\
.select('moveiId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3.5))).distinct()
21/212:
bollywood= tags_and_ratings\
.select('movieId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3.5))).distinct()
21/213:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId")\
.where((F.col('tag').contains("funny") & (F.col('rating') > 3)))

df1.show(1000)
21/214:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag').contains("funny") & (F.col('rating') > 3)))

df1.show(1000)
21/215:
bollywood= tags_and_ratings\
.select('movieId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3.5))).dropDuplicates()
21/216:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag').contains("funny") & (F.col('rating') > 3))).dropDuplicates()

df1.show(1000)
21/217:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny" & (F.col('rating') > 3))).dropDuplicates()

df1.show(1000)
21/218:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3))).dropDuplicates()

df1.show(1000)
21/219:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()

df1.show(1000)
21/220:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag","userId")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()

df1.show(1000)
21/221:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer').dropDuplicates()

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3))

df1.show(1000)
21/222:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()

df1.show(1000)
21/223:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')
21/224:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')

movies_funny.show()
21/225:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')

movies_funny.show(10000)
21/226:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')

movies_funny.show(10000)

df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))
21/227:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

df2.show()
21/228:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

df2.show(10000)
21/229:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groypby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "counts")\
.sort(desc("counts"))
21/230:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "counts")\
.sort(desc("counts"))
21/231:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "counts")\
.sort(desc("counts"))

final_df2.show(1000)
21/232:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "counts")\
.sort(desc("counts"))

final_df.show(1000)
21/233:
tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)
21/234:
#QUERY 10

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3))

movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)
21/235:
#QUERY 10

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)
21/236:
df4 = tag\
.groupby("timestamp")\.
.agg(count(col("userId")))\.
.withColumnRenamed("count(timestamp)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/237:
df4 = tag\
.groupby("timestamp")\
.agg(count(col("userId")))\
.withColumnRenamed("count(timestamp)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/238:
df4 = tag\
.groupby("timestamp")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/239:
df4 = tag\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/240:
df4 = tag\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.sort(desc("number of users")).distinct()

df4.show(100)
21/241:
df4 = tag\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/242:
df4 = tag\
.groupby("timestamp")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/243:
df4 = ratings\
.groupby("timestamp")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/244:
df4 = ratings\
.groupby("timestamp")\
.agg(count(col("movieId")))\
.withColumnRenamed("count(userId)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/245:
df4 = ratings\
.groupby("timestamp")\
.agg(count(col("movieId")))\
.withColumnRenamed("count(movieId)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/246:
df4 = ratings\
.groupby("timestamp","userId")\
.agg(count(col("movieId")))\
.withColumnRenamed("count(movieId)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/247:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.sort(desc("number of users"))

df4.show(100)
21/248:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.sort(desc("number of users"))

df4.show(10000)
21/249:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(col["number of users"] != 1)
.sort(desc("number of users"))

df4.show(10000)
21/250:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(col["number of users"] != 1)\
.sort(desc("number of users"))

df4.show(10000)
21/251:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col["number of users"] != 1)\
.sort(desc("number of users"))

df4.show(10000)
21/252:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df4.show(10000)
21/253:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum(number of user))
21/254:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))
21/255:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))

print(df5)
21/256:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))

df5.show()
21/257:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))
.withColumnRenamed("sum(number of users)", "numbers of users watching the same movie the same time")\


df5.show()
21/258:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))
.withColumnRenamed("sum(number of users)", "number of movies")\


df5.show()
21/259:
df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()
21/260:
#QUERY 7

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


final_most_ratings.show(1000)
21/261:
#QUERY 5

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()


tags_and_movies = tags_and_movies.sort(col("movieId").asc())

tags_and_movies.show()
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
21/262:
#QUERY 8

movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show(200)
24/1:
import pyspark.sql.functions as F
import pandas as pd

seen_jumanji = ratings\
.filter(F.col("title") == "Jumanji (1995)").count()
24/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
25/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics.begin()
25/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
25/3:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
25/4: spark
25/5:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
25/6:
#QUERY 1

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)
25/7:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
25/8:
#QUERY 1

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)
25/9:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
25/10:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
25/11:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
25/12:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
25/13:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
25/14:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
25/15:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show()


stagemetrics.end()
stagemetrics.print_report()
25/16:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
25/17:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show()


stagemetrics.end()
stagemetrics.print_report()
25/18:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show()


stagemetrics.end()
stagemetrics.print_report()
25/19:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies.show()


tags_and_movies = tags_and_movies.sort(col("movieId").asc())

tags_and_movies.show()
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
stagemetrics.end()
stagemetrics.print_report()
25/20:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")


ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
25/21:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


final_most_ratings.show(1000)

stagemetrics.end()
stagemetrics.print_report()
25/22:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


final_most_ratings.show(1000)

stagemetrics.end()
stagemetrics.print_report()
25/23:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show(200)
.
stagemetrics.end()
stagemetrics.print_report()
25/24:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
25/25:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
25/26:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show()


stagemetrics.end()
stagemetrics.print_report()
25/27:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)


top_rated = ratings\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))

top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show()


stagemetrics.end()
stagemetrics.print_report()
25/28:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join("movies",on=['movieId'], how='left_outer')

top_rated = ratings\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))





def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show()


stagemetrics.end()
stagemetrics.print_report()
25/29:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join("movies",on=['movieId'], how='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))





def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show()


stagemetrics.end()
stagemetrics.print_report()
25/30:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join("movies",on=['movieId'], how='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))





def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show()


stagemetrics.end()
stagemetrics.print_report()
25/31:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)

final_top_rated.show()


stagemetrics.end()
stagemetrics.print_report()
25/32:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.select("title","Year","avg_rating")\
.where(col("Year") == 2005)

results.show()

stagemetrics.end()
stagemetrics.print_report()
25/33:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.select("title","Year","avg_rating")\
.where(col("Year") == 2005)

results.show()

stagemetrics.end()
stagemetrics.print_report()
25/34:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("movieId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.select("movieId","Year","avg_rating")\
.where(col("Year") == 2005)

results.show()

stagemetrics.end()
stagemetrics.print_report()
25/35:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.select("title","Year","avg_rating")\
.where(col("Year") == 2005)

results.show()

stagemetrics.end()
stagemetrics.print_report()
25/36:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
25/37:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
25/38:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
25/39:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
25/40:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.select("title","Year","avg_rating")\
.where(col("Year") == 2005)

results.show()

stagemetrics.end()
stagemetrics.print_report()
25/41:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies.sort(col("movieId").asc())

tags_and_movies.show()
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
stagemetrics.end()
stagemetrics.print_report()
25/42:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies.sort(col("movieId").asc())

tags_and_movies.show()
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
stagemetrics.end()
stagemetrics.print_report()
25/43:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")


ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
25/44:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


final_most_ratings.show(1000)

stagemetrics.end()
stagemetrics.print_report()
25/45:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show(200)

stagemetrics.end()
stagemetrics.print_report()
25/46:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)



stagemetrics.end()
stagemetrics.print_report()
25/47:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)



stagemetrics.end()
stagemetrics.print_report()
25/48:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show(200)

stagemetrics.end()
stagemetrics.print_report()
25/49:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
25/50:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
25/51:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
25/52:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
25/53:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
25/54:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
26/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
26/2: spark
26/3: spark
26/4:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
26/5:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
26/6:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
26/7:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
26/8:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.select("title","Year","avg_rating")\
.where(col("Year") == 2005)

results.show()

stagemetrics.end()
stagemetrics.print_report()
26/9:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies.sort(col("movieId").asc())

tags_and_movies.show()
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
stagemetrics.end()
stagemetrics.print_report()
26/10:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")


ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
26/11:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


final_most_ratings.show(1000)

stagemetrics.end()
stagemetrics.print_report()
26/12:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
26/13:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
26/14:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
26/15: spark
28/1:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.select("title","Year","avg_rating")\
.where(col("Year") == 2005).asc()

results.show()

stagemetrics.end()
stagemetrics.print_report()
28/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
28/3:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.select("title","Year","avg_rating")\
.where(col("Year") == 2005).asc()

results.show()

stagemetrics.end()
stagemetrics.print_report()
28/4:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.filter(F.col("Year") == "2005")

results.show()

stagemetrics.end()
stagemetrics.print_report()
28/5:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["title"], 10)



results = final_top_rated\
.filter(F.col("Year") == "2005")

results.show()

stagemetrics.end()
stagemetrics.print_report()
28/6:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.filter(F.col("Year") == "2005")
.orderBy(movie_df['title']).asc()

results.show()

stagemetrics.end()
stagemetrics.print_report()
28/7:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(movie_df['title']).asc()

results.show()

stagemetrics.end()
stagemetrics.print_report()
28/8:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(col['title']).asc()

results.show()

stagemetrics.end()
stagemetrics.print_report()
28/9:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col['title']).asc()

results.show()

stagemetrics.end()
stagemetrics.print_report()
28/10:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.filter(F.col("Year") == "2005")\
.sort(desc("title"))

results.show()

stagemetrics.end()
stagemetrics.print_report()
28/11:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.filter(F.col("Year") == "2005")\


results.show()

stagemetrics.end()
stagemetrics.print_report()
28/12:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.groupby("title")
.filter(F.col("Year") == "2005")\


results.show()

stagemetrics.end()
stagemetrics.print_report()
28/13:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)



results = final_top_rated\
.groupby("title")\
.filter(F.col("Year") == "2005")\


results.show()

stagemetrics.end()
stagemetrics.print_report()
28/14:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)


final_top_rated = final_top_rated\
.orderby("title")

results = final_top_rated\
.filter(F.col("Year") == "2005")\


results.show()

stagemetrics.end()
stagemetrics.print_report()
28/15:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
28/16: spark
29/1: spark
29/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
29/3:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
30/1:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
30/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
30/3:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
30/4:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).distinct()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
30/5:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5))


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
30/6:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
31/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Album Data Example') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
31/2: spark
31/3:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2') \
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
31/4:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")


ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
31/5:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)


final_most_ratings.show(1000)

stagemetrics.end()
stagemetrics.print_report()
31/6:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\


stagemetrics.end()
stagemetrics.print_report()
31/7:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def get_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = get_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 2)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
31/8:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\

results.show()

stagemetrics.end()
stagemetrics.print_report()
31/9:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n=2):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 3)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
31/10:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 3)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
31/11:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2') \
.master("spark://antonis:7077")
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
31/12:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2') \
.master("spark://antonis:7077")\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
31/13:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
31/14:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2') \
.master("spark://antonis:7077")\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
31/15:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
31/16: spark
31/17:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2') \
.master("spark://antonis:7077")\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
32/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2') \
.master("spark://antonis:7077")\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
32/2:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
32/3: spark
32/4:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.master("spark://antonis:7077")\
.config("spark.jars","<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
32/5:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.master("spark://antonis:7077")\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
32/6:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.master("spark://antonis:7077")\
.config("spark.jars","<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
32/7:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.master("spark://antonis:7077")\
.config("spark.jars","<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )

spark
32/8:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.master("spark://antonis:7077")\
.config("spark.jars","<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )

spark
32/9:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
from pyspark import SparkContext , SparkConf
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.master("spark://antonis:7077")\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
32/10: spark
33/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.master("spark://antonis:7077")\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
33/2:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
33/3:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
33/4:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
33/5:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\


results.show()

stagemetrics.end()
stagemetrics.print_report()
33/6:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies.sort(col("movieId").asc())

tags_and_movies.show()
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
stagemetrics.end()
stagemetrics.print_report()
33/7:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")


ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
33/8:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\

results.show()

stagemetrics.end()
stagemetrics.print_report()
33/9:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 3)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
33/10:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
33/11:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
33/12: spark
35/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.master("spark://antonis:7077")\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
35/2: spark
35/3:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.master("spark://antonis:7077")\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
36/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
36/2:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
36/3:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 3)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
36/4:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
36/5: spark
36/6:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
36/7:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\

results.show()

stagemetrics.end()
stagemetrics.print_report()
37/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
37/2:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
37/3:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/4:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
37/5:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\


results.show()

stagemetrics.end()
stagemetrics.print_report()
37/6:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies.sort(col("movieId").asc())

tags_and_movies.show()
# w, h = 8, 5;
# matrix = [[0 for x in range(w)] for y in range(h)] 

# dataCollect=tags.rdd.toLocalIterator()
# x=1

# for row in dataCollect:
#     for i in range(27278):
stagemetrics.end()
stagemetrics.print_report()
37/7:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")


ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/8:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.sort(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\

results.show()

stagemetrics.end()
stagemetrics.print_report()
37/9:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 3)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
37/10:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
37/11:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(desc("number of movies"))

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
37/12: spark
37/13:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(<title>.asc())

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/14:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(title.asc())

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/15:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(F.col('title').asc())

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/16:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(F.col('title').asc()).distinct()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/17:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(F.col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/18:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/19:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/20:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/21:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/22:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/23:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['               title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/24:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/25:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
37/26:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.orderby(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
37/27:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
37/28:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")


ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/29:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")
.sort(desc("num_ratings"))



ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/30:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc("num_ratings"))



ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/31:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\

results.show()

stagemetrics.end()
stagemetrics.print_report()
37/32:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc("num_ratings"))



ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/33:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.orderby(desc("num_ratings"))



ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/34:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.(desc(col("num_ratings")))



ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/35:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\
.sort(desc(col("num_ratings")))



ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/36:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\

ratings_count = ratings_count\
.orderby(desc("num_ratings"))



ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/37:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\

ratings_count = ratings_count\
.sort(desc("num_ratings"))



ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)
ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/38:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/39:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\

results.show()

stagemetrics.end()
stagemetrics.print_report()
37/40:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
37/41:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)


final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
37/42:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.orderby(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
37/43:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
37/44:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
37/45:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
37/46:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
37/47:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
37/48:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
37/49:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
37/50:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
37/51:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
37/52:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
37/53:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
37/54:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
37/55:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
37/56:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
37/57:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
38/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
38/2: spark
38/3:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
38/4:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
38/5:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
38/6:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
38/7:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
38/8:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
38/9:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
38/10:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
38/11:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
38/12:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
38/13:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
38/14:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
38/15:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
38/16:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
38/17:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
38/18:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
38/19:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
38/20:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
38/21:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
38/22:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
38/23:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
38/24:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
38/25:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
39/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
39/2: spark
39/3:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
39/4:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
39/5:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
39/6:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
39/7:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
39/8:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
39/9:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
39/10:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
39/11:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
39/12:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
39/13:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
39/14:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
39/15:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
39/16:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
39/17:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
39/18:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
39/19:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
39/20:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
39/21:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
39/22:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
39/23:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
39/24:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
39/25:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
39/26:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
39/27:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
39/28:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
39/29:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
40/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
40/2: spark
40/3:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
40/4:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
40/5:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
40/6:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
40/7:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
40/8:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
40/9:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
40/10:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
41/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
41/2:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
41/3:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
41/4:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
41/5:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
42/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
42/2: spark
42/3:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
42/4:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
42/5:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
42/6:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
42/7:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
42/8:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
42/9:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
42/10:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
42/11:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
42/12:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
42/13:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
42/14:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
42/15:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
42/16:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
42/17:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
42/18:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
42/19:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
42/20:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
42/21:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
42/22:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
42/23:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
42/24:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show(1000)

stagemetrics.end()
stagemetrics.print_report()
42/25:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show(10000)

stagemetrics.end()
stagemetrics.print_report()
42/26:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("movieId").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
42/27:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
42/28:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
42/29:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df5.show()

df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
42/30:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df4.show()

df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
42/31:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df4.show(1000)

df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
43/1:
from pyspark.sql import SparkSession
import json
42/32:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users")).dropDuplicates()



df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
43/2: host = http://150.140.142.92:8998
43/3: host = 'http://150.140.142.92:8998'
44/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
44/2:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby(ratings['timestamp'].substr(1,12),"movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/3:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby(ratings['timestamp'].substr(1,12),"movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df4.show()

df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/4:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby(ratings['timestamp'].substr(1,12),"movieId")\
.agg(count(col("userId").dropDuplicates()))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df4.show()

df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/5:
#QUERY 9

stagemetrics.begin()

rating = ratings\
.groupby("userId",ratings['timestamp'].substr(1,12).alias("timestamp")).dropDuplicates()



df4 = ratings\
.groupby(ratings['timestamp'].substr(1,12),"movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df4.show()

df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/6:
#QUERY 9

stagemetrics.begin()

rating = ratings\
.groupby("userId",ratings['timestamp'].substr(1,12).alias("timestamp")).distinct()



df4 = ratings\
.groupby(ratings['timestamp'].substr(1,12),"movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df4.show()

df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/7:
#QUERY 9

stagemetrics.begin()

rating = ratings\
.groupby("userId",ratings['timestamp'].substr(1,12).alias("timestamp"))

rating.show()



df4 = ratings\
.groupby(ratings['timestamp'].substr(1,12),"movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df4.show()

df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/8:
#QUERY 9

stagemetrics.begin()

ratings = ratings\
.groupby("userId",ratings['timestamp'].substr(1,12).alias("timestamp"))

ratings.show()



df4 = ratings\
.groupby(ratings['timestamp'].substr(1,12),"movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))

df4.show()

df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/9:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/10:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/11:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/12:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/13:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
44/14:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
44/15:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
44/16:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
44/17:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
44/18:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
44/19:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
44/20:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
44/21:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
44/22:
#QUERY 9

stagemetrics.begin()

df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/23:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show()

stagemetrics.end()
stagemetrics.print_report()
44/24: spark
44/25:
#QUERY 9

stagemetrics.begin()

ratings = ratings\
.groupby("timestamp","userId").distinct()


df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
44/26:
#QUERY 9

stagemetrics.begin()




df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
46/1:
from pyspark.sql import SparkSession
import json
import requests
46/2:
LIVY_HOST = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy'}

resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))

if resp.status_code == requests.codes.created:
    session_id = resp.json()['id']
else:
    raise CustomError()
46/3: /sessions/{session_id}/state
46/4: directive = f'/sessions/{session_id}/statements'
46/5:
directive = f'/sessions/{session_id}/statements'
resp = request.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))
46/6:
from pyspark.sql import SparkSession
import json
import requests
46/7:
LIVY_HOST = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy'}

resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))

if resp.status_code == requests.codes.created:
    session_id = resp.json()['id']
else:
    raise CustomError()
46/8: directive = f'/sessions/{session_id}/statements'
46/9:
from pyspark.sql import SparkSession
import json
import requests
46/10:
LIVY_HOST = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy'}

resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))

if resp.status_code == requests.codes.created:
    session_id = resp.json()['id']
else:
    raise CustomError()
46/11:
LIVY_HOST = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy'}

resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))

if resp.status_code == requests.codes.created:
    session_id = resp.json()['id']
else:
    print('kappa')
46/12:
LIVY_HOST = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy'}

resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))

if resp.status_code == requests.codes.created:
    session_id = resp.json()['id']
else:
    print('kappa')
46/13:
from pyspark.sql import SparkSession
import json
import requests
46/14:
LIVY_HOST = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy'}

resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))

if resp.status_code == requests.codes.created:
    session_id = resp.json()['id']
else:
    print('kappa')
46/15:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
46/16:
LIVY_HOST = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy'}

resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))

if resp.status_code == requests.codes.created:
    session_id = resp.json()['id']
else:
    print('kappa')
46/17: directive = f'/sessions/{session_id}/statements'
46/18:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

resp = requests.post(LIVY_HOST+directive, headers=headers, data=json.dumps(data))

if resp.status_code == requests.codes.created:
    session_id = resp.json()['id']
else:
    print('kappa')
46/19:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))

if resp.status_code == requests.codes.created:
    session_id = resp.json()['id']
else:
    print('kappa')
46/20:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'first-livy','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
46/21:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'livy','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
46/22:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'gamw to ceid','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
47/1:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
47/2:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'agapaw to ceid <3','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
47/3: /sessions/{291}/statements/{statement_id}/cancel
47/4:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}
47/5:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
47/6:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()
47/7:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

f resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
                if state in ('waiting','running'):
                    time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    raise CustomException()
                else:
                    break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/8:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
                if state in ('waiting','running'):
                    time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    raise CustomException()
                else:
                    break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/9:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
                if state in ('waiting','running'):
                    time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    raise CustomException()
                else:
                    break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/10:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
        state = info_resp.json()['state']
                if state in ('waiting','running'):
                    time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    raise CustomException()
                else:
                    break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/11:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
        state = info_resp.json()['state']
            if state in ('waiting','running'):
                    time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    raise CustomException()
                else:
                    break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/13:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
        state = info_resp.json()['state']
            if state in ('waiting','running'):
                time.sleep(2)
            elif state in ('cancelling','cancelled','error'):
                    aise CustomException()
            else:
                break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/14:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
                if state in ('waiting','running'):
                    time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    raise CustomException()
                else:
                    break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/15:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

 if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
                if state in ('waiting','running'):
                    time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    raise CustomException()
                else:
                    break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/16:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
                if state in ('waiting','running'):
                    time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    raise CustomException()
                else:
                    break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/17:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

    while True:
        info_resp = requests.get(LIVY_HOST+f'/sessions/{session_id}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
                if state in ('waiting','running'):
                    time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    raise CustomException()
                else:
                    break
        else:
            raise CustomException()
    print(info_resp.json()['output'])

else:
  #something went wrong with creation
  raise CustomException()
47/18:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
47/19:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']

resp.json()
47/20: resp.json()
47/21:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
    
    while True:
        info_resp = requests.get(host+f'/sessions/{session_id}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json(['state'])
            if state in ('waiting','runnin'):
                time.sleep(2)
                elif state in ('cancelling','cancelled','error'):
                    print("kappa")
                else:
                    break
        else:
            print("kappa")
    print(info_resp.json()['output'])
else:
    print("kappa")
47/22:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
    
    while True:
        info_resp = requests.get(host+f'/sessions/{session_id}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json(['state'])
            if state in ('waiting','runnin'):
                time.sleep(2)
                else state in ('cancelling','cancelled','error'):
                    print("cancelled")
                
        else:
            print("kappa")
    print(info_resp.json()['output'])
else:
    print("kappa")
47/23:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
    
    while True:
        info_resp = requests.get(host+f'/sessions/{session_id}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json(['state'])
            if state in ('waiting','runnin'):
                time.sleep(2)
                else if state in ('cancelling','cancelled','error'):
                    print("cancelled")
                
        else:
            print("kappa")
    print(info_resp.json()['output'])
else:
    print("kappa")
47/24:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
    
    while True:
        info_resp = requests.get(host+f'/sessions/{session_id}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json(['state'])
            if state in ('waiting','runnin'):
                time.sleep(2) 
        else:
            print("kappa")
    print(info_resp.json()['output'])
else:
    print("kappa")
47/25:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
    
    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json(['state'])
            if state in ('waiting','runnin'):
                time.sleep(2) 
        else:
            print("kappa")
    print(info_resp.json()['output'])
else:
    print("kappa")
47/26:

code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
    
    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
            if state in ('waiting','runnin'):
                time.sleep(2) 
        else:
            print("kappa")
    print(info_resp.json()['output'])
else:
    print("kappa")
47/27:
import time
code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))
if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
    
    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
            if state in ('waiting','runnin'):
                time.sleep(2) 
        else:
            print("kappa")
    print(info_resp.json()['output'])
else:
    print("kappa")
48/1:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
48/2: spark
48/3:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'U CAN DO IT BRO','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
48/4:
import time
code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{291}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))



if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
    
    while True:
        info_resp = requests.get(host+f'/sessions/{291}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
            if state in ('waiting','runnin'):
                time.sleep(2) 
        else:
            print("kappa")
    print(info_resp.json()['output'])
else:
    print("kappa")
49/1:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
49/2: spark
49/3:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'cmon','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
49/4:
import time
code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{297}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))



if resp.status_code == requests.codes.created:
    stmt_id = resp.json()['id']
    
    while True:
        info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
        if info_resp.status_code == requests.codes.ok:
            state = info_resp.json()['state']
            if state in ('waiting','runnin'):
                time.sleep(2) 
        else:
            print("kappa")
    print(info_resp.json()['output'])
else:
    print("kappa")
49/5:
import time
code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{297}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
49/6:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'what','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
49/7:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/ui/session/298'
requests.delete(session_url, headers=headers)
49/8:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/ui/session/301'
requests.delete(session_url, headers=headers)
49/9:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
49/10:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'what','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
49/11:
import time
code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{297}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
49/12:
directive = f'/sessions/{291}/statements/{0}/cancel'
data = {'code':'...'}

resp = request.post(host+directive, headers=headers, data=json.dumps(data))

resp.json()
49/13: spark
49/14:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/ui/session/301'
requests.delete(session_url, headers=headers)
50/1:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
50/2:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'the','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
50/3:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/ui/session/304'
requests.delete(session_url, headers=headers)
51/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
51/2:
stagemetrics.begin()
    
    rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
51/3:
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
51/4:
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
51/5:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
51/6:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')

stagemetrics.end()

stagemetrics.print_report()
51/7:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')

stagemetrics.end()

stagemetrics.print_report()
51/8:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')

stagemetrics.end()

stagemetrics.print_report()
51/9:
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
51/10:
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
51/11:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
51/12:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
51/13:
from pyspark.sql.functions import year, concat_ws, collect_list
stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

stegemetrics.end()
stagemetrics.print_report()
51/14:
from pyspark.sql.functions import year, concat_ws, collect_list
stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

stegemetrics.end()
stagemetrics.print_report()
51/15:
from pyspark.sql.functions import year, concat_ws, collect_list

stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

stagemetrics.end()
stagemetrics.print_report()
51/16:
from pyspark.sql.functions import year, concat_ws, collect_list

stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

stagemetrics.end()
stagemetrics.print_report()
51/17:
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stegemetrics.end()
stagemetrics.print_report()
51/18:
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
51/19:
#6
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
51/20:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')

stegemetrics.end()
stagemetrics.print_report()
51/21:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')

stagemetrics.end()
stagemetrics.print_report()
51/22:
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)

stagemetrics.end()
stagemetrics.print_report()
51/23:
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)

stagemetrics.end()
stagemetrics.print_report()
51/24:
#9
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).agg(sum(q9_df['count']))

stegemetrics.end()
stagemetrics.print_report()
51/25:
#9
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).agg(sum(q9_df['count']))

stagemetrics.end()
stagemetrics.print_report()
51/26:
#9
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).agg(sum(q9_df['count']))

stagemetrics.end()
stagemetrics.print_report()
51/27:
#9
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).agg(sum(q9_df['count']))

stagemetrics.end()
stagemetrics.print_report()
51/28:
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
q10_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q10_df = q10_df.join(tag_df,['movieId'],'left')
q10_df = q10_df.select('*',lower(col('tag')).alias('tag')).filter(q10_df["tag"] == "funny")
q10_df = q10_df.join(rating_df,['movieId'],'left')
q10_df = q10_df.filter(q10_df['rating'] >= 3.5).select("*")
q10_df = q10_df.dropDuplicates(['movieId'])
q10_df = q10_df.groupBy(q10_df['genres']).count().orderBy(q10_df['genres'].asc())

stagemetrics.end()
stagemetrics.print_report()
51/29:
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)

q8_df.show()

stagemetrics.end()
stagemetrics.print_report()
51/30:
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)

q8_df.show()

stagemetrics.end()
stagemetrics.print_report()
51/31:
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)

q8_df.show()

stagemetrics.end()
stagemetrics.print_report()
52/1:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
52/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
52/3:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
52/4:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
51/32:
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
51/33:
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
51/34:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])

boring_movie_names.show()

stagemetrics.end()

stagemetrics.print_report()
52/5:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(boring['title'].asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
51/35:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])

boring_movies.show()

boring_movie_names.show()

stagemetrics.end()

stagemetrics.print_report()
52/6:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
52/7:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
51/36:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')

stagemetrics.end()

stagemetrics.print_report()
51/37:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')

user_bollywood_good_rating.show()
stagemetrics.end()

stagemetrics.print_report()
52/8:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()

bollywood = bollywood\.
.orderby(col('userId').asc())
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
52/9:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()

bollywood = bollywood\
.orderby(col('userId').asc())
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
52/10:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct().asc()

          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
52/11:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()

          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
53/1:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
53/2:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'the','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
53/3:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
53/4:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'fak','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
53/5:
import time
code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{7}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
53/6:
import time
code = textwrap.dedent("""df = spark.createDataFrame(list(range(1,1000)),'int')
df.groupBy().sum().collect()[0]['sum(value)']""")

code_packed = {'code':code}

directive = f'/sessions/{6}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
53/7:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'tsifsaaa','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
53/8:
import time
code = textwrap.dedent(df = spark.read.csv("/home/administrator/Downloads/movielens/genome_scores.csv"))

code_packed = {'code':code}

directive = f'/sessions/{11}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
53/9:
import time
code = textwrap.dedent("""df = spark.read.csv("/home/administrator/Downloads/movielens/genome_scores.csv")""")

code_packed = {'code':code}

directive = f'/sessions/{11}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
53/10:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'arga tha pesei h douleia','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
53/11:
import time
code = textwrap.dedent("""df = spark.read.csv("/home/administrator/Downloads/movielens/genome_scores.csv")""")

code_packed = {'code':code}

directive = f'/sessions/{12}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
53/12:
import time
code = textwrap.dedent("""df = spark.read.csv("/home/administrator/Downloads/movielens/genome_scores.csv")""")

code_packed = {'code':code}

directive = f'/sessions/{12}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
53/13:
code = textwrap.dedent("""df.show()")
""")

code_packed = {'code':code}

directive = f'/sessions/{12}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()
53/14:
code = textwrap.dedent("""df.show())
""")

code_packed = {'code':code}

directive = f'/sessions/{12}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()
53/15:
code = textwrap.dedent("""df = spark.read.csv("/home/administrator/Downloads/movielens/genome_scores.csv""")

code_packed = {'code':code}

directive = f'/sessions/{12}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()
53/16:
code = textwrap.dedent("""df.show()""")

code_packed = {'code':code}

directive = f'/sessions/{12}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()
53/17:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'POIOS MOU SVHNEI TA SESSION?','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
53/18:
import time
code = textwrap.dedent("""df = spark.read.csv("/home/administrator/Downloads/movielens/rating.csv")
""")

code_packed = {'code':code}

directive = f'/sessions/{19}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
53/19:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/ui/session/19'
requests.delete(session_url, headers=headers)
54/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
54/2: spark
55/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
55/2: spark
55/3:
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
55/4:
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)



stagemetrics.end()
stagemetrics.print_report()
55/5:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
55/6:
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
55/7:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])



stagemetrics.end()

stagemetrics.print_report()
55/8:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')


stagemetrics.end()

stagemetrics.print_report()
55/9:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
55/10:
from pyspark.sql.functions import year, concat_ws, collect_list

stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

stagemetrics.end()
stagemetrics.print_report()
55/11:
#6
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
55/12:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')

stagemetrics.end()
stagemetrics.print_report()
55/13:
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)



stagemetrics.end()
stagemetrics.print_report()
55/14:
#9
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).agg(sum(q9_df['count']))

stagemetrics.end()
stagemetrics.print_report()
55/15:
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
q10_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q10_df = q10_df.join(tag_df,['movieId'],'left')
q10_df = q10_df.select('*',lower(col('tag')).alias('tag')).filter(q10_df["tag"] == "funny")
q10_df = q10_df.join(rating_df,['movieId'],'left')
q10_df = q10_df.filter(q10_df['rating'] >= 3.5).select("*")
q10_df = q10_df.dropDuplicates(['movieId'])
q10_df = q10_df.groupBy(q10_df['genres']).count().orderBy(q10_df['genres'].asc())

stagemetrics.end()
stagemetrics.print_report()
55/16: spark
56/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
57/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
57/2:
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
57/3:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])



stagemetrics.end()

stagemetrics.print_report()
57/4:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')


stagemetrics.end()

stagemetrics.print_report()
57/5:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
57/6:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
57/7:
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
57/8:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])



stagemetrics.end()

stagemetrics.print_report()
57/9:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')


stagemetrics.end()

stagemetrics.print_report()
57/10:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
57/11:
from pyspark.sql.functions import year, concat_ws, collect_list

stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

stagemetrics.end()
stagemetrics.print_report()
57/12:
#6
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
57/13:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')

stagemetrics.end()
stagemetrics.print_report()
57/14:
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)



stagemetrics.end()
stagemetrics.print_report()
57/15:
#9
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).agg(sum(q9_df['count']))

stagemetrics.end()
stagemetrics.print_report()
57/16:
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
q10_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q10_df = q10_df.join(tag_df,['movieId'],'left')
q10_df = q10_df.select('*',lower(col('tag')).alias('tag')).filter(q10_df["tag"] == "funny")
q10_df = q10_df.join(rating_df,['movieId'],'left')
q10_df = q10_df.filter(q10_df['rating'] >= 3.5).select("*")
q10_df = q10_df.dropDuplicates(['movieId'])
q10_df = q10_df.groupBy(q10_df['genres']).count().orderBy(q10_df['genres'].asc())

stagemetrics.end()
stagemetrics.print_report()
57/17: spark
58/1:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
58/2:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'POIOS MOU SVHNEI TA SESSION?','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

# if resp.status_code == requests.codes.created:
#     session_id = resp.json()['id']
# else:
#     print('kappa')
58/3:
import time
code = textwrap.dedent("""df = spark.read.csv("/home/administrator/Downloads/movielens/rating.csv")
""")

code_packed = {'code':code}

directive = f'/sessions/{19}/statements'
resp = requests.post(host+directive, headers=headers, data=json.dumps(code_packed))

resp.json()

# if resp.status_code == requests.codes.created:
#     stmt_id = resp.json()['id']
    
#     while True:
#         info_resp = requests.get(host+f'/sessions/{297}/statements/{stmt_id}')
#         if info_resp.status_code == requests.codes.ok:
#             state = info_resp.json()['state']
#             if state in ('waiting','runnin'):
#                 time.sleep(2) 
#         else:
#             print("kappa")
#     print(info_resp.json()['output'])
# else:
#     print("kappa")
59/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
59/2: spark
59/3:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
60/1:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')

q7_df.show()

stagemetrics.end()
stagemetrics.print_report()
60/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
61/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
61/2: spark
61/3:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')

stagemetrics.end()
stagemetrics.print_report()
61/4:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')

q7_df.show()

stagemetrics.end()
stagemetrics.print_report()
61/5:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10 & col('year') == 1995).select('userId')

q7_df.show()

stagemetrics.end()
stagemetrics.print_report()
61/6:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')

q7_df.show()

stagemetrics.end()
stagemetrics.print_report()
61/7:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId').where(col('year') == '1995')

q7_df.show()

stagemetrics.end()
stagemetrics.print_report()
61/8:
#8
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)



stagemetrics.end()
stagemetrics.print_report()
61/9:
#8
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)

q8_df.show()

stagemetrics.end()
stagemetrics.print_report()
62/1:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
62/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
62/3:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
62/4:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(F.col('tag').contains("boring"))

boring = boring\
.orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
61/10:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])



stagemetrics.end()

stagemetrics.print_report()
61/11:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])

boring_movie_names.show()


stagemetrics.end()

stagemetrics.print_report()
62/5:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.filter(tags_and_movies('tag').contains("boring"))

boring = boring\
.orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/6:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.filter(tags_and_movies('tag').contains("boring")).asc().dropDuplicates()

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/7:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()

          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
62/8:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.filter(tags_and_movies('tag').contains("boring")).asc().dropDuplicates()

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/9:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.groupby('title')\
.filter(tags_and_movies('tag').contains("boring")).asc().dropDuplicates()

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/10:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.filter(tags_and_movies('tag').contains("boring")).asc().dropDuplicates()

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/11:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.filter(tags_and_movies['tag'].contains("boring")).asc().dropDuplicates()

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/12:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.filter(tags_and_movies['tag'].contains("boring")).dropDuplicates()

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/13:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.filter(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/14:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.filter(tags_and_movies['tag'].contains("boring")).dropDuplicates().asc()

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/15:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates().asc()

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/16:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

# boring = boring\
# .orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/17:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title').asc()).dropDuplicates()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/18:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title')

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/19:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title')

# boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/20:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title'))

# boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/21:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title'))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/22:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title')).asc()

boring.show()

stagemetrics.end()
stagemetrics.print_report()
62/23:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title'))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
61/12:
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])

boring_movie_names.show()


stagemetrics.end()

stagemetrics.print_report()
61/13:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')

user_bollywood_good_rating.show()

stagemetrics.end()

stagemetrics.print_report()
63/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
63/2: spark
63/3:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stegemetrics.end()
stagemetrics.print_report()
63/4:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stagemetrics.end()
stagemetrics.print_report()
63/5:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stagemetrics.end()
stagemetrics.print_report()
63/6:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stagemetrics.end()
stagemetrics.print_report()
64/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
63/7:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stagemetrics.end()
stagemetrics.print_report()
65/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
65/2: spark
65/3:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
65/4:
#1
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
65/5:
#2
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])

boring_movie_names.show()


stagemetrics.end()

stagemetrics.print_report()
65/6:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')


stagemetrics.end()
stagemetrics.print_report()
65/7:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
65/8:
#1
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
65/9:
#2
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])




stagemetrics.end()

stagemetrics.print_report()
65/10:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')



stagemetrics.end()

stagemetrics.print_report()
65/11:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
65/12:
#5
from pyspark.sql.functions import year, concat_ws, collect_list

stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

stagemetrics.end()
stagemetrics.print_report()
65/13:
#6
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
65/14:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')


stagemetrics.end()
stagemetrics.print_report()
65/15:
#8
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)


stagemetrics.end()
stagemetrics.print_report()
65/16:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stagemetrics.end()
stagemetrics.print_report()
65/17:
#10
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
q10_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q10_df = q10_df.join(tag_df,['movieId'],'left')
q10_df = q10_df.select('*',lower(col('tag')).alias('tag')).filter(q10_df["tag"] == "funny")
q10_df = q10_df.join(rating_df,['movieId'],'left')
q10_df = q10_df.filter(q10_df['rating'] >= 3.5).select("*")
q10_df = q10_df.dropDuplicates(['movieId'])
q10_df = q10_df.groupBy(q10_df['genres']).count().orderBy(q10_df['genres'].asc())

stagemetrics.end()
stagemetrics.print_report()
65/18: spark
66/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
66/2: spark
66/3:
#1
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
66/4:
#2
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])




stagemetrics.end()

stagemetrics.print_report()
66/5:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
66/6:
#1
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
66/7:
#2
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])




stagemetrics.end()

stagemetrics.print_report()
66/8:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')



stagemetrics.end()

stagemetrics.print_report()
66/9:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
66/10:
#5
from pyspark.sql.functions import year, concat_ws, collect_list

stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

stagemetrics.end()
stagemetrics.print_report()
66/11:
#6
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
66/12:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')


stagemetrics.end()
stagemetrics.print_report()
66/13:
#8
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)


stagemetrics.end()
stagemetrics.print_report()
66/14:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stagemetrics.end()
stagemetrics.print_report()
66/15:
#10
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
q10_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q10_df = q10_df.join(tag_df,['movieId'],'left')
q10_df = q10_df.select('*',lower(col('tag')).alias('tag')).filter(q10_df["tag"] == "funny")
q10_df = q10_df.join(rating_df,['movieId'],'left')
q10_df = q10_df.filter(q10_df['rating'] >= 3.5).select("*")
q10_df = q10_df.dropDuplicates(['movieId'])
q10_df = q10_df.groupBy(q10_df['genres']).count().orderBy(q10_df['genres'].asc())

stagemetrics.end()
stagemetrics.print_report()
67/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
67/2: spark
67/3:
#1
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
67/4:
#2
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])




stagemetrics.end()

stagemetrics.print_report()
67/5:
#1
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
67/6:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')



stagemetrics.end()

stagemetrics.print_report()
67/7:
#6
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
67/8:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')


stagemetrics.end()
stagemetrics.print_report()
67/9:
#8
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)


stagemetrics.end()
stagemetrics.print_report()
67/10:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stagemetrics.end()
stagemetrics.print_report()
67/11:
#10
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
q10_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q10_df = q10_df.join(tag_df,['movieId'],'left')
q10_df = q10_df.select('*',lower(col('tag')).alias('tag')).filter(q10_df["tag"] == "funny")
q10_df = q10_df.join(rating_df,['movieId'],'left')
q10_df = q10_df.filter(q10_df['rating'] >= 3.5).select("*")
q10_df = q10_df.dropDuplicates(['movieId'])
q10_df = q10_df.groupBy(q10_df['genres']).count().orderBy(q10_df['genres'].asc())

stagemetrics.end()
stagemetrics.print_report()
67/12: spark
67/13:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
67/14:
#1
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
67/15:
#2
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])




stagemetrics.end()

stagemetrics.print_report()
67/16:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')



stagemetrics.end()

stagemetrics.print_report()
67/17:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
67/18:
#5
from pyspark.sql.functions import year, concat_ws, collect_list

stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

stagemetrics.end()
stagemetrics.print_report()
67/19:
#6
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
67/20:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')


stagemetrics.end()
stagemetrics.print_report()
67/21:
#8
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)


stagemetrics.end()
stagemetrics.print_report()
67/22:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stagemetrics.end()
stagemetrics.print_report()
67/23:
#10
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
q10_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q10_df = q10_df.join(tag_df,['movieId'],'left')
q10_df = q10_df.select('*',lower(col('tag')).alias('tag')).filter(q10_df["tag"] == "funny")
q10_df = q10_df.join(rating_df,['movieId'],'left')
q10_df = q10_df.filter(q10_df['rating'] >= 3.5).select("*")
q10_df = q10_df.dropDuplicates(['movieId'])
q10_df = q10_df.groupBy(q10_df['genres']).count().orderBy(q10_df['genres'].asc())

stagemetrics.end()
stagemetrics.print_report()
67/24:
9.
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
stagemetrics.end()
stagemetrics.print_report()
69/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
69/2: spark
69/3:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title'))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
70/1:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')

user_bollywood_good_rating.show()


stagemetrics.end()

stagemetrics.print_report()
70/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
70/3:
#3
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(user_bollywood_good_rating["rating"]>3.0).select('userId')

user_bollywood_good_rating.show()


stagemetrics.end()

stagemetrics.print_report()
69/4:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').contains("bollywood") & (F.col('rating') > 3))).distinct()

          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
70/4:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])


stagemetrics.end()

stagemetrics.print_report()
69/5:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3)).distinct()

          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
69/6:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3)).distinct().asc()

          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
69/7:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3)).distinct()

bollywood = bollywood\
.orderby(bollywood["userId"])
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
69/8:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.filter((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3)).distinct()

bollywood = bollywood\
.orderby(bollywood["userId"])
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
69/9:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3)).distinct()

bollywood.sort_values(by='userId')
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
69/10:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3)).distinct()

bollywood.sort(by='userId')
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
69/11:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3)).distinct()

bollywood.sort('userId')
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
69/12:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
70/5:
#4
from pyspark.sql.functions import year, row_number, col
from pyspark.sql.window import Window

stagemetrics.begin()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating')
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).select(movie_df['title'])

q4_df.show()

stagemetrics.end()

stagemetrics.print_report()
69/13:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
70/6:
#5
from pyspark.sql.functions import year, concat_ws, collect_list

stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

tag_df_2015.show()

stagemetrics.end()
stagemetrics.print_report()
69/14:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc()).dropDuplicates()

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/15:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/16:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").dropDuplicates().asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/17:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").dropDuplicates().asc()

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/18:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title")).dropDuplicates().asc()

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/19:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/20:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3).asc()).distinct()

# bollywood.sort('userId')
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
69/21:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId']).dropDuplicates()


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/22:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId']).dropDuplicates()


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report(100)
69/23:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId']).dropDuplicates()


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/24:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId']).distinct()


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/25:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.orderby(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/26:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'], how = "left-outer")


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/27:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'], how = "outer")


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/28:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'], how = "inner")


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/29:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'], how = "innerd")


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/30:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'], how = "left_outer")


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/31:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'], how = "left")


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/32:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'], how = "right")


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/33:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
70/7:
#6
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
69/34:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tag_and_movies = tag2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(tag015.tag))\
.alias('tags of movie'))\
.orderBy(tag_df_2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/35:
#QUERY 5

stagemetrics.begin()

tags2015= tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tag_and_movies = tags2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(tags2015.tag))\
.alias('tags of movie'))\
.orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/36:
#QUERY 5

stagemetrics.begin()

tag = ratings.join(movies, on=['movieId'])

tags2015= tag\
.select("movieId","tag",)\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tag_and_movies = tags2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(tags2015.tag))\
.alias('tags of movie'))\
.orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/37:
#QUERY 5

stagemetrics.begin()

tags = ratings.join(movies, on=['movieId'])

tags2015= tag\
.select("movieId","tag",)\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tag_and_movies = tags2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(tags2015.tag))\
.alias('tags of movie'))\
.orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/38:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags2015 = tags.join(movies, on=['movieId'])


tags2015 = tags2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(tags2015.tag))\
.alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/39:
#QUERY 5

stagemetrics.begin()

tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags2015 = tags.join(movies, on=['movieId'])


tags2015 = tags2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(tags2015.tag))\
.alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/40:
#QUERY 5

stagemetrics.begin()

tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

# tags2015 = tags.join(movies, on=['movieId'])


# tags2015 = tags2015.groupBy('movieId','title')\
# .agg(concat_ws(", ", collect_list(tags2015.tag))\
# .alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/41:
tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')
69/42:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

# tags2015 = tags.join(movies, on=['movieId'])


# tags2015 = tags2015.groupBy('movieId','title')\
# .agg(concat_ws(", ", collect_list(tags2015.tag))\
# .alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/43:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags2015 = tags.join(movies, on=['movieId'])


tags2015 = tags2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(tags2015.tag))\
.alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/44:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags2015 = tags.join(movies, on=['movieId'])


tags2015 = tags2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(tags2015["tag"]))\
.alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/45:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags2015 = tags.join(movies, on=['movieId'])


tags2015 = tags2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(col('tag')))\
.alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/46:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags2015 = tags.join(movies, on=['movieId'])


tags2015 = tags2015.groupBy('movieId','title')\
.agg(concat_ws(", ", collect_list(col('tag')))\
.alias('tags')).orderBy(tags2015['movieId'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/47:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags2015 = tags.join(movies, on=['movieId'])

tags2015.show()

# tags2015 = tags2015.groupBy('movieId','title')\
# .agg(concat_ws(", ", collect_list(col('tag')))\
# .alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/48:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags2015.show()

tags2015 = tags.join(movies, on=['movieId'])



# tags2015 = tags2015.groupBy('movieId','title')\
# .agg(concat_ws(", ", collect_list(col('tag')))\
# .alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/49:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')



tags2015 = tags.join(movie_df.select('movieId', 'title'), on=['movieId'])



# tags2015 = tags2015.groupBy('movieId','title')\
# .agg(concat_ws(", ", collect_list(col('tag')))\
# .alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/50:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')



tags2015 = tags.join(movies.select('movieId', 'title'), on=['movieId'])



# tags2015 = tags2015.groupBy('movieId','title')\
# .agg(concat_ws(", ", collect_list(col('tag')))\
# .alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/51:
#QUERY 5

stagemetrics.begin()

tagd = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tagd\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')



tags2015 = tags.join(movies.select('movieId', 'title'), on=['movieId'])



# tags2015 = tags2015.groupBy('movieId','title')\
# .agg(concat_ws(", ", collect_list(col('tag')))\
# .alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/52:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag","title")\
.where(col('timestamp').substr(1,4) == '2015')



tags2015 = tags.join(movies.select('movieId', 'title'), on=['movieId'])



# tags2015 = tags2015.groupBy('movieId','title')\
# .agg(concat_ws(", ", collect_list(col('tag')))\
# .alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/53:
#QUERY 5

stagemetrics.begin()

tag = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)



tags2015 = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')



tags2015 = tags.join(movies.select('movieId', 'title'), on=['movieId'], how='left_outer')



# tags2015 = tags2015.groupBy('movieId','title')\
# .agg(concat_ws(", ", collect_list(col('tag')))\
# .alias('tags')).orderBy(tags2015['title'].asc())







# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags2015.show(100)

stagemetrics.end()
stagemetrics.print_report()
69/54:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/55:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies = tags_and_movies\
.groupby('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/56:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies = tags_and_movies\
.groupby('movieId','title').agg(concat_ws(", ", collect_list(col(tag)).orderBy(tags_and_movies['title'].asc())

# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/57:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies = tags_and_movies\
.groupby('movieId','title').agg(concat_ws(", ", collect_list(col(tag)).orderBy(tags_and_movies['title'].asc()))

# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/58:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies = tags_and_movies\
.groupby('movieId','title').agg(concat_ws(", ", collect_list(col(tag)).orderBy(tags_and_movies['title'].asc())

# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/59:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])

tags_and_movies = tags_and_movies\
.groupby('movieId','title').agg(concat_ws(", ", collect_list(col(tag)).orderBy(tags_and_movies['title'].asc())))

# tags_and_movies = tags_and_movies\
# .sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/60:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])
z

tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
69/61:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
70/8:
#5
from pyspark.sql.functions import year, concat_ws, collect_list

stagemetrics.begin()

movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())

tag_df_2015.show()

stagemetrics.end()
stagemetrics.print_report()
69/62:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
70/9:
#6
from pyspark.sql.functions import count

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc()).select(movie_df['title'])

stagemetrics.end()
stagemetrics.print_report()
69/63:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
70/10:
#7
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count

stagemetrics.begin()
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).select('userId')

q7_df.show()

stagemetrics.end()
stagemetrics.print_report()
69/64:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings_genres = final_most_ratings_genres\
.sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
70/11:
#8
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)


stagemetrics.end()
stagemetrics.print_report()
70/12:
#8
from pyspark.sql.window import Window
from pyspark.sql.functions import year, row_number, col, count, split, explode

stagemetrics.begin()
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)

q8_df = movie_df.withColumn('genres',explode(split('genres',"[^\w]"))) #seperates rows for each genre in a movie
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genres','title').count()
window = Window.partitionBy(q8_df['genres']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('r')).filter(col('r') <= 5)

q8_df.show()

stagemetrics.end()
stagemetrics.print_report()
69/65:
#QUERY 8

stagemetrics.begin()


movies_and_ratings = movies.join(ratings,on=['movieId'], how='left_outer')


genres = movies_and_ratings.withColumn("genres",explode(split("genres","[|]")))


FinalGenres = genres\
.groupby("genres","title")\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "num_ratings")\
.sort(desc("num_ratings"))

# FinalGenres.show(2000)

TheFinalGenres = FinalGenres\
.groupby("genres","title")\
.agg(max(col("num_ratings")))\
.withColumnRenamed("max(num_ratings)","ratings_num")\
.sort(desc("ratings_num"))

def retrieve_topN(df, group_by_columns, order_by_column, n):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df


final_most_ratings_genres = retrieve_topN(TheFinalGenres, TheFinalGenres["genres"],TheFinalGenres["ratings_num"], 1)

final_most_ratings.show()

# final_most_ratings_genres = final_most_ratings_genres\
# .sort(col('genres').asc())

final_most_ratings_genres.show()

stagemetrics.end()
stagemetrics.print_report()
69/66:
#QUERY 9

stagemetrics.begin()




df4 = ratings\
.groupby("timestamp","movieId")\
.agg(count(col("userId")))\
.withColumnRenamed("count(userId)", "number of users")\
.filter(F.col("number of users") != 1)\
.sort(desc("number of users"))


df5 = df4\
.select(sum("number of users"))



df5.show()

stagemetrics.end()
stagemetrics.print_report()
69/67:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show()

stagemetrics.end()
stagemetrics.print_report()
72/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
72/2:
H = np.array([[1, 2, 3, 4],
              [5, 6, 7, 8],
              [9, 10, 11, 12],
              [13, 14, 15, 16]])



fig = plt.figure(figsize=(6, 3.2))

ax = fig.add_subplot(111)
ax.set_title('colorMap')
plt.imshow(H)
ax.set_aspect('equal')

cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
cax.get_xaxis().set_visible(False)
cax.get_yaxis().set_visible(False)
cax.patch.set_alpha(0)
cax.set_frame_on(False)
plt.colorbar(orientation='vertical')
plt.show()
72/3:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
72/4:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
72/5:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
72/6:
H = np.array([[1, 2, 3, 4],
              [5, 6, 7, 8],
              [9, 10, 11, 12],
              [13, 14, 15, 16]])



fig = plt.figure(figsize=(6, 3.2))

ax = fig.add_subplot(111)
ax.set_title('colorMap')
plt.imshow(H)
ax.set_aspect('equal')

cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
cax.get_xaxis().set_visible(False)
cax.get_yaxis().set_visible(False)
cax.patch.set_alpha(0)
cax.set_frame_on(False)
plt.colorbar(orientation='vertical')
plt.show()
72/7:
H = np.array([[1, 2, 3, 4],
              [5, 6, 7, 8],
              [9, 10, 11, 12],
              [13, 14, 15, 16]])




fig = pyplot.figure()
plots = fig.add_subplot(1, 1, 1)


plots.plot(H[0], label='local')





# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/8:
H = np.array([[1, 2, 3, 4],
              [5, 6, 7, 8],
              [9, 10, 11, 12],
              [13, 14, 15, 16]])




fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)


plots.plot(H[0], label='local')





# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/9:
H = np.array([[1, 2, 3, 4],
              [5, 6, 7, 8],
              [9, 10, 11, 12],
              [13, 14, 15, 16]])




fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)


plots.plot(H[0], label='local')



plots.set_xlabel("machines")
plots.set_ylabel("time")

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/10:
H = np.array([[1],
              [5],
              [9],
              [13]])




fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)


plots.plot(H[0], label='local')



plots.set_xlabel("machines")
plots.set_ylabel("time")

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/11:
H = np.array([[1],
              [5],
              [9],
              [13]])




fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)


plots.plot(H[0], label='local')
plots.plot(H[1], label='local')



plots.set_xlabel("machines")
plots.set_ylabel("time")

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/12:
H = np.array([[1,2,3,4]])




fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)


plots.plot(H[0], label='local')
plots.plot(H[1], label='local')



plots.set_xlabel("machines")
plots.set_ylabel("time")

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/13:
data = {'local': ['2', '3', '4', '5'], 'Cluster 1': [6, 3, 2, 3]} 

df = pd.DataFrame(data)

print(df)



# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/14:
data = {'local': ['2', '3', '4', '5'], 'Cluster 1': [6, 3, 2, 3]\
       ,'local': ['2', '3', '4', '5'], 'Cluster 1': [6, 3, 2, 3]} 

df = pd.DataFrame(data)

print(df)



# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/15:
data = {'local': ['2', '3', '4', '5'], 'Cluster 1': [6, 3, 2, 3]\
       ,'locadsdl': ['2', '3', '4', '5'], 'Cdsa': [6, 3, 2, 3]} 

df = pd.DataFrame(data)

print(df)



# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/16:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)

print(df)



# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/17:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)



print(df)

fig = pyplot.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/18:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)



print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/19:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)
mean_values = []
for i in range(4):
    mean_values.append(df['local'].mean())
    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/20:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)
mean_values = []
for i in range(4):
    mean_values.append(df[i].mean())
    
    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/21:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)
mean_values = []
for i in range(4):
    mean_values.append(mean(df,axis=1))
    
    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/22:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)
mean_values = []
for i in range(4):
    mean_values.append(data.mean(df,axis=1))
    
    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/23:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)
mean_values = []
for i in range(4):
    mean_values.append(df.mean(df,axis=1))
    
    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/24:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)
mean_values = []
for i in range(4):
    mean_values.append(df.mean(df,axis=0))
    
    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/25:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)
mean_values = []
for i in range(4):
#     mean_values.append(df.mean(df,axis=0))
    print(i)
    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/26:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)
mean_values = []
for i in range(4):
    mean_values.append(df[0].mean())

    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/27:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

df = pd.DataFrame(data)
mean_values = []
for i in range(4):
    mean_values[i] = df[i].mean()

    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/28:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

avg_column = df.mean(axis=0)
    

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/29:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

avg_column = df.mean(axis=0)
print(avg_column)

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(df['local'].mean(), label='categorical_crossentropy')

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
72/30:
data = {'local': [1,23,44,55], 'Cluster 1': [6, 3, 2, 3]\
       ,'CLuster 2': [44,33,56,43], 'Cluster 3': [6, 3, 2, 3]} 

avg_column = df.mean(axis=0)
print(avg_column)

print(df)

fig = plt.figure()
plots = fig.add_subplot(1, 1, 1)

plots.plot(avg_column)

# fig = plt.figure(figsize=(6, 3.2))

# ax = fig.add_subplot(111)
# ax.set_title('colorMap')
# plt.imshow(H)
# ax.set_aspect('equal')

# cax = fig.add_axes([0.12, 0.1, 0.78, 0.8])
# cax.get_xaxis().set_visible(False)
# cax.get_yaxis().set_visible(False)
# cax.patch.set_alpha(0)
# cax.set_frame_on(False)
# plt.colorbar(orientation='vertical')
# plt.show()
74/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
74/2: spark
74/3:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
74/4: spark
75/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
75/2: spark
75/3:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.config("spark.driver.memory","16g")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
75/4:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.config("spark.driver.memory","16g")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
75/5:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.config("spark.driver.memory","16g")\
.config("spark.executor.memory","8g")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
75/6:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.config("spark.driver.memory","16g")\
.config("spark.executor.memory","8g")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
75/7:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.master("spark://antonis:7077")\
.config("spark.driver.memory","16g")\
.config("spark.executor.memory","8g")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
75/8:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
75/9:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title'))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
75/10:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3)).distinct()

bollywood.sort('userId')
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
75/11:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
75/12:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
75/13:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.master("spark://antonis:7077")\
.config("spark.driver.memory","16g")\
.config("spark.executor.memory","8g")\
.config("spark.executor.cores","6")\
.config("spark.driver.cores","12")
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
75/14:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.master("spark://antonis:7077")\
.config("spark.driver.memory","16g")\
.config("spark.executor.memory","8g")\
.config("spark.executor.cores","6")\
.config("spark.driver.cores","12")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
75/15:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
75/16:
#QUERY 2

stagemetrics.begin()

tags_and_movies= movies.join(tag, on=['movieId'], how='left_outer')

boring = tags_and_movies\
.select('title')\
.where(tags_and_movies['tag'].contains("boring")).dropDuplicates()

boring = boring\
.orderBy(col('title'))

boring.show()

stagemetrics.end()
stagemetrics.print_report()
75/17:
#QUERY 3

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=['userId'], how='left_outer')


# bollywood= tags_and_ratings\
# .select('userId')\
# .where(F.col('tag').contains("bollywood")).distinct()


bollywood= tags_and_ratings\
.select('userId')\
.where((F.col('tag').like("%bollywood") | F.col('tag').like("%BOLLYWOOD") | F.col('tag').like("%Bollywood")) & (F.col('rating') > 3)).distinct()

bollywood.sort('userId')
          
bollywood.show()

stagemetrics.end()
stagemetrics.print_report()
75/18:
#QUERY 4

stagemetrics.begin()
# top_rated = ratings\
# .groupBy("movieId",ratings['timestamp'].substr(1,4))\
# .agg(avg(col("rating")))\
# .withColumnRenamed("avg(rating)", "avg_rating")\
# .sort(desc("avg_rating"))

# top_rated_movies = top_rated.join(movies, top_rated.movieId == movies.movieId)
# top_rated_movies.show(10)

query4 = ratings.join(movies,on=['movieId'], how ='left_outer')

top_rated = query4\
.groupBy("title",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(avg(col("rating")))\
.withColumnRenamed("avg(rating)", "avg_rating")\
.sort(desc("avg_rating"))



def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_top_rated = get_topN(top_rated, top_rated["Year"],top_rated["avg_rating"], 10)




results = final_top_rated\
.filter(F.col("Year") == "2005")\
.orderBy(F.col("title").asc())


results.show()

stagemetrics.end()
stagemetrics.print_report()
77/1:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'query1','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

session_url = host+ r.headers['location']
77/2:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
77/3:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'query1','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

session_url = host+ r.headers['location']
77/4:
codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark'
    'code':textwrap.dedent(code)
}

statement_url = session_url + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
77/5:
codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark'
    'code': textwrap.dedent(code)
}

statement_url = session_url + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
77/6:
codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
77/7:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/ui/session/277'
requests.delete(session_url, headers=headers)
77/8:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
77/9:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'query2','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

session_url = host+ r.headers['location']
77/10:
codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
77/11:
codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
78/1:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
78/2:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/ui/session/278'
requests.delete(session_url, headers=headers)
78/3:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/session/278'
requests.delete(session_url, headers=headers)
78/4:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
78/5:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/session/278'
requests.delete(session_url, headers=headers)
78/6:
host = 'http://150.140.142.92:8998'

headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/session/278'
requests.delete(session_url, headers=headers)

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'query2','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

session_url = host+ r.headers['location']
78/7:
host = 'http://150.140.142.92:8998'

headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/session/278'
requests.delete(session_url, headers=headers)

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'query2','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

session_url = host+ r.headers['location']
78/8:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
78/9:
host = 'http://150.140.142.92:8998'

headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/session/278'
requests.delete(session_url, headers=headers)

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'query2','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

session_url = host+ r.headers['location']
78/10:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
78/11:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'query2','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

session_url = host+ r.headers['location']
80/1:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
80/2:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'query2','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

session_url = host+ r.headers['location']
80/3:
codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
80/4:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/ui/session/278'
requests.delete(session_url, headers=headers)
81/1:
from pyspark.sql import SparkSession
import json
import requests
import textwrap
81/2:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'query2','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host+directive, headers=headers, data=json.dumps(data))


r.json()

session_url = host+ r.headers['location']
80/5:
host = 'http://150.140.142.92:8998'

directive = '/sessions'
headers = {'Content-Type': 'application/json'}

data = {'kind':'pyspark','name':'attempt1','conf':{'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + directive, data=json.dumps(data), headers=headers)


r.json()
80/6:
codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
80/7:
state_url = "http://150.140.142.92:8998/ui/session/285" + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
80/8:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()

codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
80/9:


codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = "http://150.140.142.92:8998/ui/session/285" + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
80/10:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
80/11:


codefile = open('query1.py','r')
code = codefile.read()
codefile.close()
data={
    'kind':'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = "http://150.140.142.92:8998/session/285" + '/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
80/12:
r = requests.delete("http://150.140.142.92:8998/session/285", headers=headers)
pprint.pprint(r.json())
80/13:
r = requests.delete("http://150.140.142.92:8998/session/285", headers=headers)
r.json()
80/14:
import requests
headers = {'Content-Type': 'application/json'}
session_url = 'http://150.140.142.92:8998/session/285'
requests.delete(session_url, headers=headers)
84/1:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/2:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/3:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/4:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/5:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/6:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/7:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/8:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/9:
data = {'kind': 'pyspark',\
        'name':'query1' #,
        'conf':{
            'master':'spark://localhost:7077'}\
       }#, 'heartbeatTimeoutInSecond':240}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/10:
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/11:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/12:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/13:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/14:
data = {'kind': 'pyspark',\
        'name':'query1' #,
        'conf':{
            'master':'spark://localhost:7077'}\
       }#, 'heartbeatTimeoutInSecond':240}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/15:
data = {'kind': 'pyspark',\
        'name':'query1' #,
        'conf': {
            'master':'spark://administrator-OptiPlex-390:7077'}\
       }#, 'heartbeatTimeoutInSecond':240}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/16:
data = {'kind': 'pyspark',\
        'name':'query1' #,
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/17:
data = {'kind': 'pyspark',\
        'name':'query1',\
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/18:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/19:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/20:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/21:
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/22:
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/23:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/24:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/25:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/26:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/27:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/28:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/29:
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/30:
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/31:
#get last statement output
output = last_statement['output']
if output['status']=="error":
    pprint.pprint(output['traceback'])
else:
    fres = output['data']['text/plain'].replace("'","")
    #pprint.pprint(output['data']['text/plain'])
    fres = json.loads(fres)
print(fres[0]['author'])
84/32:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
84/33:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/34:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/35:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/36:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/37:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/38:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/39:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/40:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/41:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/42:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/43:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/44:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/45:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/46:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/47:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/48:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/49:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
84/50:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/51:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/52:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/53:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/54:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/55:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/56:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/57:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/58:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
84/59:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/60:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/61:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/62:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/63:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/64:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/65:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/66:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/67:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
84/68:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/69:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/70:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/71:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/72:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/73:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/74:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/75:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/76:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
85/1:
#QUERY 6

stagemetrics.begin()

ratings_count = ratings\
.groupBy("movieId")\
.agg(count("userId"))\
.withColumnRenamed("count(userId)", "num_ratings")\





ratings_per_movie = ratings_count.join(movies, ratings_count.movieId == movies.movieId)


ratings_per_movie = ratings_per_movie\
.sort(desc("num_ratings"))

ratings_per_movie.show(20, truncate=False)

stagemetrics.end()
stagemetrics.print_report()
85/2:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.master("spark://antonis:7077")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
84/77:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/78:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/79:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/80:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/81:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/82:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/83:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/84:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/85:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
84/86:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/87:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/88:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/89:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/90:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/91:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/92:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/93:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/94:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
84/95:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/96:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/97:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/98:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/99:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/100:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/101:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/102:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/103:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
84/104:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/105:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/106:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/107:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/108:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/109:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/110:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/111:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/112:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
84/113:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/114:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/115:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/116:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/117:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/118:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/119:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/120:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/121:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
84/122:
r = requests.delete(session_url, headers=headers)
pprint.pprint(r.json())
84/123:
import json, pprint, requests, textwrap, time
host = 'http://150.140.142.92:8998'
84/124:
#get number of sessions
headers = {'Content-Type': 'application/json'}
r = requests.get(host+'/sessions', headers=headers)
con_var=r.json()['total']
print ("Waiting... active sesssions: ", con_var)
while con_var!=0:
    r = requests.get(host+'/sessions', headers=headers)
    con_var=r.json()['total']
    print ("Waiting... active sesssions: ", con_var)
    time.sleep(60)
print ("Good to go!")
84/125:
sessionid = 285
r = requests.delete("http://150.140.142.92:8998/sessions/"+str(sessionid), headers={'Content-Type': 'application/json'})
pprint.pprint(r.json())
84/126:
data = {'kind': 'pyspark',\
        'name':'query1',
        'conf': {'master':'spark://administrator-OptiPlex-390:7077'}}

r = requests.post(host + '/sessions', data=json.dumps(data), headers=headers)
r.json()
84/127:
session_url = host + r.headers['location']
r = requests.get(session_url, headers=headers)
r.json()
84/128:
#show session state, repeat until idle to proceed with result extraction
state_url = session_url + '/state'
con_var=".."
while con_var!="idle":
    r = requests.get(state_url, headers=headers)
    con_var = r.json()['state']
    print("Job is %s" % r.json()['state'])
    time.sleep(10)
print ("Good to go!")
84/129:
#send some python statements for execution
codefile = open('query1.py', 'r')
code = codefile.read()
codefile.close()
data = {
    'kind': 'pyspark',
    'code': textwrap.dedent(code)
}

statement_url = session_url+'/statements'
r = requests.post(statement_url, data=json.dumps(data), headers=headers)
84/130:
#get stats from the latest statement in the session
statements_url = session_url + '/statements'
print(statements_url)
r = requests.get(statements_url, headers=headers)
slist = r.json()
last_statement = slist['statements'][slist['total_statements']-1]
if last_statement['progress']==1:
    print ("Job completed")

print("Total Time %s ms" % (last_statement['completed'] - last_statement['started']))
90/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.master("spark://antonis:7077")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
90/2:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show()

stagemetrics.end()
stagemetrics.print_report()
90/3:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(50)

stagemetrics.end()
stagemetrics.print_report()
90/4:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc().dropduplicates())

tags_and_movies.show(50)

stagemetrics.end()
stagemetrics.print_report()
90/5:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

final = tags_and_movies.dropDuplicates()

tags_and_movies.show(50)

stagemetrics.end()
stagemetrics.print_report()
90/6:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

final = tags_and_movies.dropDuplicates()

final.show(50)

stagemetrics.end()
stagemetrics.print_report()
90/7:
#QUERY 5

stagemetrics.begin()

tags = tag\
.select("movieId","tag")\
.where(col('timestamp').substr(1,4) == '2015')

tags_and_movies = tags.join(movies, on=['movieId'])


tags_and_movies = tags_and_movies\
.sort(col("title").asc())

tags_and_movies.show(50)

stagemetrics.end()
stagemetrics.print_report()
93/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.master("spark://antonis:7077")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
93/2:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
93/3:
#QUERY 7

stagemetrics.begin()

most_ratings = ratings\
.groupBy("userId",ratings['timestamp'].substr(1,4).alias("Year"))\
.agg(count(col("rating")))\
.withColumnRenamed("count(rating)", "counts")\
.orderBy(desc("counts"))


def get_topN(df, group_by_columns, order_by_column, n=10):
    window_group_by_columns = Window.partitionBy(group_by_columns)
    ordered_df = df.select(df.columns + [
        f.row_number().over(window_group_by_columns.orderBy(order_by_column.desc())).alias('row_rank')])
    topN_df = ordered_df.filter(f"row_rank <= {n}").drop("row_rank")
    return topN_df

final_most_ratings = get_topN(most_ratings, most_ratings["Year"],most_ratings["counts"], 10)

results = final_most_ratings\
.filter(F.col("Year") == "1995")\
.sort(col('userId').asc())

results.show()

stagemetrics.end()
stagemetrics.print_report()
93/4:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.master("spark://antonis:7077")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
94/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
94/2:
spark = SparkSession \
    .builder \
    .appName('Project Vaseis') \
    .getOrCreate()
94/3: spark
93/5: spark
93/6:
#QUERY 10

stagemetrics.begin()

tags_and_ratings = ratings.join(tag,on=["movieId"], how='left_outer')

df1 = tags_and_ratings\
.select("movieId","tag")\
.where((F.col('tag') == "funny") & (F.col('rating') > 3.5)).dropDuplicates()


movies_funny = df1.join(movies,on=["movieId"], how='left_outer')



df2 = movies_funny.withColumn("genres",explode(split("genres","[|]")))

final_df = df2\
.groupby("genres")\
.agg(count(col("title")))\
.withColumnRenamed("count(title)", "number of movies")\
.sort(col("genres").asc())

final_df.show()

stagemetrics.end()
stagemetrics.print_report()
95/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re
import matplotlib.pyplot as plt



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.master("spark://antonis:7077")\
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
95/2:
#QUERY 1

stagemetrics.begin()

merged = movies.join(ratings, on=['movieId'], how='left_outer')

seen_jumanji = merged\
.filter(F.col("title") == "Jumanji (1995)").count()

print(seen_jumanji)

stagemetrics.end()
stagemetrics.print_report()
100/1:
from pyspark.sql import SparkSession
from sparkmeasure import StageMetrics
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour, sum, concat_ws, collect_list, lower
from pyspark.sql.window import Window

spark = SparkSession \
    .builder \
    .appName('vaseis-2') \
    .config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
    .master("spark://antonis:7077") \
    .getOrCreate()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
101/1:
from pyspark.sql import SparkSession
from sparkmeasure import StageMetrics
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour, sum, concat_ws, collect_list, lower
from pyspark.sql.window import Window

spark = SparkSession \
    .builder \
    .appName('vaseis-2') \
    .config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
    .master("spark://antonis:7077") \
    .getOrCreate()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
101/2:
# QUERY 1
stagemetrics.begin()

movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID
q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji
print(q1)

stagemetrics.end()
stagemetrics.print_report()
101/3:
# QUERY 10
stagemetrics.begin()

q10_df = rating_df.join(tag_df, ['movieId'], 'inner')
q10_df = q10_df.select("movieId","tag").where((col('tag') == "funny") & (col('rating') > 3.5)).dropDuplicates()
q10_df = q10_df.join(movie_df, ['movieId'], 'inner')
q10_df = q10_df.withColumn("genre",explode(split("genres","[|]")))
q10_df = q10_df = q10_df.groupBy(q10_df['genre']).count().orderBy(q10_df['genre'].asc())
q10_df.show()

# stagemetrics.end()
# stagemetrics.print_report()
101/4:
# QUERY 1
stagemetrics.begin()

movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID
q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji
print(q1)

# stagemetrics.end()
# stagemetrics.print_report()
101/5:
# QUERY 1
# stagemetrics.begin()

movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID
q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji
print(q1)

# stagemetrics.end()
# stagemetrics.print_report()
101/6:
# QUERY 10
# stagemetrics.begin()

q10_df = rating_df.join(tag_df, ['movieId'], 'inner')
q10_df = q10_df.select("movieId","tag").where((col('tag') == "funny") & (col('rating') > 3.5)).dropDuplicates()
q10_df = q10_df.join(movie_df, ['movieId'], 'inner')
q10_df = q10_df.withColumn("genre",explode(split("genres","[|]")))
q10_df = q10_df = q10_df.groupBy(q10_df['genre']).count().orderBy(q10_df['genre'].asc())
q10_df.show()

# stagemetrics.end()
# stagemetrics.print_report()
101/7:
# QUERY 2
# stagemetrics.begin()

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])
boring_movie_names.show() #show the first 5

# stagemetrics.end()
# stagemetrics.print_report()
101/8:
# QUERY 2
# stagemetrics.begin()

boring_movies = tag.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])
boring_movie_names.show() #show the first 5

# stagemetrics.end()
# stagemetrics.print_report()
101/9:
# QUERY 2
# stagemetrics.begin()

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])
boring_movie_names.show() #show the first 5

# stagemetrics.end()
# stagemetrics.print_report()
101/10:
# QUERY 2
# stagemetrics.begin()

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])
boring_movie_names.show() #show the first 5

# stagemetrics.end()
# stagemetrics.print_report()
102/1:
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import col
from pyspark.sql.functions import from_unixtime, hour, dayofyear
from pyspark.sql.functions import isnull, when, count, col
from pyspark.sql.functions import rank, col
from pyspark.sql.functions import *
from pyspark.sql import Window
import pyspark.sql.functions as f
import pyspark.sql.functions as F
import pandas as pd
from sparkmeasure import StageMetrics
import numpy as np
import random
from pyspark.sql.functions import broadcast
import re



spark = SparkSession \
.builder \
.appName('Vaseis2')\
.config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
.getOrCreate()


stagemetrics = StageMetrics(spark)


movies = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("movie.csv") #filename to read from
     )

ratings = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("rating.csv") #filename to read from
     )


tag = (spark.read
      .format("csv")
      .option('header', 'true') #means that the first line contains column names
      .option("delimiter", ",") #set the delimiter to comma
      .option("inferSchema", "true") #automatically try to infer the column data types
      .load("tag.csv") #filename to read from
     )
102/2:
#1
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
102/3:
#2
stagemetrics.begin()

tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])




stagemetrics.end()

stagemetrics.print_report()
102/4:
#1
stagemetrics.begin()
    
rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID

q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji 
#output
# 22243

stagemetrics.end()
stagemetrics.print_report()
103/1:
# QUERY 2
# stagemetrics.begin()

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])
boring_movie_names.show() #show the first 5

# stagemetrics.end()
# stagemetrics.print_report()
103/2:
# QUERY 1
# stagemetrics.begin()

movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID
q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji
print(q1)

# stagemetrics.end()
# stagemetrics.print_report()
103/3:
from pyspark.sql import SparkSession
from sparkmeasure import StageMetrics
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour, sum, concat_ws, collect_list, lower
from pyspark.sql.window import Window

spark = SparkSession \
    .builder \
    .appName('vaseis-2') \
    .config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
    .master("spark://antonis:7077") \
    .getOrCreate()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
103/4:
# QUERY 1
# stagemetrics.begin()

movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID
q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji
print(q1)

# stagemetrics.end()
# stagemetrics.print_report()
103/5:
# QUERY 2
# stagemetrics.begin()

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])
boring_movie_names.show() #show the first 5

# stagemetrics.end()
# stagemetrics.print_report()
103/6:
# QUERY 3
# stagemetrics.begin()

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(col("rating")>3.0).dropDuplicates(['userId']).orderBy(col('userId')).select('userId')
user_bollywood_good_rating.show()


# stagemetrics.end()
# stagemetrics.print_report()
103/7:
# QUERY 4
# stagemetrics.begin()

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating') #.join(movie_df.select('movieId','title'),['movieId'])
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).filter(col('year') == 2005).select(col('title'))
q4_df.show()

# stagemetrics.end()
# stagemetrics.print_report()
103/8:
# QUERY 5
# stagemetrics.begin()

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())
tag_df_2015.show()

# stagemetrics.end()
# stagemetrics.print_report()
103/9:
# QUERY 6
# stagemetrics.begin()

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc())
q6_df.select(col('title'),col('count')).show()

# stagemetrics.end()
# stagemetrics.print_report()
103/10:
# QUERY 7
# stagemetrics.begin()

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10)
q7_df.filter(q7_df['year'] == 1995).select(col('userId')).orderBy(col('userId').asc()).show()

# stagemetrics.end()
# stagemetrics.print_report()
103/11:
# QUERY 8
# stagemetrics.begin()

q8_df = movie_df.withColumn('genre',explode(split('genres',"[|]")))
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genre','title').count()
window = Window.partitionBy(q8_df['genre']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('rank')).filter(col('rank') == 1).orderBy(col('genre').asc())
q8_df.select(col('genre'), col('title'), col('count')).show()

# stagemetrics.end()
# stagemetrics.print_report()
103/12:
# QUERY 9
# stagemetrics.begin()

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count'))
q9.show()

# stagemetrics.end()
# stagemetrics.print_report()
103/13:
# QUERY 10
# stagemetrics.begin()

q10_df = rating_df.join(tag_df, ['movieId'], 'inner')
q10_df = q10_df.select("movieId","tag").where((col('tag') == "funny") & (col('rating') > 3.5)).dropDuplicates()
q10_df = q10_df.join(movie_df, ['movieId'], 'inner')
q10_df = q10_df.withColumn("genre",explode(split("genres","[|]")))
q10_df = q10_df = q10_df.groupBy(q10_df['genre']).count().orderBy(q10_df['genre'].asc())
q10_df.show()

# stagemetrics.end()
# stagemetrics.print_report()
103/14:
# QUERY 1
stagemetrics.begin()

movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID
q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji
print(q1)

stagemetrics.end()
stagemetrics.print_report()
103/15:
from pyspark.sql import SparkSession
from sparkmeasure import StageMetrics
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour, sum, concat_ws, collect_list, lower
from pyspark.sql.window import Window

spark = SparkSession \
    .builder \
    .appName('vaseis-2') \
    .config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
    .master("spark://antonis:7077") \
    .getOrCreate()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
stagemetrics = StageMetrics(spark)
103/16:
# QUERY 1
stagemetrics.begin()

movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID
q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji
print(q1)

stagemetrics.end()
stagemetrics.print_report()
103/17:
# QUERY 2
stagemetrics.begin()

boring_movies = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"].contains('boring')).drop_duplicates(subset=['movieId']).select("movieId")
boring_movie_names = boring_movies.join(movie_df, boring_movies.movieId == movie_df.movieId).orderBy(movie_df['title']).select(movie_df['title'])
boring_movie_names.show() #show the first 5

stagemetrics.end()
stagemetrics.print_report()
103/18:
# QUERY 3
stagemetrics.begin()

bollywood_tag = tag_df.select('userId','movieId', lower(col('tag')).alias('tag')).filter(tag_df["tag"] == ('bollywood')).select('userId','movieId')
user_bollywood_good_rating = bollywood_tag.join(rating_df.select('userId','rating'), ["userId"], 'inner').distinct()
user_bollywood_good_rating = user_bollywood_good_rating.filter(col("rating")>3.0).dropDuplicates(['userId']).orderBy(col('userId')).select('userId')
user_bollywood_good_rating.show()


stagemetrics.end()
stagemetrics.print_report()
103/19:
# QUERY 4
stagemetrics.begin()

q4_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','movieId').avg('rating') #.join(movie_df.select('movieId','title'),['movieId'])
window = Window.partitionBy(q4_df['year']).orderBy(q4_df['avg(rating)'].desc())

q4_df = q4_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10).join(movie_df.select('movieId','title'),['movieId']).orderBy(movie_df['title'].asc()).filter(col('year') == 2005).select(col('title'))
q4_df.show()

stagemetrics.end()
stagemetrics.print_report()
103/20:
# QUERY 5
stagemetrics.begin()

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())
tag_df_2015.show()

stagemetrics.end()
stagemetrics.print_report()
103/21:
# QUERY 6
stagemetrics.begin()

q6_df = rating_df.groupBy('movieId').count()
q6_df = q6_df.join(movie_df.select('movieId','title'),['movieId']).orderBy(q6_df['count'].desc())
q6_df.select(col('title'),col('count')).show()

stagemetrics.end()
stagemetrics.print_report()
103/22:
# QUERY 7
stagemetrics.begin()

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10)
q7_df.filter(q7_df['year'] == 1995).select(col('userId')).orderBy(col('userId').asc()).show()

stagemetrics.end()
stagemetrics.print_report()
103/23:
# QUERY 7
stagemetrics.begin()

q7_df = rating_df.withColumn('year', year(rating_df['timestamp'])).groupBy('year','userId').count()
window = Window.partitionBy(q7_df['year']).orderBy(q7_df['count'].desc())
q7_df = q7_df.select('*', row_number().over(window).alias('row_num')).filter(col('row_num') <= 10)
q7_df.filter(q7_df['year'] == 1995).select(col('userId')).orderBy(col('userId').asc()).show()

stagemetrics.end()
stagemetrics.print_report()
103/24:
# QUERY 9
stagemetrics.begin()

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count').alias('count'))
q9.show()

stagemetrics.end()
stagemetrics.print_report()
103/25:
# QUERY 10
stagemetrics.begin()

q10_df = rating_df.join(tag_df, ['movieId'], 'inner')
q10_df = q10_df.select("movieId","tag").where((col('tag') == "funny") & (col('rating') > 3.5)).dropDuplicates()
q10_df = q10_df.join(movie_df, ['movieId'], 'inner')
q10_df = q10_df.withColumn("genre",explode(split("genres","[|]")))
q10_df = q10_df = q10_df.groupBy(q10_df['genre']).count().orderBy(q10_df['genre'].asc())
q10_df.show()

stagemetrics.end()
stagemetrics.print_report()
103/26:
# QUERY 5
stagemetrics.begin()

tag_df_2015 = tag_df.filter(year(tag_df['timestamp']) == 2015).select('movieId', 'tag').join(movie_df.select('movieId', 'title'), ['movieId'])
tag_df_2015 = tag_df_2015.groupBy('movieId','title').agg(concat_ws(", ", collect_list(tag_df_2015.tag)).alias('tags')).orderBy(tag_df_2015['title'].asc())
tag_df_2015.select(col('title'), col('tags')).show()

stagemetrics.end()
stagemetrics.print_report()
103/27:
# QUERY 8
stagemetrics.begin()

q8_df = movie_df.withColumn('genre',explode(split('genres',"[|]")))
q8_df = q8_df.join(rating_df,['movieId'],'left')
q8_df = q8_df.groupBy('genre','title').count()
window = Window.partitionBy(q8_df['genre']).orderBy(q8_df['count'].desc())
q8_df = q8_df.select('*', row_number().over(window).alias('rank')).filter(col('rank') == 1).orderBy(col('genre').asc())
q8_df.select(col('genre'), col('title')).show()

stagemetrics.end()
stagemetrics.print_report()
103/28:
from pyspark.sql import SparkSession
from sparkmeasure import StageMetrics
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour, sum, concat_ws, collect_list, lower
from pyspark.sql.window import Window

spark = SparkSession \
    .builder \
    .appName('vaseis-2') \
    .config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
    .master("spark://antonis:7077") \
    .getOrCreate()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
stagemetrics = StageMetrics(spark)
103/29:
# QUERY 1
stagemetrics.begin()

movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID
q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji
print(q1)

stagemetrics.end()
stagemetrics.print_report()
103/30:
# QUERY 9
stagemetrics.begin()

q9_df = rating_df.withColumn('year', year(rating_df['timestamp']))
q9_df = q9_df.withColumn('doy', dayofyear(rating_df['timestamp']))
q9_df = q9_df.withColumn('hour', hour(rating_df['timestamp']))
q9_df = q9_df.groupBy(q9_df['movieId'],q9_df['year'],q9_df['doy'],q9_df['hour']).count()
q9 = q9_df.filter(q9_df['count']>1).select(sum('count').alias('count'))
q9.show()

stagemetrics.end()
stagemetrics.print_report()
105/1:
from pyspark.sql import SparkSession
from sparkmeasure import StageMetrics
from pyspark.sql.functions import year, row_number, col, count, split, explode, rank, dayofyear, hour, sum, concat_ws, collect_list, lower
from pyspark.sql.window import Window

spark = SparkSession \
    .builder \
    .appName('vaseis-2') \
    .config("spark.jars", "<path-to-jar>/spark-measure_2.12-0.17.jar") \
    .master("spark://antonis:7077") \
    .getOrCreate()

rating_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("rating.csv") #file to be processed
)
movie_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("movie.csv") #file to be processed
)
tag_df = (spark.read
    .format("csv")
    .option('header','true') #csv files have headers
    .option('delimiter', ',') #delimiter of the csv files that are processed
    .option('inferSchema', 'true') #if false, all values are strings
    .load("tag.csv") #file to be processed
)
stagemetrics = StageMetrics(spark)
105/2:
# QUERY 1
stagemetrics.begin()

movie = movie_df.filter(movie_df["title"] == "Jumanji (1995)") #get rows with title jumanji
movieID = movie.collect()[0][0] #get movieID
q1 = rating_df.filter(rating_df["movieId"] == movieID).select(rating_df["userId"]).count() #count how many people have seen the movie jumanji
print(q1)

stagemetrics.end()
stagemetrics.print_report()
109/1:
import numpy as np
import os
import json
import csv
import tqdm
import unicodedata
import re
from string import printable
import pandas as pd
109/2:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

final_df = pd.DataFrame()
109/3:
dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path) as file:
                    data = json.load(file)

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]

                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair

                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x))

print(final_df)

final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
116/1:
dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path) as file:
                    data = json.load(file)

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]

                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair
                    print(new_conversation)

                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




# final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
# final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


# # Lowercase, trim, and remove non-letter characters
# def normalize(s):
#     s = s.lower().strip()
#     s = re.sub(r"([.!?])", r" \1", s)
#     s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
#     return s

# final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x))
# final_df["me"] = final_df["me"].apply(lambda x: normalize(x))

# print(final_df)

# final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
116/2:
import numpy as np
import os
import json
import csv
import tqdm
import unicodedata
import re
from string import printable
import pandas as pd
#test1
116/3:
import numpy as np
import os
import json
import csv
import tqdm
import unicodedata
import re
from string import printable
import pandas as pd
#test1
116/4:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
#test1
116/5:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
#test1
116/6:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

final_df = pd.DataFrame()
116/7:
dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path) as file:
                    data = json.load(file)

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]

                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair
                    print(new_conversation)

                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




# final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
# final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


# # Lowercase, trim, and remove non-letter characters
# def normalize(s):
#     s = s.lower().strip()
#     s = re.sub(r"([.!?])", r" \1", s)
#     s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
#     return s

# final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x))
# final_df["me"] = final_df["me"].apply(lambda x: normalize(x))

# print(final_df)

# final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
116/8:
dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path) as file:
                    data = json.load(file)

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]

                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair
                    print(new_conversation)

                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x))

print(final_df)

final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
116/9: final_df.shape
116/10:
dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path) as file:
                    data = json.load(file)

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]

                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair
                    print(new_conversation)

                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x))

print(final_df)

final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
116/11: from tensorflow.keras.preprocessing.sequence import pad_sequences
116/12: from tensorflow.keras.preprocessing.text import Tokenizer
116/13:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
116/14:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
116/15:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])
116/16:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])
print(train)
116/17:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)
116/18:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)
print(x_train)
116/19:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['x_train'])
116/20:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['x_train'])
116/21:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
116/22:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
print(y_train)
116/23:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
116/24:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
116/25:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
116/26:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model
116/27:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model

#creating the model
i = Input(shape=(input_shape,))
x = Embedding(vocabulary+1,10)(i)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_length,activation="softmax")(x)
model  = Model(i,x)
#compiling the model
model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])
#training the model
train = model.fit(x_train,y_train,epochs=200)
116/28:
#input length
input_shape = x_train.shape[1]
print(input_shape)
#define vocabulary
vocabulary = len(tokenizer.word_index)
print("number of unique words : ",vocabulary)
#output length
output_length = le.classes_.shape[0]
print("output length: ",output_length)
116/29:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model

#creating the model
i = Input(shape=(input_shape,))
x = Embedding(vocabulary+1,10)(i)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_length,activation="softmax")(x)
model  = Model(i,x)
#compiling the model
model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])
#training the model
train = model.fit(x_train,y_train,epochs=200)
116/30:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model

#creating the model
i = Input(shape=(input_shape,))
x = Embedding(vocabulary+1,10)(i)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_length,activation="softmax")(x)
model  = Model(i,x)
#compiling the model
model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])
#training the model
train = model.fit(x_train,y_train,epochs=20)
116/31:
#chatting
import random
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print("Going Merry : ",random.choice(responses[response_tag]))
  if response_tag == "goodbye":
    break
116/32:
#chatting
import random
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print("Going Merry : ",random.choice(responses[response_tag]))
  if response_tag == "goodbye":
    break
116/33:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print("Going Merry : ",random.choice(responses[response_tag]))
  if response_tag == "goodbye":
    break
116/34:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
#   #finding the right tag and predicting
#   response_tag = le.inverse_transform([output])[0]
#   print("Going Merry : ",random.choice(responses[response_tag]))
#   if response_tag == "goodbye":
#     break
116/35:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
    print(output)

#   #finding the right tag and predicting
#   response_tag = le.inverse_transform([output])[0]
#   print("Going Merry : ",random.choice(responses[response_tag]))
#   if response_tag == "goodbye":
#     break
116/36:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
print(output)

#   #finding the right tag and predicting
#   response_tag = le.inverse_transform([output])[0]
#   print("Going Merry : ",random.choice(responses[response_tag]))
#   if response_tag == "goodbye":
#     break
116/37:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()


#   #finding the right tag and predicting
#   response_tag = le.inverse_transform([output])[0]
#   print("Going Merry : ",random.choice(responses[response_tag]))
#   if response_tag == "goodbye":
#     break
116/38:
import random
prediction_input = "geia"
#removing punctuation and converting to lowercase
prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
prediction_input = ''.join(prediction_input)
texts_p.append(prediction_input)
 #tokenizing and padding
prediction_input = tokenizer.texts_to_sequences(texts_p)
prediction_input = np.array(prediction_input).reshape(-1)
prediction_input = pad_sequences([prediction_input],input_shape)
#getting output from model
output = model.predict(prediction_input)
output = output.argmax()

print(output)
116/39:
import random
prediction_input = "geia"
#removing punctuation and converting to lowercase
prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
prediction_input = ''.join(prediction_input)
texts_p.append(prediction_input)
 #tokenizing and padding
prediction_input = tokenizer.texts_to_sequences(texts_p)
prediction_input = np.array(prediction_input).reshape(-1)
prediction_input = pad_sequences([prediction_input],input_shape)
#getting output from model
output = model.predict(prediction_input)
output = output.argmax()

response = le.inverse_transform([output])[0]
print(response)
116/40:
import random
prediction_input = "geia"
#removing punctuation and converting to lowercase
prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
prediction_input = ''.join(prediction_input)
texts_p.append(prediction_input)
 #tokenizing and padding
prediction_input = tokenizer.texts_to_sequences(texts_p)
prediction_input = np.array(prediction_input).reshape(-1)
prediction_input = pad_sequences([prediction_input],input_shape)
#getting output from model
output = model.predict(prediction_input)
output = output.argmax()

response = le.inverse_transform([output])[0]
print(response)
116/41:
import random
prediction_input = "geia"
#removing punctuation and converting to lowercase
prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
prediction_input = ''.join(prediction_input)
texts_p.append(prediction_input)
 #tokenizing and padding
prediction_input = tokenizer.texts_to_sequences(texts_p)
prediction_input = np.array(prediction_input).reshape(-1)
prediction_input = pad_sequences([prediction_input],input_shape)
#getting output from model
output = model.predict(prediction_input)
output = output.argmax()

response = le.inverse_transform([output])[0]
print(response)
116/42:
import random
prediction_input = "gwm
"
#removing punctuation and converting to lowercase
prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
prediction_input = ''.join(prediction_input)
texts_p.append(prediction_input)
 #tokenizing and padding
prediction_input = tokenizer.texts_to_sequences(texts_p)
prediction_input = np.array(prediction_input).reshape(-1)
prediction_input = pad_sequences([prediction_input],input_shape)
#getting output from model
output = model.predict(prediction_input)
output = output.argmax()

response = le.inverse_transform([output])[0]
print(response)
116/43:
import random
prediction_input = "gwm"
#removing punctuation and converting to lowercase
prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
prediction_input = ''.join(prediction_input)
texts_p.append(prediction_input)
 #tokenizing and padding
prediction_input = tokenizer.texts_to_sequences(texts_p)
prediction_input = np.array(prediction_input).reshape(-1)
prediction_input = pad_sequences([prediction_input],input_shape)
#getting output from model
output = model.predict(prediction_input)
output = output.argmax()

response = le.inverse_transform([output])[0]
print(response)
116/44:
import random
prediction_input = "gwm"
#removing punctuation and converting to lowercase
prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
prediction_input = ''.join(prediction_input)
texts_p.append(prediction_input)
 #tokenizing and padding
prediction_input = tokenizer.texts_to_sequences(texts_p)
prediction_input = np.array(prediction_input).reshape(-1)
prediction_input = pad_sequences([prediction_input],input_shape)
#getting output from model
output = model.predict(prediction_input)
output = output.argmax()

response = le.inverse_transform([output])[0]
print(response)
116/45:
#chatting
import random
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print("Going Merry : ",random.choice(responses[response_tag]))
116/46:
#chatting
import random
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(response_tag)
116/47:
#chatting
import random
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(output)
116/48:
#chatting
import random
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(response_tag)
117/1:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
#test1
117/2:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

final_df = pd.DataFrame()

fix_mojibake_escapes = partial(
     re.compile(rb'\\u00([\da-f]{2})').sub,
     lambda m: bytes.fromhex(m.group(1).decode()))
117/3:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
#test1
117/4:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

final_df = pd.DataFrame()

fix_mojibake_escapes = partial(
     re.compile(rb'\\u00([\da-f]{2})').sub,
     lambda m: bytes.fromhex(m.group(1).decode()))
117/5:
dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path, 'rb') as file:
                    repaired = fix_mojibake_escapes(file.read())
                    data = json.loads(repaired.decode('utf8'))

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]
                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair


                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    # conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)
                    print(final_df)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x))

final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
117/6:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
print(y_train)
117/7:
dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path, 'rb') as file:
                    repaired = fix_mojibake_escapes(file.read())
                    data = json.loads(repaired.decode('utf8'))

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]
                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair


                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    # conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)
                    




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x))

final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
117/8:
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
print(y_train)
117/9:
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
print(y_train)
117/10:
#input length
input_shape = x_train.shape[1]
print(input_shape)
#define vocabulary
vocabulary = len(tokenizer.word_index)
print("number of unique words : ",vocabulary)
#output length
output_length = le.classes_.shape[0]
print("output length: ",output_length)
117/11:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model

#creating the model
i = Input(shape=(input_shape,))
x = Embedding(vocabulary+1,10)(i)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_length,activation="softmax")(x)
model  = Model(i,x)
#compiling the model
model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])
#training the model
train = model.fit(x_train,y_train,epochs=20)
117/12:
#chatting
import random
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(response_tag)
117/13:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(response_tag)
117/14:
dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path, 'rb') as file:
                    repaired = fix_mojibake_escapes(file.read())
                    data = json.loads(repaired.decode('utf8'))

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]
                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair


                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    # conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)
                    




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
#     s = re.sub(r"([.!?])", r" \1", s)
#     s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x))
print(final_df)
final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
117/15:
#input length
input_shape = x_train.shape[1]
print(input_shape)
#define vocabulary
vocabulary = len(tokenizer.word_index)
print("number of unique words : ",vocabulary)
#output length
output_length = le.classes_.shape[0]
print("output length: ",output_length)
117/16:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model

#creating the model
i = Input(shape=(input_shape,))
x = Embedding(vocabulary+1,10)(i)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_length,activation="softmax")(x)
model  = Model(i,x)
#compiling the model
model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])
#training the model
train = model.fit(x_train,y_train,epochs=20)
117/17:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(response_tag)
117/18:
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
print(y_train)
print(final_df['me'])
117/19:
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
print(y_train)
print(final_df['other'])
117/20:
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
print(y_train)
print(final_df['sender'])
117/21:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(response_tag)
118/1:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
#test1
118/2:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

final_df = pd.DataFrame()

fix_mojibake_escapes = partial(
     re.compile(rb'\\u00([\da-f]{2})').sub,
     lambda m: bytes.fromhex(m.group(1).decode()))
118/3:

dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path, 'rb') as file:
                    repaired = fix_mojibake_escapes(file.read())
                    data = json.loads(repaired.decode('utf8'))

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]
                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair


                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    # conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


#
final_df["sender"] = final_df["sender"].apply(lambda x: myconverter.convert(x))
final_df["me"] = final_df["me"].apply(lambda x: myconverter.convert(x))




# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x[0]))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x[0]))

print(final_df.head(20))


final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
118/4:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
118/5:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
119/1:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
119/2:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
119/3:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

myconverter = Converter(max_expansions=1)
final_df = pd.DataFrame()

fix_mojibake_escapes = partial(
     re.compile(rb'\\u00([\da-f]{2})').sub,
     lambda m: bytes.fromhex(m.group(1).decode()))
119/4:

dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path, 'rb') as file:
                    repaired = fix_mojibake_escapes(file.read())
                    data = json.loads(repaired.decode('utf8'))

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]
                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair


                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    # conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


#
final_df["sender"] = final_df["sender"].apply(lambda x: myconverter.convert(x))
final_df["me"] = final_df["me"].apply(lambda x: myconverter.convert(x))




# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x[0]))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x[0]))

print(final_df.head(20))


final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
119/5:
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
119/6:
#input length
input_shape = x_train.shape[1]
print(input_shape)
#define vocabulary
vocabulary = len(tokenizer.word_index)
print("number of unique words : ",vocabulary)
#output length
output_length = le.classes_.shape[0]
print("output length: ",output_length)
119/7:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model

#creating the model
i = Input(shape=(input_shape,))
x = Embedding(vocabulary+1,10)(i)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_length,activation="softmax")(x)
model  = Model(i,x)
#compiling the model
model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])
#training the model
train = model.fit(x_train,y_train,epochs=20)
119/8:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('You : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(response_tag)
121/1:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('steile ston antwnh : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(response_tag)
122/1:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
122/2:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
122/3:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

myconverter = Converter(max_expansions=1)
final_df = pd.DataFrame()

fix_mojibake_escapes = partial(
     re.compile(rb'\\u00([\da-f]{2})').sub,
     lambda m: bytes.fromhex(m.group(1).decode()))
122/4:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

myconverter = Converter(max_expansions=1)
final_df = pd.DataFrame()

fix_mojibake_escapes = partial(
     re.compile(rb'\\u00([\da-f]{2})').sub,
     lambda m: bytes.fromhex(m.group(1).decode()))
122/5:

dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path, 'rb') as file:
                    repaired = fix_mojibake_escapes(file.read())
                    data = json.loads(repaired.decode('utf8'))

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]
                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair


                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    # conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


#
final_df["sender"] = final_df["sender"].apply(lambda x: myconverter.convert(x))
final_df["me"] = final_df["me"].apply(lambda x: myconverter.convert(x))




# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x[0]))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x[0]))

print(final_df.head(20))


final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
122/6:
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
122/7:
#input length
input_shape = x_train.shape[1]
print(input_shape)
#define vocabulary
vocabulary = len(tokenizer.word_index)
print("number of unique words : ",vocabulary)
#output length
output_length = le.classes_.shape[0]
print("output length: ",output_length)
122/8:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model

#creating the model
i = Input(shape=(input_shape,))
x = Embedding(vocabulary+1,10)(i)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_length,activation="softmax")(x)
model  = Model(i,x)
#compiling the model
model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])
#training the model
train = model.fit(x_train,y_train,epochs=200)
model.save('chatbot.h5')
122/9:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('steile ston antwnh : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()
  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print(response_tag)
122/10:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('steile ston antwnh : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  output = model.predict(prediction_input)
  output = output.argmax()
  response = le.inverse_transform([output])[0]
  print(response)
123/1:
#chatting
import random
import string
while True:
  texts_p = []
  prediction_input = input('steile ston antwnh : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  output = model.predict(prediction_input)
  output = output.argmax()
  response = le.inverse_transform([output])[0]
  print(response)
123/2:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
123/3:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

myconverter = Converter(max_expansions=1)
final_df = pd.DataFrame()

fix_mojibake_escapes = partial(
     re.compile(rb'\\u00([\da-f]{2})').sub,
     lambda m: bytes.fromhex(m.group(1).decode()))
123/4:

dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path, 'rb') as file:
                    repaired = fix_mojibake_escapes(file.read())
                    data = json.loads(repaired.decode('utf8'))

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]
                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair


                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    # conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


#
final_df["sender"] = final_df["sender"].apply(lambda x: myconverter.convert(x))
final_df["me"] = final_df["me"].apply(lambda x: myconverter.convert(x))




# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x[0]))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x[0]))

print(final_df.head(20))


final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
123/5:
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
123/6:
#input length
input_shape = x_train.shape[1]
print(input_shape)
#define vocabulary
vocabulary = len(tokenizer.word_index)
print("number of unique words : ",vocabulary)
#output length
output_length = le.classes_.shape[0]
print("output length: ",output_length)
123/7:
#chatting
import random
import string
model = keras.models.load_model('chatbot.h5')
while True:
  texts_p = []
    
  prediction_input = input('steile ston antwnh : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  output = model.predict(prediction_input)
  output = output.argmax()
  response = le.inverse_transform([output])[0]
  print(response)
123/8:
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
123/9:
#chatting
import random
import string
model = keras.models.load_model('chatbot.h5')
while True:
  texts_p = []
    
  prediction_input = input('steile ston antwnh : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  output = model.predict(prediction_input)
  output = output.argmax()
  response = le.inverse_transform([output])[0]
  print(response)
123/10:
#chatting
import keras
import random
import string
model = keras.models.load_model('chatbot.h5')
while True:
  texts_p = []
    
  prediction_input = input('steile ston antwnh : ')
  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)
  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)
  output = model.predict(prediction_input)
  output = output.argmax()
  response = le.inverse_transform([output])[0]
  print(response)
124/1:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
124/2:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
124/3:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

myconverter = Converter(max_expansions=1)
final_df = pd.DataFrame()

fix_mojibake_escapes = partial(
     re.compile(rb'\\u00([\da-f]{2})').sub,
     lambda m: bytes.fromhex(m.group(1).decode()))
124/4:

dir = "inbox"
folders = os.listdir(dir)
path_list = []
paths_senders = {}
for folder in folders:
    for file in os.listdir(os.path.join("inbox",folder)):
        if file.startswith("message"):
            path = os.path.join(dir,folder,file)
            try:
                with open(path, 'rb') as file:
                    repaired = fix_mojibake_escapes(file.read())
                    data = json.loads(repaired.decode('utf8'))

                    sender_name = data["participants"][0]["name"]
                    messages = data["messages"]
                    new_conversation = []
                    for k in messages:
                        if "content" in k.keys():
                            new_conversation.append(k)
                    pairs = []
                    message = [] #message pair


                    for m in new_conversation[::-1]:
                        if len(message) == 0 or len(message) == 2:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message = []
                                message.append(m["content"])
                        elif len(message) == 1:
                            if m["sender_name"] != "Antonis Parlapanis":
                                message[0] = message[0]+ " " + m["content"] #enwnei duo munhmata sth seira
                            else:
                                message.append(m["content"])
                        else:
                            if m["sender_name"] == "Antonis Parlapanis":
                                message[1] = message[1] + " " + m["content"]
                        pairs.append(message)
                    new_pair = []
                    for pair in pairs:
                        if len(pair) == 2:
                            new_pair.append(pair)


                    me = []
                    sender = []

                    for pair in new_pair:
                        sender.append(pair[0])
                        me.append(pair[1])

                    conv_df = pd.DataFrame()
                    conv_df["sender"] = np.array(sender)
                    conv_df["me"] = np.array(me)
                    conv_df = conv_df.drop_duplicates(keep="first")

                    # conv_df = remove_emoji(conv_df)
                    final_df = pd.concat([conv_df,final_df],axis=0)




            except json.decoder.JSONDecodeError:
                print("There was a problem accessing the equipment data.")




final_df.drop(final_df[final_df["sender"]=="  "].index,inplace=True)
final_df.drop(final_df[final_df["me"]=="  "].index,inplace=True)


#
final_df["sender"] = final_df["sender"].apply(lambda x: myconverter.convert(x))
final_df["me"] = final_df["me"].apply(lambda x: myconverter.convert(x))




# Lowercase, trim, and remove non-letter characters
def normalize(s):
    s = s.lower().strip()
    s = re.sub(r"([.!?])", r" \1", s)
    s = re.sub(r"[^a-zA-Z.!?]+", r" ", s)
    return s

final_df["sender"] = final_df["sender"].apply(lambda x: normalize(x[0]))
final_df["me"] = final_df["me"].apply(lambda x: normalize(x[0]))

print(final_df.head(20))


final_df.to_csv('out.csv', sep='\t', encoding='utf-8')
124/5:
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer=Tokenizer()
tokenizer.fit_on_texts(final_df['sender'])
train = tokenizer.texts_to_sequences(final_df['sender'])

x_train = pad_sequences(train)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(final_df['me'])
124/6:
#input length
input_shape = x_train.shape[1]
print(input_shape)
#define vocabulary
vocabulary = len(tokenizer.word_index)
print("number of unique words : ",vocabulary)
#output length
output_length = le.classes_.shape[0]
print("output length: ",output_length)
124/7:
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalMaxPooling1D,Flatten
from tensorflow.keras.models import Model

#creating the model
i = Input(shape=(input_shape,))
x = Embedding(vocabulary+1,10)(i)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_length,activation="softmax")(x)
model  = Model(i,x)
#compiling the model
model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])
#training the model
train = model.fit(x_train,y_train,epochs=20)
# model.save('chatbot.h5')
130/1:
import numpy as np
import os
import json
import csv
import unicodedata
import re
from string import printable
import pandas as pd
import re
from functools import partial
from greeklish.converter import Converter
#test1
130/2:
def remove_emoji(df):
  return df.applymap(lambda y: ''.join(filter(lambda x: x in printable, y)))

myconverter = Converter(max_expansions=1)
final_df = pd.DataFrame()

fix_mojibake_escapes = partial(
     re.compile(rb'\\u00([\da-f]{2})').sub,
     lambda m: bytes.fromhex(m.group(1).decode()))
131/1:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/Users/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                cleaned[count].append(WNL.lemmatize(words.lower()))
                print(cleaned[count])
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train)
131/2:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                cleaned[count].append(WNL.lemmatize(words.lower()))
                print(cleaned[count])
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train)
131/3:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                print(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train)
131/4:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train)
131/5: print(X_train)
131/6: tf.get_feature_names()
131/7: tf.get_feature_names_out(7706)
131/8: tf.get_feature_names_out()
131/9: tf.get_feature_names()
131/10: print(tf.vocabulary())
131/11: tf.vocabulary()
131/12: tf.get_feature_names()
131/13: feature_names = vectorizer.get_feature_names_out()
131/14: feature_names = tf.et_feature_names_out()
131/15:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train.shape())
131/16: print(X_train.shape())
131/17: print(X_train)
131/18: print(X_train.shape)
131/19: print(X_train)
131/20:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()

df_news.content =df_news.content.replace(to_replace='From:(.*\n)',value='',regex=True) ##remove from to email
df_news.content =df_news.content.replace(to_replace='lines:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='Subject:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except
df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)
df_news.content =df_news.content.replace(to_replace='\s+',value=' ',regex=True)    #remove new line
df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space
df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

df_news['content']=[entry.lower() for entry in df_news['content']]


def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train.shape())
131/21:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()

df_news.content =df_news.content.replace(to_replace='From:(.*\n)',value='',regex=True) ##remove from to email
df_news.content =df_news.content.replace(to_replace='lines:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='Subject:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except
df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)
df_news.content =df_news.content.replace(to_replace='\s+',value=' ',regex=True)    #remove new line
df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space
df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

df_news['content']=[entry.lower() for entry in df_news['content']]


def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train)
131/22: print(x_train)
131/23: print(x_train.h)
131/24: print(x_train.head())
131/25: print(x_train[0]
131/26: print(x_train[0])
131/27: print(x_train[0].tag)
131/28: print(x_train[tag])
131/29: print(x_train.tag[0])
131/30: print(x_train.tag)
131/31:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()

df_news.content =df_news.content.replace(to_replace='From:(.*\n)',value='',regex=True) ##remove from to email
df_news.content =df_news.content.replace(to_replace='lines:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='Subject:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except
df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)
df_news.content =df_news.content.replace(to_replace='\s+',value=' ',regex=True)    #remove new line
df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space
df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

df_news['content']=[entry.lower() for entry in df_news['content']]


def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        print(count)
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train)
131/32:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()

df_news.content =df_news.content.replace(to_replace='From:(.*\n)',value='',regex=True) ##remove from to email
df_news.content =df_news.content.replace(to_replace='lines:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='Subject:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except
df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)
df_news.content =df_news.content.replace(to_replace='\s+',value=' ',regex=True)    #remove new line
df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space
df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

df_news['content']=[entry.lower() for entry in df_news['content']]


def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        print data[group]
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train)
131/33:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()

df_news.content =df_news.content.replace(to_replace='From:(.*\n)',value='',regex=True) ##remove from to email
df_news.content =df_news.content.replace(to_replace='lines:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='Subject:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except
df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)
df_news.content =df_news.content.replace(to_replace='\s+',value=' ',regex=True)    #remove new line
df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space
df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

df_news['content']=[entry.lower() for entry in df_news['content']]


def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        print(data[group])
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train)
131/34:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
path = "/home/antonis/Downloads/20news-bydate/20news-bydate-train"

df_news = pd.DataFrame()
for file in os.listdir(path):
    tag = file
    for doc in os.listdir(path+'/'+file):
        docpath = path+'/'+file+'/'+doc
        f = open(docpath, "r",encoding='cp1252')
        content = f.read()
        temp = pd.DataFrame(
            {
                'content':content,
                'tag':tag
            },index=[0]
        )
        df_news = pd.concat([df_news,temp])

all_names = names.words()
WNL = WordNetLemmatizer()

df_news.content =df_news.content.replace(to_replace='From:(.*\n)',value='',regex=True) ##remove from to email
df_news.content =df_news.content.replace(to_replace='lines:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='Subject:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except
df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)
df_news.content =df_news.content.replace(to_replace='\s+',value=' ',regex=True)    #remove new line
df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space
df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

df_news['content']=[entry.lower() for entry in df_news['content']]


def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news['content'])

print(x_train)


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)

print(X_train)
131/35: print(X_train.vocabulary_)
131/36: print(tf.vocabulary_)
131/37: print(tf.vocabulary_[8000])
131/38: print(tf.vocabulary_[8000])
131/39: print(tf.vocabulary_)
131/40: print(tf.vocabulary_.key())
131/41: print(tf.vocabulary_.keys())
131/42: print(tf.vocabulary_.values())
131/43: print(len(x_train))
131/44: print(len(df_news['content']))
131/45: print(len(df_news['content'][0]))
131/46: print(len(df_news['content'][0]))
131/47: print(df_news['content'][0])
131/48: print(df_news['content'][1])
131/49: print(df_news['content'])
131/50: print(df_news['content'].iloc[0])
131/51: print(df_news['tag'].iloc[0])
131/52: print(tf[0])
131/53: tf = tfidf_matrix.todense()
131/54: tf.shape
131/55: print(tf.shape)
131/56: print(tf.shape())
131/57: print(X_train.shape)
131/58: df_idf = pd.DataFrame(X_train,index=tf.get_feature_names(),columns=["idf_weights"])
131/59: df_idf = pd.DataFrame(tf,index=tf.get_feature_names(),columns=["idf_weights"])
131/60:
df_idf = pd.DataFrame(tf,index=tf.get_feature_names(),columns=["idf_weights"])
print(df_idf)
131/61:
df_idf = pd.DataFrame(tf.idf,index=tf.get_feature_names(),columns=["idf_weights"])
print(df_idf)
131/62:
df_idf = pd.DataFrame(X_train.idf_,index=tf.get_feature_names(),columns=["idf_weights"])
print(df_idf)
131/63:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test = "/home/antonis/Downloads/20news-bydate/20news-bydate-test"


def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)
df_news_test = gather(path_test)


def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])
x_test = clean(df_news_test['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)
X_test = tf.fit_transform(x_test)
131/64: print(x_test)
131/65: print(X_test.vocabulary_)
131/66: print(tf.vocabulary_)
131/67: print(tf.vocabulary)
131/68:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test = "/home/antonis/Downloads/20news-bydate/20news-bydate-test"


def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)
df_news_test = gather(path_test)


def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])
x_test = clean(df_news_test['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000)
tf_test = TfidfVectorizer(stop_words='english', max_features=8000, vocabulary=tf.vocabulary_)
X_train = tf.fit_transform(x_train)
X_test = tf_test.fit_transform(x_test)
131/69:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test = "/home/antonis/Downloads/20news-bydate/20news-bydate-test"


def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)
df_news_test = gather(path_test)


def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])
x_test = clean(df_news_test['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000)
X_train = tf.fit_transform(x_train)
131/70: print(tf.vocabulary_)
131/71: vocabulary = tf.vocabulary_
131/72: vocabulary = tf.vocabulary_
131/73:
vocabulary = tf.vocabulary_
print(vocabulary)
131/74:
def gen_vector_T(tokens):
Q = np.zeros((len(vocabulary)))    
    x= tfidf.transform(tokens)
    #print(tokens[0].split(','))
    for token in tokens[0].split(','):
        #print(token)
        try:
            ind = vocabulary.index(token)
            Q[ind]  = x[0, tfidf.vocabulary_[token]]
        except:
            pass
    return Q
131/75:
def gen_vector_T(tokens):
    Q = np.zeros((len(vocabulary)))    
        x= tfidf.transform(tokens)
        #print(tokens[0].split(','))
        for token in tokens[0].split(','):
            #print(token)
            try:
                ind = vocabulary.index(token)
                Q[ind]  = x[0, tfidf.vocabulary_[token]]
            except:
                pass
        return Q
131/76:
def gen_vector_T(tokens):
    Q = np.zeros((len(vocabulary)))    
    x= tfidf.transform(tokens)
    #print(tokens[0].split(','))
    for token in tokens[0].split(','):
        #print(token)
        try:
            ind = vocabulary.index(token)
            Q[ind]  = x[0, tfidf.vocabulary_[token]]
        except:
            pass
    return Q
131/77:
def gen_vector_T(tokens):
    Q = np.zeros((len(vocabulary)))    
    x= tfidf.transform(tokens)
    #print(tokens[0].split(','))
    for token in tokens[0].split(','):
        #print(token)
        try:
            ind = vocabulary.index(token)
            Q[ind]  = x[0, tfidf.vocabulary_[token]]
        except:
            pass
    return Q
131/78:
def cosine_sim(a, b):
    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))
    return cos_sim
131/79:
def cosine_similarity_T(k, query):
    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()
    tokens = word_tokenize(str(preprocessed_query))
    q_df = pd.DataFrame(columns=['q_clean'])
    q_df.loc[0,'q_clean'] =tokens
    q_df['q_clean'] =wordLemmatizer(q_df.q_clean)
    d_cosines = []
    
    query_vector = gen_vector_T(q_df['q_clean'])
    for d in tfidf_tran.A:
        d_cosines.append(cosine_sim(query_vector, d))
                    
    out = np.array(d_cosines).argsort()[-k:][::-1]
    #print("")
    d_cosines.sort()
    a = pd.DataFrame()
    for i,index in enumerate(out):
        a.loc[i,'index'] = str(index)
        a.loc[i,'Subject'] = df_news['Subject'][index]
    for j,simScore in enumerate(d_cosines[-k:][::-1]):
        a.loc[j,'Score'] = simScore
    return a
131/80: cosine_similarity_T(10,computer science)
131/81: cosine_similarity_T(10,'computer science')
131/82:
import re
def cosine_similarity_T(k, query):
    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()
    tokens = word_tokenize(str(preprocessed_query))
    q_df = pd.DataFrame(columns=['q_clean'])
    q_df.loc[0,'q_clean'] =tokens
    q_df['q_clean'] =wordLemmatizer(q_df.q_clean)
    d_cosines = []
    
    query_vector = gen_vector_T(q_df['q_clean'])
    for d in tfidf_tran.A:
        d_cosines.append(cosine_sim(query_vector, d))
                    
    out = np.array(d_cosines).argsort()[-k:][::-1]
    #print("")
    d_cosines.sort()
    a = pd.DataFrame()
    for i,index in enumerate(out):
        a.loc[i,'index'] = str(index)
        a.loc[i,'Subject'] = df_news['Subject'][index]
    for j,simScore in enumerate(d_cosines[-k:][::-1]):
        a.loc[j,'Score'] = simScore
    return a
131/83:
import re
def cosine_similarity_T(k, query):
    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()
    tokens = word_tokenize(str(preprocessed_query))
    q_df = pd.DataFrame(columns=['q_clean'])
    q_df.loc[0,'q_clean'] =tokens
    q_df['q_clean'] =wordLemmatizer(q_df.q_clean)
    d_cosines = []
    
    query_vector = gen_vector_T(q_df['q_clean'])
    for d in tfidf_tran.A:
        d_cosines.append(cosine_sim(query_vector, d))
                    
    out = np.array(d_cosines).argsort()[-k:][::-1]
    #print("")
    d_cosines.sort()
    a = pd.DataFrame()
    for i,index in enumerate(out):
        a.loc[i,'index'] = str(index)
        a.loc[i,'Subject'] = df_news['Subject'][index]
    for j,simScore in enumerate(d_cosines[-k:][::-1]):
        a.loc[j,'Score'] = simScore
    return a
131/84: cosine_similarity_T(10,'computer science')
131/85:
import re
from nltk.tokenize import word_tokenize
def cosine_similarity_T(k, query):
    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()
    tokens = word_tokenize(str(preprocessed_query))
    q_df = pd.DataFrame(columns=['q_clean'])
    q_df.loc[0,'q_clean'] =tokens
    q_df['q_clean'] =wordLemmatizer(q_df.q_clean)
    d_cosines = []
    
    query_vector = gen_vector_T(q_df['q_clean'])
    for d in tfidf_tran.A:
        d_cosines.append(cosine_sim(query_vector, d))
                    
    out = np.array(d_cosines).argsort()[-k:][::-1]
    #print("")
    d_cosines.sort()
    a = pd.DataFrame()
    for i,index in enumerate(out):
        a.loc[i,'index'] = str(index)
        a.loc[i,'Subject'] = df_news['Subject'][index]
    for j,simScore in enumerate(d_cosines[-k:][::-1]):
        a.loc[j,'Score'] = simScore
    return a
131/86: cosine_similarity_T(10,'computer science')
131/87:
import re
from nltk.tokenize import word_tokenize
def cosine_similarity_T(k, query):
    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()
    tokens = word_tokenize(str(preprocessed_query))
    q_df = pd.DataFrame(columns=['q_clean'])
    q_df.loc[0,'q_clean'] =tokens
    q_df['q_clean'] =WNL.lemmatize(q_df.q_clean)
    d_cosines = []
    
    query_vector = gen_vector_T(q_df['q_clean'])
    for d in tfidf_tran.A:
        d_cosines.append(cosine_sim(query_vector, d))
                    
    out = np.array(d_cosines).argsort()[-k:][::-1]
    #print("")
    d_cosines.sort()
    a = pd.DataFrame()
    for i,index in enumerate(out):
        a.loc[i,'index'] = str(index)
        a.loc[i,'Subject'] = df_news['Subject'][index]
    for j,simScore in enumerate(d_cosines[-k:][::-1]):
        a.loc[j,'Score'] = simScore
    return a
131/88:
import re
from nltk.tokenize import word_tokenize
def cosine_similarity_T(k, query):
    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()
    tokens = word_tokenize(str(preprocessed_query))
    q_df = pd.DataFrame(columns=['q_clean'])
    q_df.loc[0,'q_clean'] =tokens
    q_df['q_clean'] =WNL.lemmatize(q_df.q_clean)
    d_cosines = []
    
    query_vector = gen_vector_T(q_df['q_clean'])
    for d in tfidf_tran.A:
        d_cosines.append(cosine_sim(query_vector, d))
                    
    out = np.array(d_cosines).argsort()[-k:][::-1]
    #print("")
    d_cosines.sort()
    a = pd.DataFrame()
    for i,index in enumerate(out):
        a.loc[i,'index'] = str(index)
        a.loc[i,'Subject'] = df_news['Subject'][index]
    for j,simScore in enumerate(d_cosines[-k:][::-1]):
        a.loc[j,'Score'] = simScore
    return a
131/89: cosine_similarity_T(10,'computer science')
131/90:
tokens = ['computer','science']
Q = np.zeros((len(vocabulary)))    
x= tfidf.transform(tokens)
#print(tokens[0].split(','))
for token in tokens[0].split(','):
    #print(token)
    try:
        ind = vocabulary.index(token)
        Q[ind]  = x[0, tfidf.vocabulary_[token]]
    except:
        pass
131/91:
tokens = ['computer','science']
Q = np.zeros((len(vocabulary)))
print(Q)
x= tfidf.transform(tokens)
#print(tokens[0].split(','))
for token in tokens[0].split(','):
    #print(token)
    try:
        ind = vocabulary.index(token)
        Q[ind]  = x[0, tfidf.vocabulary_[token]]
    except:
        pass
131/92:
tokens = ['computer','science']
Q = np.zeros((len(vocabulary)))
tf = TfidfVectorizer(stop_words='english', max_features=8000,vocabulary=vocabulary)
print(Q)
x= tf.fit_transform(tokens)
#print(tokens[0].split(','))
for token in tokens[0].split(','):
    #print(token)
    try:
        ind = vocabulary.index(token)
        Q[ind]  = x[0, tfidf.vocabulary_[token]]
    except:
        pass
131/93:
tokens = ['computer','science']
Q = np.zeros((len(vocabulary)))
tf = TfidfVectorizer(stop_words='english', max_features=8000,vocabulary=vocabulary)
print(Q)
x= tf.fit_transform(tokens)
#print(tokens[0].split(','))
for token in tokens[0].split(','):
    #print(token)
    try:
        ind = vocabulary.index(token)
        Q[ind]  = x[0, tfidf.vocabulary_[token]]
    except:
        pass

print(Q)
131/94:
tokens = ['computer','science']
Q = np.zeros((len(vocabulary)))
tf = TfidfVectorizer(stop_words='english', max_features=8000,vocabulary=vocabulary)
print(Q)
x= tf.fit_transform(tokens)
#print(tokens[0].split(','))
for token in tokens[0].split(','):
    #print(token)
    try:
        ind = vocabulary.index(token)
        Q[ind]  = x[0, tfidf.vocabulary_[token]]
    except:
        pass

print(x)
131/95:
tokens = ['computer','science']
Q = np.zeros((len(vocabulary)))
tf = TfidfVectorizer(stop_words='english', max_features=8000,vocabulary=vocabulary)
print(Q)
x= tf.fit_transform(tokens)
#print(tokens[0].split(','))
for token in tokens[0].split(','):
    #print(token)
    try:
        ind = vocabulary.index(token)
        Q[ind]  = x[0, tfidf.vocabulary_[token]]
    except:
        pass

print(x)
131/96:
tokens = ['computer','science']
Q = np.zeros((len(vocabulary)))
tf = TfidfVectorizer(stop_words='english', max_features=8000,vocabulary=vocabulary)
print(Q)
x= tf.fit_transform(tokens)
#print(tokens[0].split(','))
for token in tokens[0].split(','):
    #print(token)
    try:
        ind = vocabulary.index(token)
        Q[ind]  = x[0, tfidf.vocabulary_[token]]
    except:
        pass

print(x)
131/97: print(vocabulary.index('computer'))
131/98:
tokens = ['computer','science']
for token in tokens[0].split(','):
    ind = vocabulary.index(token)
    print(ind)
131/99:
tokens = ['computer','science']
for token in tokens[0].split(','):
    ind = vocabulary.index(token)
    print(ind)
134/1:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/Home/antonis/Downloads/20news-bydate/20news-bydate-train"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)
134/2:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)
135/1:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
nltk.download('names')
from sklearn.feature_extraction.text import TfidfVectorizer
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)
135/2:
from sklearn.feature_extraction.text import CountVectorizer

vocab = tf.vocabulary_

countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)

count_wm = countvectorizer.fit_transform(x_train)
135/3:
from sklearn.feature_extraction.text import CountVectorizer

vocab = tf.vocabulary_

countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)

count_wm = countvectorizer.fit_transform(x_train)
135/4:
from sklearn.feature_extraction.text import TfidfTransformer
from numpy import asarray
from numpy import savetxt


#using Tfidftransformer and count vectorizer

tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) 
tfidf_transformer.fit(count_wm)


# print idf values 
df_idf = pd.DataFrame(tfidf_transformer.idf_, index=countvectorizer.get_feature_names(),columns=["idf_weights"]) 
 
# sort ascending 
df_idf.sort_values(by=['idf_weights'])

# count matrix 
count_vector=countvectorizer.transform(x_train) 
 
# tf-idf scores 
tf_idf_vector=tfidf_transformer.transform(count_vector)


feature_names = countvectorizer.get_feature_names() 
 
#get tfidf vector for first document 
first_document_vector=tf_idf_vector[10342] 
 
#print the scores 
df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=["tfidf"]) 
df.sort_values(by=['tfidf'],ascending=False)

#using Tfidfvectorizer

# get the first vector out (for the first document) 
first_vector_tfidfvectorizer=tfidf[0] 

# place tf-idf values in a pandas data frame
df_tfidf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tf.get_feature_names(), columns=["tfidf"]) 
df.sort_values(by=["tfidf"],ascending=False)


print(df_tfidf)

def get_key(val):
    for key, value in tf.vocabulary_.items():
         if val == value:
             return key
 
    return "key doesn't exist"
135/5:
import json

f = open('/Users/antonis/PycharmProjects/soup/news/Huffpost.json')
data = json.load(f)

documents = []

for i in data:
    documents.append(data[i])
135/6:
import json

f = open('/home/antonis/PycharmProjects/soup/news/Huffpost.json')
data = json.load(f)

documents = []

for i in data:
    documents.append(data[i])
137/1:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from numpy import asarray
from numpy import savetxt

nltk.download('names')
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/Users/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test= "/Users/antonis/Downloads/20news-bydate/20news-bydate-test"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)

vocab = tf.vocabulary_
137/2:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from numpy import asarray
from numpy import savetxt

nltk.download('names')
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test= "/home/antonis/Downloads/20news-bydate/20news-bydate-test"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)

vocab = tf.vocabulary_
136/1:
from sklearn.feature_extraction.text import CountVectorizer

vocab = tf.vocabulary_

countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)

count_wm = countvectorizer.fit_transform(x_train)
137/3:
from sklearn.feature_extraction.text import CountVectorizer

countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)

count_wm = countvectorizer.fit_transform(x_train)

#using Tfidftransformer and count vectorizer

tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) 
tfidf_transformer.fit(count_wm)


# print idf values 
df_idf = pd.DataFrame(tfidf_transformer.idf_, index=countvectorizer.get_feature_names(),columns=["idf_weights"]) 
 
# sort ascending 
df_idf.sort_values(by=['idf_weights'])

# count matrix 
count_vector=countvectorizer.transform(x_train) 
 
# tf-idf scores 
tf_idf_vector=tfidf_transformer.transform(count_vector)

feature_names = countvectorizer.get_feature_names() 
 
#get tfidf vector for first document 
first_document_vector=tf_idf_vector[10342] 
 
#print the scores 
df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=["tfidf"]) 
df.sort_values(by=['tfidf'],ascending=False)

#using Tfidfvectorizer

# get the first vector out (for the first document) 
first_vector_tfidfvectorizer=tfidf[0] 

# place tf-idf values in a pandas data frame
df_tfidf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tf.get_feature_names(), columns=["tfidf"]) 
df.sort_values(by=["tfidf"],ascending=False)

def get_key(val):
    for key, value in tf.vocabulary_.items():
         if val == value:
             return key
 
    return "key doesn't exist"
137/4:
#PART 2 ... TEST

df_news_test = gather(path_test)
x_test = clean(df_news_test['content'])
137/5: print(x_test)
137/6:
#PART 2 ... TEST

df_news_test = gather(path_test)
x_test = clean(df_news_test['content'])
137/7:
#PART 2 ... TEST

df_news_test = gather(path_test)
x_test = clean(df_news_test['content'])
137/8:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = [document]
    testvector = tf.transform(doc)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
    actual = df_news_train.loc[df_news_test['content'] == document,'tag']
    print(actual)
137/9: print(x_test)
137/10: print(x_test[100])
137/11: classify(x_test[5000],tfidf)
137/12: print(x_test)
137/13:
classify(x_test[50],tfidf)
print('actual = ' + df_news_test['tag'].iloc[50])
137/14: classify(x_test[50],tfidf)
137/15: print(df_news_test[1])
137/16: print(df_news_test[1])
137/17: print(df_news_test[0])
137/18: print(df_news_test['content'].iloc[0])
137/19: print(df_news_test['content'].iloc[5000])
137/20: print(df_news_test['content'].iloc[500])
137/21: print(df_news_test['content'].iloc[5002)
137/22: print(df_news_test['content'].iloc[5002]
137/23: print(df_news_test['content'].iloc[53]
137/24: print(df_news_test['content'].iloc[53])
137/25: print(df_news_test['tag'].iloc[53])
137/26:
classify(df_news_test['content'].iloc[53])
print(df_news_test['tag'].iloc[53])
137/27:
classify(df_news_test['content'].iloc[53],tfidf)
print(df_news_test['tag'].iloc[53])
137/28:
classify(df_news_test['content'].iloc[1],tfidf)
print(df_news_test['tag'].iloc[53])
137/29:

print(df_news_test['tag'].iloc[53])
137/30: classify(x_test[53],tfidf)
137/31: print(x_test[53])
137/32:
#PART 2 ... TEST

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].apply(clean)

#x_test = clean(df_news_test['content'])
137/33: print(df_news_test)
137/34:
#PART 2 ... TEST

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].applymap(clean)

#x_test = clean(df_news_test['content'])
137/35:
#PART 2 ... TEST

df_news_test = gather(path_test)

#df_news_test['content'] = df_news_test['content'].apply(clean)

#x_test = clean(df_news_test['content'])

for i in df_news_test['content']:
    print(i)
137/36:
#PART 2 ... TEST


def clean_test(data):
    cleaned = defaultdict(list)
    count = 0
        for words in data.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].apply(clean_test)

#x_test = clean(df_news_test['content'])
137/37:
#PART 2 ... TEST


def clean_test(data):
    cleaned = defaultdict(list)
    count = 0
    for words in data.split():
        if words.isalpha() and words not in all_names:
            words = ps.stem(words)
            cleaned[count].append(WNL.lemmatize(words.lower()))
    cleaned[count] = ' '.join(cleaned[count])
    count +=1
return(list(cleaned.values()))

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].apply(clean_test)

#x_test = clean(df_news_test['content'])
137/38:
#PART 2 ... TEST


def clean_test(data):
    cleaned = defaultdict(list)
    count = 0
    for words in data.split():
        if words.isalpha() and words not in all_names:
            words = ps.stem(words)
            cleaned[count].append(WNL.lemmatize(words.lower()))
    cleaned[count] = ' '.join(cleaned[count])
    count +=1
    return(list(cleaned.values()))

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].apply(clean_test)

#x_test = clean(df_news_test['content'])
137/39: print(df_news_test)
137/40: print(x_train)
137/41: print(x_train[0])
137/42: print(x_train[2000])
137/43: classify(df_news_test['content'][250])
137/44: classify(df_news_test['content'].iloc[250])
137/45: classify(df_news_test['content'].iloc[250].tfidf)
137/46: classify(df_news_test['content'].iloc[250],tfidf)
137/47: print(df_news_test['content'].iloc[205])
137/48: classify(df_news_test['content'].iloc[250],tfidf)
137/49: classify(df_news_test['content'].iloc[250].item(),tfidf)
137/50: classify(df_news_test['content'].iloc[250].value(),tfidf)
137/51:
#classify(df_news_test['content'].iloc[250],tfidf)
print(df_news_test['content'].iloc[250])
137/52:
#PART 2 ... TEST


def clean_test(data):
    cleaned = defaultdict(list)
    count = 0
    for words in data.split():
        if words.isalpha() and words not in all_names:
            words = ps.stem(words)
            cleaned[count].append(WNL.lemmatize(words.lower()))
    cleaned[count] = ' '.join(cleaned[count])
    count +=1
    return(list(cleaned.values()))

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].apply(clean_test)

x_test = clean(df_news_test['content'])
137/53:
#classify(df_news_test['content'].iloc[250],tfidf)
print(df_news_test['content'].iloc[250])
print(x_test[250])
137/54:
#classify(df_news_test['content'].iloc[250],tfidf)
print(df_news_test['content'].iloc[250].value())
print(x_test[250])
137/55:
#classify(df_news_test['content'].iloc[250],tfidf)
print(df_news_test['content'].iloc[250].get(0))
print(x_test[250])
137/56:
#classify(df_news_test['content'].iloc[250],tfidf)
print(df_news_test['content'].iloc[250].value)
print(x_test[250])
137/57:
#classify(df_news_test['content'].iloc[250],tfidf)
print(df_news_test['content'].iloc[250])
print(x_test[250])
137/58:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/59:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = document[0]
    print(doc)
    testvector = tf.transform(doc)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/60:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = document[0]
    print(doc)
#     testvector = tf.transform(doc)
#     distances = cosine_similarity(testvector, tfidf)
#     prediction = np.argmax(distances, 1)
#     print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/61:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/62:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/63:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = document[0]
    testvector = tf.transform(doc)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/64:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/65:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = document[0]
    doc = [doc]
    testvector = tf.transform(doc)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/66:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/67:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = [document[0]]
    testvector = tf.transform(doc)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/68:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/69:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = [document[0]]
    print(doc)
#     testvector = tf.transform(doc)
#     distances = cosine_similarity(testvector, tfidf)
#     prediction = np.argmax(distances, 1)
#     print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/70:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/71:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = [document[0]]
    print(document)
    print(doc)
#     testvector = tf.transform(doc)
#     distances = cosine_similarity(testvector, tfidf)
#     prediction = np.argmax(distances, 1)
#     print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/72:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/73:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = [document[0]]
    print(document)
    print(doc)
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/74:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/75:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    doc = [document[0]]
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/76:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/77:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/78:
classify(df_news_test['content'].iloc[250],tfidf)
#print(df_news_test['content'].iloc[250])
#print(x_test[250])
137/79: classify(df_news_test['content'].iloc[250],tfidf)
137/80:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
137/81: classify(df_news_test['content'].iloc[250],tfidf)
137/82:
classify(df_news_test['content'].iloc[250],tfidf)
print('actual = '+df_news_test['tag'].iloc[250])
137/83:
val = 270
classify(df_news_test['content'].iloc[val],tfidf)
print('actual = '+df_news_test['tag'].iloc[val])
137/84:

classify(df_news_test['content'].iloc[270],tfidf)
print('actual = '+df_news_test['tag'].iloc[270])
137/85:

classify(df_news_test['content'].iloc[10],tfidf)
print('actual = '+df_news_test['tag'].iloc[10])
137/86:

classify(df_news_test['content'].iloc[170],tfidf)
print('actual = '+df_news_test['tag'].iloc[170])
137/87:

classify(df_news_test['content'].iloc[1770],tfidf)
print('actual = '+df_news_test['tag'].iloc[1770])
137/88:

classify(df_news_test['content'].iloc[1770],tfidf)
print('actual = '+df_news_test['tag'].iloc[1770])
137/89:

classify(df_news_test['content'].iloc[1670],tfidf)
print('actual = '+df_news_test['tag'].iloc[1670])
137/90:

classify(df_news_test['content'].iloc[1670],tfidf)
print('actual = '+df_news_test['tag'].iloc[1670])
137/91:

classify(df_news_test['content'].iloc[1670],tfidf)
print('actual = '+df_news_test['tag'].iloc[1670])
137/92: print(x_train[2000])
137/93:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
    print('actual = '+df_news_test['tag'])
    actual = df_news_train.loc[df_news_test['content'] == document,'tag'].item()
    print(actual)
137/94:

classify(df_news_test['content'].iloc[1670],tfidf)
print('actual = '+df_news_test['tag'].iloc[1670])
137/95:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
    print('actual = '+df_news_test['tag'])
    actual = df_news_train.loc[df_news_test['content'] == document,'tag']
    print(actual)
137/96:

classify(df_news_test['content'].iloc[1670],tfidf)
print('actual = '+df_news_test['tag'].iloc[1670])
137/97:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
    print('actual = '+df_news_test['tag'])
    actual = df_news_train.loc[df_news_test['content'] == document,'tag']
    print(actual)
137/98:

classify(df_news_test['content'].iloc[1670],tfidf)
print('actual = '+df_news_test['tag'].iloc[1670])
137/99:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
    print('actual = '+df_news_test['tag'])
    actual = df_news_train.loc[df_news_test['content'] == document[0],'tag']
    print(actual)
137/100:

classify(df_news_test['content'].iloc[1670],tfidf)
print('actual = '+df_news_test['tag'].iloc[1670])
137/101:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
    print('actual = '+df_news_test['tag'])
    actual = df_news_train.loc[df_news_test['content'] == document[0],'tag']
137/102:

classify(df_news_test['content'].iloc[1670],tfidf)
print('actual = '+df_news_test['tag'].iloc[1670])
137/103:

classify(df_news_test['content'].iloc[1670],tfidf)
# print('actual = '+df_news_test['tag'].iloc[1670])
138/1:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from numpy import asarray
from numpy import savetxt

nltk.download('names')
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test= "/home/antonis/Downloads/20news-bydate/20news-bydate-test"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)

vocab = tf.vocabulary_
138/2:
from sklearn.feature_extraction.text import CountVectorizer

countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)

count_wm = countvectorizer.fit_transform(x_train)

#using Tfidftransformer and count vectorizer

tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) 
tfidf_transformer.fit(count_wm)


# print idf values 
df_idf = pd.DataFrame(tfidf_transformer.idf_, index=countvectorizer.get_feature_names(),columns=["idf_weights"]) 
 
# sort ascending 
df_idf.sort_values(by=['idf_weights'])

# count matrix 
count_vector=countvectorizer.transform(x_train) 
 
# tf-idf scores 
tf_idf_vector=tfidf_transformer.transform(count_vector)

feature_names = countvectorizer.get_feature_names() 
 
#get tfidf vector for first document 
first_document_vector=tf_idf_vector[10342] 
 
#print the scores 
df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=["tfidf"]) 
df.sort_values(by=['tfidf'],ascending=False)

#using Tfidfvectorizer

# get the first vector out (for the first document) 
first_vector_tfidfvectorizer=tfidf[0] 

# place tf-idf values in a pandas data frame
df_tfidf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tf.get_feature_names(), columns=["tfidf"]) 
df.sort_values(by=["tfidf"],ascending=False)

def get_key(val):
    for key, value in tf.vocabulary_.items():
         if val == value:
             return key
 
    return "key doesn't exist"
138/3:
#PART 2 ... TEST


def clean_test(data):
    cleaned = defaultdict(list)
    count = 0
    for words in data.split():
        if words.isalpha() and words not in all_names:
            words = ps.stem(words)
            cleaned[count].append(WNL.lemmatize(words.lower()))
    cleaned[count] = ' '.join(cleaned[count])
    count +=1
    return(list(cleaned.values()))

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].apply(clean_test)

x_test = clean(df_news_test['content'])
138/4:
#PART 2 ... TEST


def clean_test(data):
    cleaned = defaultdict(list)
    count = 0
    for words in data.split():
        if words.isalpha() and words not in all_names:
            words = ps.stem(words)
            cleaned[count].append(WNL.lemmatize(words.lower()))
    cleaned[count] = ' '.join(cleaned[count])
    count +=1
    return(list(cleaned.values()))

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].apply(clean_test)

# x_test = clean(df_news_test['content'])
138/5:

classify(df_news_test['content'].iloc[1670],tfidf)
# print('actual = '+df_news_test['tag'].iloc[1670])
138/6:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
    print('actual = '+df_news_test['tag'])
#     actual = df_news_train.loc[df_news_test['content'] == document[0],'tag']
138/7:

classify(df_news_test['content'].iloc[1670],tfidf)
# print('actual = '+df_news_test['tag'].iloc[1670])
138/8:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
    actual = df_news_train.loc[df_news_test['content'] == document[0],'tag']
138/9:

classify(df_news_test['content'].iloc[1670],tfidf)
# print('actual = '+df_news_test['tag'].iloc[1670])
138/10:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print('predicted = ' +df_news_test['tag'].iloc[prediction])
138/11:

classify(df_news_test['content'].iloc[1670],tfidf)
# print('actual = '+df_news_test['tag'].iloc[1670])
138/12:

classify(df_news_test['content'].iloc[1670],tfidf)
print('actual = '+df_news_test['tag'].iloc[1670])
138/13:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_test['tag'].iloc[prediction]
    print('predicted = ' +predicted)
    return predicted
138/14:
correct = 0
for i in range(0,1000):
    predicted = classify(df_news_test['content'].iloc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    if predicted = actual:
        correct = correct + 1
138/15:
correct = 0
for i in range(0,1000):
    predicted = classify(df_news_test['content'].iloc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    if predicted == actual:
        correct = correct + 1
138/16:
correct = 0
for i in range(0,1000):
    print(i)
#     predicted = classify(df_news_test['content'].iloc[i],tfidf)
#     actual = df_news_test['tag'].iloc[i]
#     if predicted == actual:
#         correct = correct + 1
138/17:
correct = 0
for i in range(1,1000):
    print(i)
    predicted = classify(df_news_test['content'].iloc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    if predicted == actual:
        correct = correct + 1
138/18:
correct = 0
for i in range(1,1000):
    print(i)
    predicted = classify(df_news_test['content'].iloc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    if predicted == actual:
        correct = correct + 1
138/19:
correct = 0
for i in range(1,1000):
    predicted = classify(df_news_test['content'].iloc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    print('actual: '+actual+'predicted: '+predicted)
138/20:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_test['tag'].iloc[prediction]
    return predicted
138/21:
correct = 0
for i in range(1,1000):
    predicted = classify(df_news_test['content'].iloc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    print('actual: '+actual+'predicted: '+predicted)
138/22:
correct = 0
for i in range(200,1000):
    predicted = classify(df_news_test['content'].iloc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    print('actual: '+actual+'predicted: '+predicted)
138/23:
correct = 0
for i in range(200,1000):
    predicted = classify(df_news_test['content'].loc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    print('actual: '+actual+'predicted: '+predicted)
138/24:
correct = 0
for i in range(200,1000):
    predicted = classify(df_news_test['content'].iloc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    print('actual: '+actual+'predicted: '+predicted)
138/25:
correct = 0
for i in range(200,1000):
    predicted = classify(df_news_test['content'].iloc[i,0],tfidf)
    actual = df_news_test['tag'].iloc[i]
    print('actual: '+actual+'predicted: '+predicted)
138/26:
correct = 0
for i in range(200,1000):
    predicted = classify(df_news_test['content'].loc[i,0],tfidf)
    actual = df_news_test['tag'].iloc[i]
    print('actual: '+actual+'predicted: '+predicted)
138/27:
correct = 0
for i in range(200,1000):
    predicted = classify(df_news_test['content'].iloc[i],tfidf)
    actual = df_news_test['tag'].iloc[i]
    print('actual: '+actual+'predicted: '+predicted)
138/28:
for index, data in df_news_test.iterrows():
    print(data)
138/29:
for index, data in df_news_test.iterrows():
    print(data['content'])
138/30:
for index, data in df_news_test.iterrows():
    classify(data[content],tfidf)
138/31:
for index, data in df_news_test.iterrows():
    classify(data['content'],tfidf)
138/32:
for index, data in df_news_test.iterrows():
    classify(df_news_test['content'].iloc[index],tfidf)
138/33:
for index, data in df_news_test.iterrows():
    print(df_news_test['tag'].iloc[index])
138/34:
for index, data in df_news_test.iterrows():
    print(index)
138/35: df_news_test.info()
138/36:
for index, data in df_news_test.iterrows():
    classify(df_news_test['content'].iloc[index],tfidf)
138/37:
for index, data in df_news_test.iterrows():
    classify(data['content'],tfidf)
138/38:
for index, data in df_news_test.iterrows():
    print(data['content'])
138/39:
for index, data in df_news_test.iteritems():
    print(data['content'])
138/40:
for index, data in df_news_test.iteritems():
    print(data)
138/41:
for row in df_news_test.itertuples():
    print(row)
138/42:
for row in df_news_test.itertuples():
    print(row['content'])
138/43:
for row in df_news_test.itertuples():
    print(row[content])
138/44:
for row in df_news_test.itertuples():
    print(row[2])
138/45:
for row in df_news_test.itertuples():
    print(row[1])
138/46:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_test['tag'].iloc[prediction]
    return predicted
138/47:
for row in df_news_test.itertuples():
    classify(row[1],tfidf)
138/48:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_test['tag'].iloc[prediction]
    print(predicted)
138/49:
for row in df_news_test.itertuples():
    classify(row[1],tfidf)
138/50:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test[0],tfidf)
138/51:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[1],tfidf)
138/52:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[0],tfidf)
138/53:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[3],tfidf)
138/54:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[4],tfidf)
138/55:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[5],tfidf)
138/56:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[6],tfidf)
138/57:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[7],tfidf)
138/58:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[8],tfidf)
138/59:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print(prediction)
#     predicted = df_news_test['tag'].iloc[prediction]
    print(predicted)
138/60:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[8],tfidf)
138/61:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    print(prediction)
    predicted = df_news_train['tag'].iloc[prediction]
    print(predicted)
138/62:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[8],tfidf)
138/63:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1))
    predicted = df_news_train['tag'].iloc[prediction]
    print(predicted)
138/64:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]
    print(predicted)
138/65:
# for row in df_news_test.itertuples():
#     classify(row[1],tfidf)
    
classify(df_news_test['content'].iloc[8],tfidf)
138/66:
for row in df_news_test.itertuples():
    classify(row[1],tfidf)
138/67:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def classify(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]
    return predicted
138/68:
correct = 0
for row in df_news_test.itertuples():
    predicted = classify(row[1],tfidf)
    actual = row[2]
    if actual == predicted:
        correct = correct + 1
138/69:
correct = 0
for row in df_news_test.itertuples():
    predicted = classify(row[1],tfidf)
    actual = row[2]
    print('actual is'+actual+'predicted is'+predicted)
138/70:
correct = 0
for row in df_news_test.itertuples():
    predicted = classify(row[1],tfidf)
    actual = row[2]
    print('actual is  '+actual+'predicted is  '+predicted)
138/71:
correct = 0
for row in df_news_test.itertuples():
    predicted = classify(row[1],tfidf)
    actual = row[2]
    if actual == predicted:
        correct = correct + 1
138/72:
correct = 0
for row in df_news_test.itertuples():
    predicted = classify(row[1],tfidf)
    actual = row[2]
    print(type(predicted))
138/73:
correct = 0
for row in df_news_test.itertuples():
    predicted = classify(row[1],tfidf)
    actual = row[2]
    print(type(actual))
138/74:
correct = 0
for row in df_news_test.itertuples():
    predicted = classify(row[1],tfidf)
    actual = row[2]
    print(predicted[0])
138/75:
correct = 0
for row in df_news_test.itertuples():
    predicted = classify(row[1],tfidf)
    actual = row[2]
    if actual == predicted[0]:
        correct=correct + 1
print(correct)
139/1:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from numpy import asarray
from numpy import savetxt

nltk.download('names')
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test= "/home/antonis/Downloads/20news-bydate/20news-bydate-test"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for words in group.split():
        if words.isalpha() and words not in all_names:
            words = ps.stem(words)
            cleaned[count].append(WNL.lemmatize(words.lower()))
    cleaned[count] = ' '.join(cleaned[count])
    count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)

vocab = tf.vocabulary_
139/2:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from numpy import asarray
from numpy import savetxt

nltk.download('names')
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test= "/home/antonis/Downloads/20news-bydate/20news-bydate-test"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for words in data.split():
        if words.isalpha() and words not in all_names:
            words = ps.stem(words)
            cleaned[count].append(WNL.lemmatize(words.lower()))
    cleaned[count] = ' '.join(cleaned[count])
    count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)

vocab = tf.vocabulary_
138/76: print(x_train[2000])
138/77:
correct = 0
for row in df_news_test.itertuples():
    predicted = classify(row[1],tfidf)
    actual = row[2]
    if actual == predicted[0]:
        correct=correct + 1
print(correct)
138/78:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics

def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]
    return predicted
   1:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from numpy import asarray
from numpy import savetxt

nltk.download('names')
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test= "/home/antonis/Downloads/20news-bydate/20news-bydate-test"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)

vocab = tf.vocabulary_
   2:
from sklearn.feature_extraction.text import CountVectorizer

countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)

count_wm = countvectorizer.fit_transform(x_train)

#using Tfidftransformer and count vectorizer

tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) 
tfidf_transformer.fit(count_wm)


# print idf values 
df_idf = pd.DataFrame(tfidf_transformer.idf_, index=countvectorizer.get_feature_names(),columns=["idf_weights"]) 
 
# sort ascending 
df_idf.sort_values(by=['idf_weights'])

# count matrix 
count_vector=countvectorizer.transform(x_train) 
 
# tf-idf scores 
tf_idf_vector=tfidf_transformer.transform(count_vector)

feature_names = countvectorizer.get_feature_names() 
 
#get tfidf vector for first document 
first_document_vector=tf_idf_vector[10342] 
 
#print the scores 
df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=["tfidf"]) 
df.sort_values(by=['tfidf'],ascending=False)

#using Tfidfvectorizer

# get the first vector out (for the first document) 
first_vector_tfidfvectorizer=tfidf[0] 

# place tf-idf values in a pandas data frame
df_tfidf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tf.get_feature_names(), columns=["tfidf"]) 
df.sort_values(by=["tfidf"],ascending=False)

def get_key(val):
    for key, value in tf.vocabulary_.items():
         if val == value:
             return key
 
    return "key doesn't exist"
   3:
#PART 2 ... TEST


def clean_test(data):
    cleaned = defaultdict(list)
    count = 0
    for words in data.split():
        if words.isalpha() and words not in all_names:
            words = ps.stem(words)
            cleaned[count].append(WNL.lemmatize(words.lower()))
    cleaned[count] = ' '.join(cleaned[count])
    count +=1
    return(list(cleaned.values()))

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].apply(clean_test)

# x_test = clean(df_news_test['content'])
   4:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics



def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]

#df=df_news_test
def classify(df):
    classified = pd.Dataframe()
    doc_count = len(df.index)
    correct = 0
    for row in df.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
        temp = pd.Dataframe(
        {
            'document':row[1],
            'class':predicted
        },index[0])
        df = pd.concat([df, temp])
        actual = row[2]
        if actual == predicted[0]:
            correct=correct + 1
    accuracy = (correct/doc_count)*100
    percentage = "{:.0%}".format(accuracy)
    print(percentage)
    return classified

classified_docs = classify(df_news_test)
   5:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics



def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]

#df=df_news_test
def classify(df):
    classified = pd.DataFrame()
    doc_count = len(df.index)
    correct = 0
    for row in df.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
        temp = pd.Dataframe(
        {
            'document':row[1],
            'class':predicted
        },index[0])
        df = pd.concat([df, temp])
        actual = row[2]
        if actual == predicted[0]:
            correct=correct + 1
    accuracy = (correct/doc_count)*100
    percentage = "{:.0%}".format(accuracy)
    print(percentage)
    return classified

classified_docs = classify(df_news_test)
   6:
from sklearn.feature_extraction.text import CountVectorizer

countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)

count_wm = countvectorizer.fit_transform(x_train)

#using Tfidftransformer and count vectorizer

tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) 
tfidf_transformer.fit(count_wm)


# print idf values 
df_idf = pd.DataFrame(tfidf_transformer.idf_, index=countvectorizer.get_feature_names(),columns=["idf_weights"]) 
 
# sort ascending 
df_idf.sort_values(by=['idf_weights'])

# count matrix 
count_vector=countvectorizer.transform(x_train) 
 
# tf-idf scores 
tf_idf_vector=tfidf_transformer.transform(count_vector)

feature_names = countvectorizer.get_feature_names() 
 
#get tfidf vector for first document 
first_document_vector=tf_idf_vector[10342] 
 
#print the scores 
df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=["tfidf"]) 
df.sort_values(by=['tfidf'],ascending=False)

#using Tfidfvectorizer

# get the first vector out (for the first document) 
first_vector_tfidfvectorizer=tfidf[0] 

# place tf-idf values in a pandas data frame
df_tfidf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tf.get_feature_names(), columns=["tfidf"]) 
df.sort_values(by=["tfidf"],ascending=False)

def get_key(val):
    for key, value in tf.vocabulary_.items():
         if val == value:
             return key
 
    return "key doesn't exist"
   7:
#PART 2 ... TEST


def clean_test(data):
    cleaned = defaultdict(list)
    count = 0
    for words in data.split():
        if words.isalpha() and words not in all_names:
            words = ps.stem(words)
            cleaned[count].append(WNL.lemmatize(words.lower()))
    cleaned[count] = ' '.join(cleaned[count])
    count +=1
    return(list(cleaned.values()))

df_news_test = gather(path_test)

df_news_test['content'] = df_news_test['content'].apply(clean_test)

# x_test = clean(df_news_test['content'])
   8:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics



def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]

#df=df_news_test
def classify(df):
    classified = pd.DataFrame()
    doc_count = len(df.index)
    correct = 0
    for row in df.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
        temp = pd.Dataframe(
        {
            'document':row[1],
            'class':predicted
        },index[0])
        df = pd.concat([df, temp])
        actual = row[2]
        if actual == predicted[0]:
            correct=correct + 1
    accuracy = (correct/doc_count)*100
    percentage = "{:.0%}".format(accuracy)
    print(percentage)
    return classified

classified_docs = classify(df_news_test)
   9:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics



def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]

#df=df_news_test
def classify(df):
    classified = pd.DataFrame()
    doc_count = len(df.index)
    correct = 0
    for row in df.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
        temp = pd.Dataframe(
        {
            'document':row[1],
            'class':predicted
        },index[0])
        df = pd.concat([df, temp])
        actual = row[2]
        if actual == predicted[0]:
            correct=correct + 1
    accuracy = (correct/doc_count)*100
    percentage = "{:.0%}".format(accuracy)
    print(percentage)
    return classified

classified_docs = classify(df_news_test[1])
  10:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics



def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]

#df=df_news_test
def classify(df):
    classified = pd.DataFrame()
    doc_count = len(df.index)
    correct = 0
    for row in df.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
        temp = pd.Dataframe(
        {
            'document':row[1],
            'class':predicted
        },index[0])
        df = pd.concat([df, temp])
        actual = row[2]
        if actual == predicted[0]:
            correct=correct + 1
    accuracy = (correct/doc_count)*100
    percentage = "{:.0%}".format(accuracy)
    print(percentage)
    return classified

classified_docs = classify(df_news_test['content'])
  11: print(df_news_test['content'])
  12:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics



def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]

#df=df_news_test
def classify(df):
    classified = pd.DataFrame()
    doc_count = len(df.index)
    correct = 0
    for row in df.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
        temp = pd.Dataframe(
        {
            'document':row[1],
            'class':predicted
        },index[0])
        df = pd.concat([df, temp])
        actual = row[2]
        if actual == predicted[0]:
            correct=correct + 1
    accuracy = (correct/doc_count)*100
    percentage = "{:.0%}".format(accuracy)
    print(percentage)
    return classified

classified_docs = classify(df_news_test['content'].iloc[4])
  13:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics



def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]

#df=df_news_test
def classify(df):
    classified = pd.DataFrame()
    doc_count = len(df.index)
    correct = 0
    for row in df.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
        temp = pd.Dataframe(
        {
            'document':row[1],
            'class':predicted
        },index[0])
        df = pd.concat([df, temp])
        actual = row[2]
        if actual == predicted[0]:
            correct=correct + 1
    accuracy = (correct/doc_count)*100
    percentage = "{:.0%}".format(accuracy)
    print(percentage)
    return classified

classified_docs = classify(df_news_test)
  14:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics



def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]

#df=df_news_test
def classify(df):
    classified = pd.DataFrame()
    doc_count = len(df.index)
    correct = 0
    for row in df.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
        temp = pd.Dataframe(
        {
            'document':row[1],
            'class':predicted
        },index[0])
        df = pd.concat([df, temp])
        actual = row[2]
        if actual == predicted[0]:
            correct=correct + 1
    accuracy = (correct/doc_count)*100
    percentage = "{:.0%}".format(accuracy)
    print(percentage)
    return classified

classified_docs = classify(df_news_test['content'])
  15:
from sklearn.metrics.pairwise import cosine_similarity
from sklearn import metrics



def cosine_similarity(document,tfidf):
    testvector = tf.transform(document)
    distances = cosine_similarity(testvector, tfidf)
    prediction = np.argmax(distances, 1)
    predicted = df_news_train['tag'].iloc[prediction]

#df=df_news_test
def classify(df):
    classified = pd.DataFrame()
    doc_count = len(df.index)
    correct = 0
    for row in df.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
        temp = pd.Dataframe(
        {
            'document':row[1],
            'class':predicted
        },index[0])
        df = pd.concat([df, temp])
        actual = row[2]
        if actual == predicted[0]:
            correct=correct + 1
    accuracy = (correct/doc_count)*100
    percentage = "{:.0%}".format(accuracy)
    print(percentage)
    return classified

classified_docs = classify(df_news_test)
  16:
for row in df_news_test.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
  17:
for row in df_news_test.itertuples():
        print(row[1])
  18:
for row in df_news_test.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
  19:
for row in df_news_test.itertuples():
        input = [row[1]]
        predicted = cosine_similarity(row[1],tfidf)
  20:
for row in df_news_test.itertuples():
        input = [row[1]]
        predicted = cosine_similarity(input,tfidf)
  21:
for row in df_news_test.itertuples():
        predicted = cosine_similarity(row[1],tfidf)
  22: cosine_similarity(df_news_test['content'].iloc[5],tfidf)
  23: cosine_similarity(df_news_test['content'],tfidf)
  24:
from sklearn.datasets import fetch_20newsgroups
from collections import defaultdict
from nltk.stem import WordNetLemmatizer
from nltk.corpus import names
from nltk.stem import PorterStemmer
import numpy as np
import nltk
import pandas as pd
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from numpy import asarray
from numpy import savetxt

nltk.download('names')
ps = PorterStemmer()
all_names = names.words()
WNL = WordNetLemmatizer()

path_train= "/home/antonis/Downloads/20news-bydate/20news-bydate-train"
path_test= "/home/antonis/Downloads/20news-bydate/20news-bydate-test"



def gather(path):
    df = pd.DataFrame()
    for file in os.listdir(path):
        tag = file
        for doc in os.listdir(path+'/'+file):
            docpath = path+'/'+file+'/'+doc
            f = open(docpath, "r",encoding='cp1252')
            content = f.read()
            temp = pd.DataFrame(
                {
                    'content':content,
                    'tag':tag
                },index=[0]
            )
            df = pd.concat([df, temp])



    df.content =df.content.replace(to_replace='From:(.*\n)', value='', regex=True) ##remove from to email
    df.content =df.content.replace(to_replace='lines:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='Subject:(.*\n)', value='', regex=True)
    df.content =df.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except
    df.content =df.content.replace(to_replace='-', value=' ', regex=True)
    df.content =df.content.replace(to_replace='\s+', value=' ', regex=True)    #remove new line
    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space
    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace

    df['content']=[entry.lower() for entry in df['content']]
    return df

df_news_train = gather(path_train)



def clean(data):
    cleaned = defaultdict(list)
    count = 0
    for group in data:
        for words in group.split():
            if words.isalpha() and words not in all_names:
                words = ps.stem(words)
                cleaned[count].append(WNL.lemmatize(words.lower()))
        cleaned[count] = ' '.join(cleaned[count])
        count +=1
    return(list(cleaned.values()))

x_train = clean(df_news_train['content'])


tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)
tfidf = tf.fit_transform(x_train)

vocab = tf.vocabulary_
  25:

%history -g -f history

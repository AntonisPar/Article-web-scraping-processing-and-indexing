{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21811307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/antonis/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "nltk.download('names')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ps = PorterStemmer()\n",
    "all_names = names.words()\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "path_train= \"/Users/antonis/Downloads/20news-bydate/20news-bydate-train\"\n",
    "\n",
    "\n",
    "\n",
    "def gather(path):\n",
    "    df = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        tag = file\n",
    "        for doc in os.listdir(path+'/'+file):\n",
    "            docpath = path+'/'+file+'/'+doc\n",
    "            f = open(docpath, \"r\",encoding='cp1252')\n",
    "            content = f.read()\n",
    "            temp = pd.DataFrame(\n",
    "                {\n",
    "                    'content':content,\n",
    "                    'tag':tag\n",
    "                },index=[0]\n",
    "            )\n",
    "            df = pd.concat([df, temp])\n",
    "\n",
    "\n",
    "\n",
    "    df.content =df.content.replace(to_replace='From:(.*\\n)', value='', regex=True) ##remove from to email\n",
    "    df.content =df.content.replace(to_replace='lines:(.*\\n)', value='', regex=True)\n",
    "    df.content =df.content.replace(to_replace='Subject:(.*\\n)', value='', regex=True)\n",
    "    df.content =df.content.replace(to_replace='[!\"#$%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except\n",
    "    df.content =df.content.replace(to_replace='-', value=' ', regex=True)\n",
    "    df.content =df.content.replace(to_replace='\\s+', value=' ', regex=True)    #remove new line\n",
    "    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space\n",
    "    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace\n",
    "\n",
    "    df['content']=[entry.lower() for entry in df['content']]\n",
    "    return df\n",
    "\n",
    "df_news_train = gather(path_train)\n",
    "\n",
    "\n",
    "\n",
    "def clean(data):\n",
    "    cleaned = defaultdict(list)\n",
    "    count = 0\n",
    "    for group in data:\n",
    "        for words in group.split():\n",
    "            if words.isalpha() and words not in all_names:\n",
    "                words = ps.stem(words)\n",
    "                cleaned[count].append(WNL.lemmatize(words.lower()))\n",
    "        cleaned[count] = ' '.join(cleaned[count])\n",
    "        count +=1\n",
    "    return(list(cleaned.values()))\n",
    "\n",
    "x_train = clean(df_news_train['content'])\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)\n",
    "tfidf = tf.fit_transform(x_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79c4d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab = tf.vocabulary_\n",
    "\n",
    "countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)\n",
    "\n",
    "count_wm = countvectorizer.fit_transform(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc3f7911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tfidf\n",
      "aa         0.0\n",
      "aaa        0.0\n",
      "aaron      0.0\n",
      "ab         0.0\n",
      "abandon    0.0\n",
      "...        ...\n",
      "zubov      0.0\n",
      "zurich     0.0\n",
      "zv         0.0\n",
      "zx         0.0\n",
      "zz         0.0\n",
      "\n",
      "[8000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "\n",
    "#using Tfidftransformer and count vectorizer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(count_wm)\n",
    "\n",
    "\n",
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=countvectorizer.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "# count matrix \n",
    "count_vector=countvectorizer.transform(x_train) \n",
    " \n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "\n",
    "feature_names = countvectorizer.get_feature_names() \n",
    " \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[10342] \n",
    " \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=['tfidf'],ascending=False)\n",
    "\n",
    "#using Tfidfvectorizer\n",
    "\n",
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf[0] \n",
    "\n",
    "# place tf-idf values in a pandas data frame\n",
    "df_tfidf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tf.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "\n",
    "print(df_tfidf)\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in tf.vocabulary_.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad069c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open('/Users/antonis/PycharmProjects/soup/news/Huffpost.json')\n",
    "\n",
    "data = json.load(f)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in data:\n",
    "    documents.append(data[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1f512e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    soc.religion.christian\n",
      "Name: tag, dtype: object\n",
      "['Yes, government officials can’t quite agree on whether we should go or not – and advising us to avoid kissing strangers under the mistletoe if we do. But what is a festive party without any snogging? (And let’s not forget that some government officials are partial to a Chrimbo party themselves. Not to mention an illicit snog). Whether it’s colleagues who’ve been making eyes at each other in the office for weeks leading up to the big event, or just those who happened to be, well, standing next to each other, a cheeky pash between co-workers is all part of the fun. But work and pensions secretary, Thérèse Coffey, has said no more of that please. “I don’t think there should be much snogging under the mistletoe,” she told ITV’s Peston programme.“You don’t need to do things like that. But I think we should all be trying to enjoy the Christmas ahead of us.” Which has got us thinking, what else do we love (or hate) or love to hate about the annual work do shindig? Enjoy our little game of Christmas party bingo – how many of these have you seen or partaken in during Christmases past? As mentioned above, this is just a festive staple. Somewhere very near you, two colleagues are making out.  Who doesn’t love an open bar, on the company’s dime? This is the time to make up for all those extra hours and early starts. Grab that first glass of fizz when you head in the door, then another and another. But, err, maybe don’t overdo it. Your bosses are still around, after all. It’s not our first rodeo, we know we have to get some food down us before the heavy drinking commences, so chasing those tiny salmon crostinis and pigs in blankets will do. It’s all about getting yourself in sight line of the kitchen. If there’s a live band or karaoke machine, then someone is going to get up there and do their rendition of Spice Girls and/or Backstreet Boys. It has to happen, so you might as well relent and give in. Alcohol certainly lowers one’s inhibitions and maybe it’s a good thing when it comes to the dance floor. Some people will be tearing it up while others will be doing big fish, little fish, cardboard box. And we’ll all have a good time. If you didn’t take selfies, did it really happen? And if your office heads have done the good thing and paid for a photo booth, then you best get in line – the silly hats and comically large moustaches aren’t going to wear themselves. Free booze and a closed environment with lots of different people – someone is bound to step on someone else’s foot, both figuratively and literally. And so it’s no surprise if a shouting match or quiet bitching session starts to take place. We love the office party when it allows us to see a looser, more fun side to the people we spend most of the year with.  See point seven. Full on free bubbly with our closest work pals, it’s easy to let something slip, whether you were supposed to or not. This is already a huge perk for many women on an average night out and the work party is no exception. There’s no bonding like sharing a spritz of perfume in the loo or a pair of straighteners if you’re getting ready in the office. Also toilet based. No comment. But it happens.  Oh no, you really have to go? No, just stay for another, and another, and then one more for the road. Before you know it, it’s 3am and the party is over. And lastly, arguably the best part, gossiping about all the shenanigans of the night before, mostly about who hooked up with whom. It’s the only thing that gets us through the almightly hangover. Life Reporter']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import metrics\n",
    "    \n",
    "input=[documents[50]]\n",
    "\n",
    "# this steps generates word counts for the words in your docs \n",
    "# word_count_vector=countvectorizer.fit_transform(input)\n",
    "\n",
    "testvector = tf.transform(input)\n",
    "\n",
    "distances = cosine_similarity(testvector, tfidf)\n",
    "prediction = np.argmax(distances, 1)\n",
    "\n",
    "print(df_news_train['tag'].iloc[prediction])\n",
    "print(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d199c09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         idf_weights\n",
      "aa          6.750366\n",
      "aaa         6.837377\n",
      "aaron       6.239540\n",
      "ab          5.812096\n",
      "abandon     6.483737\n",
      "...              ...\n",
      "zubov       7.848978\n",
      "zurich      7.625834\n",
      "zv          8.031299\n",
      "zx          6.402059\n",
      "zz          7.625834\n",
      "\n",
      "[8000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60a9d97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test= \"/Users/antonis/Downloads/20news-bydate/20news-bydate-test\"\n",
    "df_news_test = gather(path_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2b0a35db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content    lines 29 nntp posting host acad3.alaska.edu or...\n",
      "tag                                         rec.sport.hockey\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_news_test.iloc[5400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "14895fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6698]\n",
      "0    comp.windows.x\n",
      "Name: tag, dtype: object\n"
     ]
    }
   ],
   "source": [
    "thetest = df_news_test['content'].iloc[5400]\n",
    "thetest = [thetest]\n",
    "testvector = tf.transform(thetest)\n",
    "distances = cosine_similarity(testvector, tfidf)\n",
    "prediction = np.argmax(distances, 1)\n",
    "\n",
    "print(prediction)\n",
    "print(df_news_train['tag'].iloc[prediction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf93c3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ce598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

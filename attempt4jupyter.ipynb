{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21811307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/antonis/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "nltk.download('names')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "ps = PorterStemmer()\n",
    "all_names = names.words()\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "path_train= \"/home/antonis/Downloads/20news-bydate/20news-bydate-train\"\n",
    "\n",
    "\n",
    "\n",
    "def gather(path):\n",
    "    df = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        tag = file\n",
    "        for doc in os.listdir(path+'/'+file):\n",
    "            docpath = path+'/'+file+'/'+doc\n",
    "            f = open(docpath, \"r\",encoding='cp1252')\n",
    "            content = f.read()\n",
    "            temp = pd.DataFrame(\n",
    "                {\n",
    "                    'content':content,\n",
    "                    'tag':tag\n",
    "                },index=[0]\n",
    "            )\n",
    "            df = pd.concat([df, temp])\n",
    "\n",
    "\n",
    "\n",
    "    df.content =df.content.replace(to_replace='From:(.*\\n)', value='', regex=True) ##remove from to email\n",
    "    df.content =df.content.replace(to_replace='lines:(.*\\n)', value='', regex=True)\n",
    "    df.content =df.content.replace(to_replace='Subject:(.*\\n)', value='', regex=True)\n",
    "    df.content =df.content.replace(to_replace='[!\"#$%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except\n",
    "    df.content =df.content.replace(to_replace='-', value=' ', regex=True)\n",
    "    df.content =df.content.replace(to_replace='\\s+', value=' ', regex=True)    #remove new line\n",
    "    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space\n",
    "    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace\n",
    "\n",
    "    df['content']=[entry.lower() for entry in df['content']]\n",
    "    return df\n",
    "\n",
    "df_news_train = gather(path_train)\n",
    "\n",
    "\n",
    "\n",
    "def clean(data):\n",
    "    cleaned = defaultdict(list)\n",
    "    count = 0\n",
    "    for group in data:\n",
    "        for words in group.split():\n",
    "            if words.isalpha() and words not in all_names:\n",
    "                words = ps.stem(words)\n",
    "                cleaned[count].append(WNL.lemmatize(words.lower()))\n",
    "        cleaned[count] = ' '.join(cleaned[count])\n",
    "        count +=1\n",
    "    return(list(cleaned.values()))\n",
    "\n",
    "x_train = clean(df_news_train['content'])\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)\n",
    "tfidf = tf.fit_transform(x_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c4d234",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2b0eab39651e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcountvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vocab = tf.vocabulary_\n",
    "\n",
    "countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)\n",
    "\n",
    "count_wm = countvectorizer.fit_transform(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3f7911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tfidf\n",
      "aa         0.0\n",
      "aaa        0.0\n",
      "aaron      0.0\n",
      "ab         0.0\n",
      "abandon    0.0\n",
      "...        ...\n",
      "zubov      0.0\n",
      "zurich     0.0\n",
      "zv         0.0\n",
      "zx         0.0\n",
      "zz         0.0\n",
      "\n",
      "[8000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "\n",
    "#using Tfidftransformer and count vectorizer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(count_wm)\n",
    "\n",
    "\n",
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=countvectorizer.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "# count matrix \n",
    "count_vector=countvectorizer.transform(x_train) \n",
    " \n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "\n",
    "feature_names = countvectorizer.get_feature_names() \n",
    " \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[10342] \n",
    " \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=['tfidf'],ascending=False)\n",
    "\n",
    "#using Tfidfvectorizer\n",
    "\n",
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf[0] \n",
    "\n",
    "# place tf-idf values in a pandas data frame\n",
    "df_tfidf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tf.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "\n",
    "print(df_tfidf)\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in tf.vocabulary_.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad069c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open('/home/antonis/PycharmProjects/soup/news/Huffpost.json')\n",
    "data = json.load(f)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for i in data:\n",
    "    documents.append(data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f512e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55, 4773)\n"
     ]
    }
   ],
   "source": [
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer() \n",
    " \n",
    "# this steps generates word counts for the words in your docs \n",
    "word_count_vector=cv.fit_transform(documents)\n",
    "\n",
    "print(word_count_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d199c09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "60a9d97c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Series' objects are mutable, thus they cannot be hashed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-fba07317aa98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosine_similarity_T\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'computer science'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-8f6b55528595>\u001b[0m in \u001b[0;36mcosine_similarity_T\u001b[0;34m(k, query)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mq_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'q_clean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mq_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q_clean'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mWNL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0md_cosines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1935\u001b[0m         \u001b[0;31m# 0. Check the exception lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1937\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1938\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfilter_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         raise TypeError(\n\u001b[0;32m-> 1669\u001b[0;31m             \u001b[0;34mf\"{repr(type(self).__name__)} objects are mutable, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m             \u001b[0;34mf\"thus they cannot be hashed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Series' objects are mutable, thus they cannot be hashed"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

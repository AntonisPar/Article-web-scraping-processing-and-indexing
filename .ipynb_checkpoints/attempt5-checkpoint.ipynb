{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec10dd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/antonis/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import names\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "\n",
    "nltk.download('names')\n",
    "ps = PorterStemmer()\n",
    "all_names = names.words()\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "path_train= \"/home/antonis/Downloads/20news-bydate/20news-bydate-train\"\n",
    "path_test= \"/home/antonis/Downloads/20news-bydate/20news-bydate-test\"\n",
    "\n",
    "\n",
    "\n",
    "def gather(path):\n",
    "    df = pd.DataFrame()\n",
    "    for file in os.listdir(path):\n",
    "        tag = file\n",
    "        for doc in os.listdir(path+'/'+file):\n",
    "            docpath = path+'/'+file+'/'+doc\n",
    "            f = open(docpath, \"r\",encoding='cp1252')\n",
    "            content = f.read()\n",
    "            temp = pd.DataFrame(\n",
    "                {\n",
    "                    'content':content,\n",
    "                    'tag':tag\n",
    "                },index=[0]\n",
    "            )\n",
    "            df = pd.concat([df, temp])\n",
    "\n",
    "\n",
    "\n",
    "    df.content =df.content.replace(to_replace='From:(.*\\n)', value='', regex=True) ##remove from to email\n",
    "    df.content =df.content.replace(to_replace='lines:(.*\\n)', value='', regex=True)\n",
    "    df.content =df.content.replace(to_replace='Subject:(.*\\n)', value='', regex=True)\n",
    "    df.content =df.content.replace(to_replace='[!\"#$%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~]', value=' ', regex=True) #remove punctuation except\n",
    "    df.content =df.content.replace(to_replace='-', value=' ', regex=True)\n",
    "    df.content =df.content.replace(to_replace='\\s+', value=' ', regex=True)    #remove new line\n",
    "    df.content =df.content.replace(to_replace='  ', value='', regex=True)                #remove double white space\n",
    "    df.content =df.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace\n",
    "\n",
    "    df['content']=[entry.lower() for entry in df['content']]\n",
    "    return df\n",
    "\n",
    "df_news_train = gather(path_train)\n",
    "\n",
    "\n",
    "\n",
    "def clean(data):\n",
    "    cleaned = defaultdict(list)\n",
    "    count = 0\n",
    "    for group in data:\n",
    "        for words in group.split():\n",
    "            if words.isalpha() and words not in all_names:\n",
    "                words = ps.stem(words)\n",
    "                cleaned[count].append(WNL.lemmatize(words.lower()))\n",
    "        cleaned[count] = ' '.join(cleaned[count])\n",
    "        count +=1\n",
    "    return(list(cleaned.values()))\n",
    "\n",
    "x_train = clean(df_news_train['content'])\n",
    "\n",
    "\n",
    "tf = TfidfVectorizer(stop_words='english', max_features=8000,use_idf=True)\n",
    "tfidf = tf.fit_transform(x_train)\n",
    "\n",
    "vocab = tf.vocabulary_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c9ebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english',vocabulary=vocab)\n",
    "\n",
    "count_wm = countvectorizer.fit_transform(x_train)\n",
    "\n",
    "#using Tfidftransformer and count vectorizer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(count_wm)\n",
    "\n",
    "\n",
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=countvectorizer.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "# count matrix \n",
    "count_vector=countvectorizer.transform(x_train) \n",
    " \n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)\n",
    "\n",
    "feature_names = countvectorizer.get_feature_names() \n",
    " \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[10342] \n",
    " \n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=['tfidf'],ascending=False)\n",
    "\n",
    "#using Tfidfvectorizer\n",
    "\n",
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf[0] \n",
    "\n",
    "# place tf-idf values in a pandas data frame\n",
    "df_tfidf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tf.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in tf.vocabulary_.items():\n",
    "         if val == value:\n",
    "             return key\n",
    " \n",
    "    return \"key doesn't exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05e2be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 2 ... TEST\n",
    "\n",
    "\n",
    "def clean_test(data):\n",
    "    cleaned = defaultdict(list)\n",
    "    count = 0\n",
    "    for words in data.split():\n",
    "        if words.isalpha() and words not in all_names:\n",
    "            words = ps.stem(words)\n",
    "            cleaned[count].append(WNL.lemmatize(words.lower()))\n",
    "    cleaned[count] = ' '.join(cleaned[count])\n",
    "    count +=1\n",
    "    return(list(cleaned.values()))\n",
    "\n",
    "df_news_test = gather(path_test)\n",
    "\n",
    "df_news_test['content'] = df_news_test['content'].apply(clean_test)\n",
    "\n",
    "# x_test = clean(df_news_test['content'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fde6fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn import metrics\n",
    "\n",
    "def cosine_similarity(document,tfidf):\n",
    "    testvector = tf.transform(document)\n",
    "    distances = cosine_similarity(testvector, tfidf)\n",
    "    prediction = np.argmax(distances, 1)\n",
    "    predicted = df_news_train['tag'].iloc[prediction]\n",
    "    return predicted\n",
    "    \n",
    "\n",
    "correct = 0\n",
    "for row in df_news_test.itertuples():\n",
    "    predicted = cosine_similarity(row[1],tfidf)\n",
    "    actual = row[2]\n",
    "    if actual == predicted[0]:\n",
    "        correct=correct + 1\n",
    "print(correct)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0e743a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4853\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c49dd245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organ think machin corpor cambridg ma usa line distribut world nntp post host in articl alcor benali ilyess bdira write of cours you never read arab medium i don t though when i wa in israel i did make a point of listen to jtv news a well a mont carlo in the unit state i gener read the nyt and occasion a mainstream isra i read arab isra post and thi network is more than enough and western american french and british report and i can say that if we give israel and arab on the bia scale of cours you can switch the polar isra newspap will get either a or american lead newspap and tv news rang from to ye there are some that are more isra than isra the montreal suburban a local free newspap probabl is closer to kahan s view than some isra right wing newspap british rang from neutral to french that iknow of of cours rang from afro french magazin to arab offici medium rang from to egyptian to in whi no becaus they do not want to overdo it and stir peopl against israel and therefor against them sinc they are do what you may not be take into account is that the jp is no longer repres of the mainstream in it wa purchas a few year ago and in the battl for control most of the liber and left wing report walk the new owner state in the past more than onc that the jp s task should be gear toward explain and promot israel s posit more than attack the gov t likud at the time the paper that i would recommend read be middl stream and factual is ha aretz or at least thi wa the case two year the averag bia of what you read would be probabl around while that of the averag american would be the same if they do not read or read the new york time and similar news maker and if they read some other rel le bias and what about the nat l enquir but serious if one were to read some of the leftist newspap one could arriv at other the inform you receiv wa highli select and extrapol from it is a bad shai guday stealth bomber o softwar engin think machin the wing ninja of the cambridg ma\n"
     ]
    }
   ],
   "source": [
    "4853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef83de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
